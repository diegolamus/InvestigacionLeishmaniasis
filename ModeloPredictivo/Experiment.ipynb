{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentación modelo predictivo\n",
    "\n",
    "## Imports\n",
    "\n",
    "Primero, se importan las dependencias necesarias para la experimentación. Principalmente se trabajará con tensorflow, para calcular metricas, y keras para construir redes neuronales, instanciar redes ya existentes, crear generadores de datos, realizar data augmentatión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras import regularizers, initializers, Model, optimizers\n",
    "from keras.applications import VGG16, VGG19, ResNet50, InceptionV3\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, concatenate\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils import get_file \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento desde cero\n",
    "\n",
    "### Definición de función de entrenamiento\n",
    "\n",
    "A continuación se define la función con  que se entrenará los diferentes modelos. Se compila con metricas de exactitud para comparar los resultados de los diferentes modelos. Se realiza data augmentatión y se entrena el modelo. La función para entrenar recibe el modelo que debe entrenar, define los generadores de datos, compila el modelo, entrena y, finalmente, retorna la historia del entrenamiento.\n",
    "\n",
    "**Nota:** fue necesario instanciar el optimizador (adam) con parametros diferentes a los predeterminados dado que la exactitud de entrenamiento se mantenia en la linea base (76.2%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: model to be trained\n",
    "# dataset: leishmaniasis, melanoma_novus, cats_and_dogs\n",
    "# epochs: number of ephocs for training\n",
    "# save_as: name to save best weigths\n",
    "def train_model(model,dataset,epochs,image_size=224,batch_size=64,save_as='no_save'):\n",
    "    # Data generator to  rescale training images\n",
    "    # Data Augmentation: Horizontal and vertical flips\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255.0,\n",
    "                                      vertical_flip=True,\n",
    "                                      horizontal_flip=True)\n",
    "    # Data generator to rescale test images\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255.0)\n",
    "    # Data flow training images\n",
    "    train_flow = train_datagen.flow_from_directory(\n",
    "        directory='src/'+dataset+'/training',  \n",
    "        target_size=(image_size, image_size),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    # Data flow test images\n",
    "    test_flow = test_datagen.flow_from_directory(\n",
    "        directory='src/'+dataset+'/test',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    # Compile model\n",
    "    adam = optimizers.Adam(lr=0.001,\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.999,\n",
    "                          epsilon=None,\n",
    "                          decay=0.0,\n",
    "                          amsgrad=False)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=adam,\n",
    "        metrics=['acc'])\n",
    "    # Check if wegths are gona be saved\n",
    "    if( save_as != 'no_save'):\n",
    "        # Create check point call back to store best validation weigths\n",
    "        bestWeigthsPath='src/trainingWeigths/best_' + save_as+'.hdf5'\n",
    "        checkpoint = ModelCheckpoint(bestWeigthsPath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "        # Run experiment\n",
    "        history = model.fit_generator(\n",
    "            generator=train_flow,\n",
    "            epochs=epochs,\n",
    "            validation_data=test_flow,\n",
    "            callbacks=[checkpoint],\n",
    "            verbose=1)\n",
    "    else:\n",
    "        # Run experiment\n",
    "        history = model.fit_generator(\n",
    "            generator=train_flow,\n",
    "            epochs=epochs,\n",
    "            validation_data=test_flow,\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición funcion para exportar modelo\n",
    "\n",
    "Para la posterior integración del modelo con una herramienta movil se exportará el modelo en formato .hdf5. Anteriormente, creamos un checkpoint que cumple con esta función; sin embargo, solo se exporta los pesos del modelo y no el modelo completo (pesos + estructura). Teniendo en cuenta lo anterior es necesario cargar los pesos de los mejores resultados de entrenamiento (CheckPoint), y luego exportar el modelo entero (pesos + estructura)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(model,save_as):\n",
    "    # Load checkpoint weigths\n",
    "    model.load_weights('src/trainingWeigths/best_' + save_as+ '.hdf5')\n",
    "    # Remove file\n",
    "    os.remove('src/trainingWeigths/best_' + save_as+ '.hdf5')\n",
    "    # Create new file saving model and weigths\n",
    "    model.save('src/trainingWeigths/best_' + save_as+ '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición función para visualizar resultados\n",
    "\n",
    "Despues de entrenar cada modelo, se mostrara una grafica en donde se visualiza la exactitud tanto para el entrenamiento como la la validación en las diferentes epocas. Esto nos permitirá observare en que punto se llega o overfitting, y cual es el comportamiento general en este aspecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plt(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cración de modelo predictivo\n",
    "\n",
    "Se define una red neuronal con operaciones de convolucion y pooling, seguidas de capas de clasificación (Flatten, dense). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom model\n",
    "def build_CNN_model(image_size=224):\n",
    "    #Input\n",
    "    inputs = Input(shape=(image_size,image_size,3,))\n",
    "    #BranchOne\n",
    "    model = Conv2D(filters=16,kernel_size=(3,3),activation='relu')(inputs)\n",
    "    model = Conv2D(filters=32,kernel_size=(3,3),activation='relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(2,2))(model)    \n",
    "    model = Conv2D(filters=64,kernel_size=(3,3),activation='relu')(model) \n",
    "    model = Conv2D(filters=128,kernel_size=(3,3),activation='relu')(model)\n",
    "    model = MaxPooling2D(pool_size=(2,2))(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(124,activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    #Output\n",
    "    out = Dense(1, activation='sigmoid')(model)\n",
    "    # Compile Model\n",
    "    model = Model(inputs=[inputs], outputs=[out])\n",
    "    model.summary()\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perros y gatos para probar modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 220, 220, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 108, 108, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 106, 106, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 53, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 359552)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 124)               44584572  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 124)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 125       \n",
      "=================================================================\n",
      "Total params: 44,682,137\n",
      "Trainable params: 44,682,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 22500 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "352/352 [==============================] - 108s 308ms/step - loss: 0.6667 - acc: 0.6198 - val_loss: 0.5902 - val_acc: 0.6776\n",
      "Epoch 2/10\n",
      "352/352 [==============================] - 112s 319ms/step - loss: 0.5665 - acc: 0.7027 - val_loss: 0.5233 - val_acc: 0.7348\n",
      "Epoch 3/10\n",
      "352/352 [==============================] - 111s 316ms/step - loss: 0.5326 - acc: 0.7311 - val_loss: 0.4958 - val_acc: 0.7552\n",
      "Epoch 4/10\n",
      "352/352 [==============================] - 114s 323ms/step - loss: 0.4965 - acc: 0.7591 - val_loss: 0.4500 - val_acc: 0.7812\n",
      "Epoch 5/10\n",
      "352/352 [==============================] - 111s 316ms/step - loss: 0.4643 - acc: 0.7804 - val_loss: 0.4515 - val_acc: 0.7908\n",
      "Epoch 6/10\n",
      "352/352 [==============================] - 117s 332ms/step - loss: 0.4336 - acc: 0.8019 - val_loss: 0.4187 - val_acc: 0.8148\n",
      "Epoch 7/10\n",
      "352/352 [==============================] - 119s 339ms/step - loss: 0.4003 - acc: 0.8222 - val_loss: 0.4219 - val_acc: 0.8120\n",
      "Epoch 8/10\n",
      "352/352 [==============================] - 116s 330ms/step - loss: 0.3715 - acc: 0.8392 - val_loss: 0.4126 - val_acc: 0.8112\n",
      "Epoch 9/10\n",
      "352/352 [==============================] - 116s 330ms/step - loss: 0.3472 - acc: 0.8510 - val_loss: 0.3880 - val_acc: 0.8292\n",
      "Epoch 10/10\n",
      "352/352 [==============================] - 118s 337ms/step - loss: 0.3195 - acc: 0.8671 - val_loss: 0.3977 - val_acc: 0.8240\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_CNN_model()\n",
    "history = train_model(model,'dogs_and_cats',10)\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La linea base para perros y gatos es del 50%, ya que se tienen la misma cantidad de imagenes para las dos clases. Los dataset de entrenamiento y prueba se encuentran estratificados.\n",
    "\n",
    "Sobre el dataset de perros y gatos, en 10 epocas, se logra superar la linea base y alcanzar un accuracy de validación de 82,92%.\n",
    "\n",
    "### Entrenar modelo con dataset de Leishmaniasis Cutanea\n",
    "\n",
    "Despues de probar el modelo con perros y gatos se probará con leishmaniasis cutanea, para ver como desempeña y establecer un punto incial para resolver el problema. Cabe destacar que el tamaño de los datasets es diferente. El dataset de perros y gatos tiene 21500 de imagenes de entrenamiento, el de lishmaniasis cutanea tiene 1640 y está desbalanceado.\n",
    "\n",
    "La linea base, dado el número de imagenes negativas y positivas, es del 76%. Si el modelo predice que todas son leishmaniasis, se obtendrá accuracy de 76%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 220, 220, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 108, 108, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 106, 106, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 53, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 359552)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 124)               44584572  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 124)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 125       \n",
      "=================================================================\n",
      "Total params: 44,682,137\n",
      "Trainable params: 44,682,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 41s 2s/step - loss: 3.7925 - acc: 0.7511 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.7161 - acc: 0.7669 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.8339 - acc: 0.7595 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 30s 1s/step - loss: 3.8104 - acc: 0.7610 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.6925 - acc: 0.7684 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.8339 - acc: 0.7595 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 34s 1s/step - loss: 3.8339 - acc: 0.7595 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.8575 - acc: 0.7580 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.8104 - acc: 0.7610 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 34s 1s/step - loss: 3.7632 - acc: 0.7639 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.8104 - acc: 0.7610 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.8575 - acc: 0.7580 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 3.9283 - acc: 0.7536 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.8811 - acc: 0.7566 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.8575 - acc: 0.7580 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.7868 - acc: 0.7625 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 3.8104 - acc: 0.7610 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.8104 - acc: 0.7610 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.8339 - acc: 0.7595 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.8104 - acc: 0.7610 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.8339 - acc: 0.7595 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 3.8104 - acc: 0.7610 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 3.8575 - acc: 0.7580 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 3.7632 - acc: 0.7639 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.7396 - acc: 0.7654 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 3.8104 - acc: 0.7610 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.8339 - acc: 0.7595 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.8339 - acc: 0.7595 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 3.7396 - acc: 0.7654 - val_loss: 3.7883 - val_acc: 0.7624\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 3.7632 - acc: 0.7639 - val_loss: 3.7883 - val_acc: 0.7624\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_CNN_model()\n",
    "history = train_model(model,'leishmaniasis',30)\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del entrenamiento anterior se observa que no fue posible superar la linea base, tanto para el entrenamiento como para la validación. Esto presenta problemas dado que el accuracy de entrenamiento deberia de incrementar y ajustarse totalmente a los datos de entrenamiento, generado un estado de overfitting. \n",
    "\n",
    "Se procede a redefinir la función de entrenamiento con el fin de conseguir que el modelo aprenda correctamente las caracteristicas del conjunto de entrenamiento, y entré en overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: model to be trained\n",
    "# dataset: leishmaniasis, melanoma_novus, cats_and_dogs\n",
    "# epochs: number of ephocs for training\n",
    "# save_as: name to save best weigths\n",
    "def train_model(model,dataset,epochs,image_size=224,batch_size=64,save_as='no_save'):\n",
    "    # Data generator to  rescale training images\n",
    "    # Data Augmentation: Horizontal and vertical flips\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255.0,\n",
    "                                      vertical_flip=True,\n",
    "                                      horizontal_flip=True)\n",
    "    # Data generator to rescale test images\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255.0)\n",
    "    # Data flow training images\n",
    "    train_flow = train_datagen.flow_from_directory(\n",
    "        directory='src/'+dataset+'/training',  \n",
    "        target_size=(image_size, image_size),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    # Data flow test images\n",
    "    test_flow = test_datagen.flow_from_directory(\n",
    "        directory='src/'+dataset+'/test',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    # Compile model\n",
    "    adam = optimizers.Adam(lr=0.0001,\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.999,\n",
    "                          epsilon=None,\n",
    "                          decay=0.00001,\n",
    "                          amsgrad=False)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=adam,\n",
    "        metrics=['acc'])\n",
    "    # Check if wegths are gona be saved\n",
    "    if( save_as != 'no_save'):\n",
    "        # Create check point call back to store best validation weigths\n",
    "        bestWeigthsPath='src/trainingWeigths/best_' + save_as+'.h5'\n",
    "        checkpoint = ModelCheckpoint(bestWeigthsPath, monitor='val_acc',save_weights_only=False, verbose=1, save_best_only=True, mode='max')\n",
    "        # Run experiment\n",
    "        history = model.fit_generator(\n",
    "            generator=train_flow,\n",
    "            epochs=epochs,\n",
    "            validation_data=test_flow,\n",
    "            callbacks=[checkpoint],\n",
    "            verbose=1)\n",
    "    else:\n",
    "        # Run experiment\n",
    "        history = model.fit_generator(\n",
    "            generator=train_flow,\n",
    "            epochs=epochs,\n",
    "            validation_data=test_flow,\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se cambió el optimizador. Se utilizó un learning rate 10 veces menor que el utilizado en el problema de perros y gatos. Además, se definió decay= 0.00001, por lo que el learning rate disminuye a medida que corren las epocas. Se optó por esta decisión dado que es posible que al tener un learning rate alto, sin decay, no sea posible llegar a un minimo local, sino que se tienen saltos grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 220, 220, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 108, 108, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 106, 106, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 53, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 359552)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 124)               44584572  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 124)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 125       \n",
      "=================================================================\n",
      "Total params: 44,682,137\n",
      "Trainable params: 44,682,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "26/26 [==============================] - 27s 1s/step - loss: 0.5716 - acc: 0.7553 - val_loss: 0.5441 - val_acc: 0.7624\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 26s 1s/step - loss: 0.5466 - acc: 0.7639 - val_loss: 0.5181 - val_acc: 0.7624\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5435 - acc: 0.7610 - val_loss: 0.5309 - val_acc: 0.7624\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5176 - acc: 0.7669 - val_loss: 0.5090 - val_acc: 0.7624\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5207 - acc: 0.7616 - val_loss: 0.4958 - val_acc: 0.7624\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5097 - acc: 0.7595 - val_loss: 0.4840 - val_acc: 0.7624\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5097 - acc: 0.7652 - val_loss: 0.4802 - val_acc: 0.7624\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4998 - acc: 0.7652 - val_loss: 0.4770 - val_acc: 0.7673\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4924 - acc: 0.7664 - val_loss: 0.4592 - val_acc: 0.7599\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4847 - acc: 0.7601 - val_loss: 0.4599 - val_acc: 0.7599\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4857 - acc: 0.7640 - val_loss: 0.4601 - val_acc: 0.7624\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4723 - acc: 0.7700 - val_loss: 0.4504 - val_acc: 0.7673\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4584 - acc: 0.7765 - val_loss: 0.4544 - val_acc: 0.7921\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4677 - acc: 0.7664 - val_loss: 0.4488 - val_acc: 0.7649\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4531 - acc: 0.7766 - val_loss: 0.4708 - val_acc: 0.7847\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4593 - acc: 0.7691 - val_loss: 0.4403 - val_acc: 0.7673\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4559 - acc: 0.7703 - val_loss: 0.4347 - val_acc: 0.7649\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4479 - acc: 0.7679 - val_loss: 0.4443 - val_acc: 0.7673\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4352 - acc: 0.7751 - val_loss: 0.4241 - val_acc: 0.7822\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4295 - acc: 0.7826 - val_loss: 0.4265 - val_acc: 0.7797\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4261 - acc: 0.7838 - val_loss: 0.4171 - val_acc: 0.7822\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4167 - acc: 0.7835 - val_loss: 0.4255 - val_acc: 0.7797\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4144 - acc: 0.7830 - val_loss: 0.4098 - val_acc: 0.7995\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4508 - acc: 0.7958 - val_loss: 0.4240 - val_acc: 0.7822\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4235 - acc: 0.7720 - val_loss: 0.4289 - val_acc: 0.7748\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4144 - acc: 0.7895 - val_loss: 0.4126 - val_acc: 0.7970\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4052 - acc: 0.7880 - val_loss: 0.4118 - val_acc: 0.7871\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3977 - acc: 0.7965 - val_loss: 0.4121 - val_acc: 0.7970\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4022 - acc: 0.7905 - val_loss: 0.4129 - val_acc: 0.8020\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3858 - acc: 0.7950 - val_loss: 0.4097 - val_acc: 0.8193\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_CNN_model()\n",
    "history = train_model(model,'leishmaniasis',30)\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que al cambiar el learning rate es posible salir de la linea base, ahora se correran 100 épocas, con el fin de ver en que momento se entra en overfitting, y que valor máximo de accuracy de validación se puede obtener.\n",
    "\n",
    "### primer intento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 220, 220, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 108, 108, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 106, 106, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 53, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 359552)            0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 124)               44584572  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 124)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 125       \n",
      "=================================================================\n",
      "Total params: 44,682,137\n",
      "Trainable params: 44,682,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 26s 1s/step - loss: 0.6074 - acc: 0.7519 - val_loss: 0.5600 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 26s 995ms/step - loss: 0.5641 - acc: 0.7639 - val_loss: 0.5418 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5242 - acc: 0.7684 - val_loss: 0.5392 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.5370 - acc: 0.7607 - val_loss: 0.5176 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5298 - acc: 0.7610 - val_loss: 0.5034 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5290 - acc: 0.7566 - val_loss: 0.4971 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76238\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5189 - acc: 0.7610 - val_loss: 0.4913 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76238\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5153 - acc: 0.7649 - val_loss: 0.4824 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76238\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5087 - acc: 0.7643 - val_loss: 0.4831 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76238\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4998 - acc: 0.7604 - val_loss: 0.4933 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76238\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5059 - acc: 0.7621 - val_loss: 0.4787 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.76238 to 0.77228, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4907 - acc: 0.7742 - val_loss: 0.4802 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.77228\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4895 - acc: 0.7661 - val_loss: 0.4682 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.77228\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5092 - acc: 0.7542 - val_loss: 0.5339 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.77228\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5032 - acc: 0.7679 - val_loss: 0.4722 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.77228\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4824 - acc: 0.7673 - val_loss: 0.4728 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.77228 to 0.77970, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4859 - acc: 0.7676 - val_loss: 0.4705 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.77970 to 0.78218, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4810 - acc: 0.7746 - val_loss: 0.4593 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.78218\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4669 - acc: 0.7694 - val_loss: 0.4554 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.78218\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4580 - acc: 0.7868 - val_loss: 0.4536 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.78218\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4524 - acc: 0.7805 - val_loss: 0.4546 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.78218 to 0.78218, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.4464 - acc: 0.7844 - val_loss: 0.4482 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.78218\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4451 - acc: 0.7847 - val_loss: 0.4514 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.78218 to 0.79208, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.4389 - acc: 0.7847 - val_loss: 0.4474 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.79208\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4265 - acc: 0.7865 - val_loss: 0.4480 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.79208\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4332 - acc: 0.7833 - val_loss: 0.4525 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.79208\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4293 - acc: 0.7982 - val_loss: 0.4614 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.79208\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4176 - acc: 0.7976 - val_loss: 0.4716 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.79208\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4153 - acc: 0.7941 - val_loss: 0.4794 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.79208\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4042 - acc: 0.8057 - val_loss: 0.4445 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.79208\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4082 - acc: 0.7989 - val_loss: 0.4824 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.79208\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3856 - acc: 0.8143 - val_loss: 0.4458 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.79208\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3844 - acc: 0.8180 - val_loss: 0.4559 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.79208\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3897 - acc: 0.8079 - val_loss: 0.4851 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.79208\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3898 - acc: 0.8058 - val_loss: 0.4517 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.79208\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3786 - acc: 0.8136 - val_loss: 0.4770 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.79208\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3746 - acc: 0.8185 - val_loss: 0.4469 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.79208\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3743 - acc: 0.8190 - val_loss: 0.4770 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.79208\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3796 - acc: 0.8166 - val_loss: 0.4699 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.79208\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3586 - acc: 0.8271 - val_loss: 0.4444 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.79208\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3419 - acc: 0.8430 - val_loss: 0.5583 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.79208\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3594 - acc: 0.8241 - val_loss: 0.4535 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.79208\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3386 - acc: 0.8404 - val_loss: 0.4536 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.79208\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3318 - acc: 0.8380 - val_loss: 0.4519 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.79208\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3393 - acc: 0.8370 - val_loss: 0.4546 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.79208 to 0.79703, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3315 - acc: 0.8473 - val_loss: 0.4514 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.79703\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3305 - acc: 0.8470 - val_loss: 0.4681 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.79703\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3259 - acc: 0.8422 - val_loss: 0.4740 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.79703\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3075 - acc: 0.8575 - val_loss: 0.4754 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.79703\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3035 - acc: 0.8617 - val_loss: 0.4716 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.79703\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2965 - acc: 0.8614 - val_loss: 0.4574 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.79703\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2990 - acc: 0.8594 - val_loss: 0.4701 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.79703\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3014 - acc: 0.8560 - val_loss: 0.4842 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.79703\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2836 - acc: 0.8695 - val_loss: 0.4857 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.79703\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2820 - acc: 0.8696 - val_loss: 0.5595 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.79703\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2770 - acc: 0.8734 - val_loss: 0.5370 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.79703\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2580 - acc: 0.8794 - val_loss: 0.5586 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.79703 to 0.79703, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.2599 - acc: 0.8812 - val_loss: 0.4719 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.79703\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2546 - acc: 0.8879 - val_loss: 0.4964 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.79703\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2419 - acc: 0.8932 - val_loss: 0.5662 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.79703\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2552 - acc: 0.8719 - val_loss: 0.4942 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.79703\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2490 - acc: 0.8872 - val_loss: 0.5233 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.79703\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2425 - acc: 0.8870 - val_loss: 0.5815 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.79703\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2537 - acc: 0.8755 - val_loss: 0.6015 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.79703\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2359 - acc: 0.8924 - val_loss: 0.5193 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00065: val_acc improved from 0.79703 to 0.80693, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2349 - acc: 0.8870 - val_loss: 0.5657 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.80693\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2167 - acc: 0.9007 - val_loss: 0.6113 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.80693\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2365 - acc: 0.8963 - val_loss: 0.5348 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.80693\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2264 - acc: 0.8987 - val_loss: 0.5612 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.80693\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2207 - acc: 0.8960 - val_loss: 0.6200 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.80693\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2140 - acc: 0.8990 - val_loss: 0.5354 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.80693\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2201 - acc: 0.8993 - val_loss: 0.5758 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.80693\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1980 - acc: 0.9104 - val_loss: 0.5293 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.80693\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2024 - acc: 0.9110 - val_loss: 0.5344 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.80693\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1951 - acc: 0.9071 - val_loss: 0.6724 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.80693\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1912 - acc: 0.9119 - val_loss: 0.6404 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.80693\n",
      "Epoch 77/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 31s 1s/step - loss: 0.2047 - acc: 0.9063 - val_loss: 0.5716 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.80693\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1825 - acc: 0.9168 - val_loss: 0.6707 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.80693\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1781 - acc: 0.9152 - val_loss: 0.6301 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.80693\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1757 - acc: 0.9170 - val_loss: 0.6000 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.80693\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1780 - acc: 0.9110 - val_loss: 0.5639 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.80693\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1806 - acc: 0.9131 - val_loss: 0.6632 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.80693\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1706 - acc: 0.9269 - val_loss: 0.7369 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.80693\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1768 - acc: 0.9101 - val_loss: 0.6933 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.80693\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1708 - acc: 0.9215 - val_loss: 0.6134 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.80693\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1576 - acc: 0.9290 - val_loss: 0.6909 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.80693\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1460 - acc: 0.9296 - val_loss: 0.6878 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.80693\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1575 - acc: 0.9230 - val_loss: 0.7387 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.80693\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1535 - acc: 0.9263 - val_loss: 0.6364 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.80693\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1449 - acc: 0.9375 - val_loss: 0.7338 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.80693\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1438 - acc: 0.9348 - val_loss: 0.6791 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.80693\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1392 - acc: 0.9354 - val_loss: 0.6619 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.80693\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1388 - acc: 0.9338 - val_loss: 0.7815 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.80693\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1387 - acc: 0.9279 - val_loss: 0.7468 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.80693\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1343 - acc: 0.9356 - val_loss: 0.6667 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.80693\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1274 - acc: 0.9366 - val_loss: 0.7251 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.80693\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1269 - acc: 0.9420 - val_loss: 0.8115 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.80693\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1341 - acc: 0.9402 - val_loss: 0.7685 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.80693\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1400 - acc: 0.9392 - val_loss: 0.7644 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.80693\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1272 - acc: 0.9459 - val_loss: 0.8321 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.80693\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1198 - acc: 0.9435 - val_loss: 0.8804 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.80693\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1222 - acc: 0.9420 - val_loss: 0.9237 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.80693\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1109 - acc: 0.9489 - val_loss: 0.8664 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.80693\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1158 - acc: 0.9441 - val_loss: 0.8032 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.80693\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1099 - acc: 0.9480 - val_loss: 0.7904 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.80693\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1104 - acc: 0.9504 - val_loss: 0.8835 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.80693\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1146 - acc: 0.9381 - val_loss: 0.7804 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.80693\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1031 - acc: 0.9543 - val_loss: 0.8101 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.80693\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0993 - acc: 0.9573 - val_loss: 0.8496 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.80693\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1064 - acc: 0.9498 - val_loss: 0.8269 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.80693\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1116 - acc: 0.9471 - val_loss: 0.8036 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.80693\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0942 - acc: 0.9510 - val_loss: 0.8510 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.80693\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0862 - acc: 0.9591 - val_loss: 0.8692 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.80693\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1087 - acc: 0.9522 - val_loss: 0.7398 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.80693\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0988 - acc: 0.9588 - val_loss: 0.8105 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.80693\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0962 - acc: 0.9501 - val_loss: 0.8024 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.80693\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0863 - acc: 0.9657 - val_loss: 0.8402 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.80693\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0896 - acc: 0.9597 - val_loss: 0.7719 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.80693\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0943 - acc: 0.9603 - val_loss: 0.8242 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.80693\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0923 - acc: 0.9564 - val_loss: 0.7262 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.80693\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0961 - acc: 0.9612 - val_loss: 0.7481 - val_acc: 0.7797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00121: val_acc did not improve from 0.80693\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0970 - acc: 0.9570 - val_loss: 0.8132 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.80693\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0912 - acc: 0.9567 - val_loss: 0.8340 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.80693\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0903 - acc: 0.9639 - val_loss: 0.8985 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.80693\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0920 - acc: 0.9640 - val_loss: 0.8120 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.80693\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1158 - acc: 0.9501 - val_loss: 0.8421 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.80693\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1016 - acc: 0.9504 - val_loss: 0.9596 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.80693\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0864 - acc: 0.9591 - val_loss: 0.9024 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.80693\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0831 - acc: 0.9642 - val_loss: 0.8829 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.80693\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0780 - acc: 0.9681 - val_loss: 1.0332 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.80693\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0903 - acc: 0.9555 - val_loss: 0.9171 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.80693\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0829 - acc: 0.9675 - val_loss: 0.9076 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.80693\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0733 - acc: 0.9723 - val_loss: 0.9571 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.80693\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0752 - acc: 0.9672 - val_loss: 0.9910 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.80693\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0785 - acc: 0.9658 - val_loss: 0.9061 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.80693\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0732 - acc: 0.9684 - val_loss: 0.9551 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.80693\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0833 - acc: 0.9705 - val_loss: 0.8996 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00137: val_acc improved from 0.80693 to 0.80941, saving model to src/trainingWeigths/best_first_try.hdf5\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0755 - acc: 0.9733 - val_loss: 0.8219 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.80941\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0726 - acc: 0.9702 - val_loss: 1.0528 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.80941\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0805 - acc: 0.9639 - val_loss: 0.8374 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.80941\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0807 - acc: 0.9669 - val_loss: 0.9089 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.80941\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0723 - acc: 0.9687 - val_loss: 0.9168 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.80941\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0644 - acc: 0.9771 - val_loss: 1.0048 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.80941\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0621 - acc: 0.9759 - val_loss: 1.1054 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.80941\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0627 - acc: 0.9759 - val_loss: 0.9737 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.80941\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0743 - acc: 0.9741 - val_loss: 1.0120 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.80941\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0656 - acc: 0.9730 - val_loss: 0.9205 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.80941\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0687 - acc: 0.9711 - val_loss: 1.0085 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.80941\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0633 - acc: 0.9769 - val_loss: 1.0377 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.80941\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0522 - acc: 0.9862 - val_loss: 1.1401 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.80941\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_CNN_model()\n",
    "history = train_model(model,'leishmaniasis',150,save_as='first_try')\n",
    "export(model,'first_try')\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De lo anterior se observa que la máxima exactitud que se obtuvo es de 80.9%. Tambien se observa que el modelo entra en sobre ajuste. Al entrar en sobre ajuste, las caracteristicas que guarda el modelo pueden estar demasiado ajustadas al conjunto de entrenamiento. \n",
    "\n",
    "### Regularización de kernel y bias\n",
    "\n",
    "Para combatir el sobre ajuste,  se agregará regularización l2 de kernel y bias, y se agrega una capa extra de dropout. Se procede a ejecutar 150 epocas para ver si esta medida es efectiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 220, 220, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 108, 108, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 106, 106, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 53, 53, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 359552)            0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 359552)            0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 124)               44584572  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 124)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 125       \n",
      "=================================================================\n",
      "Total params: 44,682,137\n",
      "Trainable params: 44,682,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.0832 - acc: 0.7450 - val_loss: 1.9698 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 1.9328 - acc: 0.7625 - val_loss: 1.8551 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 1.8105 - acc: 0.7639 - val_loss: 1.7458 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.7247 - acc: 0.7639 - val_loss: 1.6610 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6488 - acc: 0.7669 - val_loss: 1.5848 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5799 - acc: 0.7610 - val_loss: 1.5158 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76238\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.5008 - acc: 0.7625 - val_loss: 1.4620 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76238\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4459 - acc: 0.7669 - val_loss: 1.4251 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76238\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.3992 - acc: 0.7639 - val_loss: 1.3613 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76238\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 1.3418 - acc: 0.7610 - val_loss: 1.3196 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.2909 - acc: 0.7625 - val_loss: 1.2773 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76238\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.2488 - acc: 0.7654 - val_loss: 1.2271 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76238\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.2167 - acc: 0.7610 - val_loss: 1.1859 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76238\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.1796 - acc: 0.7625 - val_loss: 1.1482 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76238\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.1597 - acc: 0.7580 - val_loss: 1.1256 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76238\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.1269 - acc: 0.7566 - val_loss: 1.1106 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76238\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.1021 - acc: 0.7580 - val_loss: 1.0776 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76238\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.0640 - acc: 0.7639 - val_loss: 1.0518 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76238\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 1.0546 - acc: 0.7595 - val_loss: 1.0314 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76238\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.0340 - acc: 0.7566 - val_loss: 1.0290 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76238\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.0094 - acc: 0.7639 - val_loss: 0.9950 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76238\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.9970 - acc: 0.7639 - val_loss: 1.0093 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76238\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.9808 - acc: 0.7669 - val_loss: 0.9716 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76238\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.9680 - acc: 0.7639 - val_loss: 0.9777 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76238\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.9588 - acc: 0.7595 - val_loss: 0.9440 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76238\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.9344 - acc: 0.7610 - val_loss: 0.9252 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76238\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.9180 - acc: 0.7589 - val_loss: 0.9110 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76238\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.9103 - acc: 0.7578 - val_loss: 0.9047 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.76238 to 0.76733, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.8886 - acc: 0.7702 - val_loss: 0.9053 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76733\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.8896 - acc: 0.7655 - val_loss: 0.8912 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76733\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.8753 - acc: 0.7724 - val_loss: 0.8704 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76733\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.8654 - acc: 0.7667 - val_loss: 0.8779 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76733\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.8535 - acc: 0.7723 - val_loss: 0.8551 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76733\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.8430 - acc: 0.7676 - val_loss: 0.8509 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.76733\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.8474 - acc: 0.7771 - val_loss: 0.8534 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.76733\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.8276 - acc: 0.7715 - val_loss: 0.8395 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.76733\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.8237 - acc: 0.7680 - val_loss: 0.8267 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.76733 to 0.76980, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.8110 - acc: 0.7706 - val_loss: 0.8306 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.76980\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.8064 - acc: 0.7775 - val_loss: 0.8260 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.76980\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.8019 - acc: 0.7791 - val_loss: 0.8283 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.76980\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.8007 - acc: 0.7694 - val_loss: 0.8271 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.76980\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.7836 - acc: 0.7769 - val_loss: 0.8040 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.76980\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7812 - acc: 0.7790 - val_loss: 0.7900 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.76980 to 0.77970, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.7687 - acc: 0.7799 - val_loss: 0.7915 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.77970\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7672 - acc: 0.7788 - val_loss: 0.7865 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.77970\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7524 - acc: 0.7859 - val_loss: 0.7985 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.77970\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7503 - acc: 0.7781 - val_loss: 0.7860 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.77970\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7520 - acc: 0.7832 - val_loss: 0.7805 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.77970\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7465 - acc: 0.7820 - val_loss: 0.7840 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.77970\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7492 - acc: 0.7793 - val_loss: 0.7656 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.77970\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7421 - acc: 0.7709 - val_loss: 0.7695 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.77970\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7378 - acc: 0.7839 - val_loss: 0.7562 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.77970\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7359 - acc: 0.7859 - val_loss: 0.7575 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.77970\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7226 - acc: 0.7845 - val_loss: 0.7586 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.77970\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7136 - acc: 0.7946 - val_loss: 0.7625 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.77970\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7151 - acc: 0.7868 - val_loss: 0.7519 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.77970\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6995 - acc: 0.7952 - val_loss: 0.7437 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.77970 to 0.78713, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.6942 - acc: 0.7899 - val_loss: 0.7389 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.78713\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6969 - acc: 0.7913 - val_loss: 0.7429 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.78713\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6917 - acc: 0.7878 - val_loss: 0.7375 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.78713\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6884 - acc: 0.7949 - val_loss: 0.7583 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.78713 to 0.79208, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.6814 - acc: 0.7940 - val_loss: 0.7555 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.79208\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6757 - acc: 0.8045 - val_loss: 0.7729 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.79208\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6681 - acc: 0.7946 - val_loss: 0.7276 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.79208\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6634 - acc: 0.8034 - val_loss: 0.7509 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.79208\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6675 - acc: 0.7937 - val_loss: 0.7320 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.79208\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6723 - acc: 0.7971 - val_loss: 0.7497 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.79208\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6496 - acc: 0.8082 - val_loss: 0.7206 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.79208\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6525 - acc: 0.7968 - val_loss: 0.7492 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.79208\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6402 - acc: 0.8096 - val_loss: 0.7211 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.79208\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6443 - acc: 0.8085 - val_loss: 0.7183 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.79208\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6350 - acc: 0.8070 - val_loss: 0.7324 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.79208\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6358 - acc: 0.8172 - val_loss: 0.7415 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.79208\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6344 - acc: 0.8034 - val_loss: 0.7147 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.79208\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6299 - acc: 0.8111 - val_loss: 0.7151 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.79208\n",
      "Epoch 76/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 30s 1s/step - loss: 0.6287 - acc: 0.8087 - val_loss: 0.7216 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.79208\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6230 - acc: 0.8115 - val_loss: 0.7236 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.79208\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6149 - acc: 0.8075 - val_loss: 0.7406 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.79208\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6053 - acc: 0.8151 - val_loss: 0.7489 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.79208\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6070 - acc: 0.8140 - val_loss: 0.7174 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.79208\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6096 - acc: 0.8050 - val_loss: 0.7100 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.79208\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5988 - acc: 0.8196 - val_loss: 0.7166 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.79208 to 0.79208, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.5873 - acc: 0.8244 - val_loss: 0.7007 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.79208\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5790 - acc: 0.8262 - val_loss: 0.7094 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.79208 to 0.79455, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.5841 - acc: 0.8298 - val_loss: 0.7261 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.79455\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5720 - acc: 0.8189 - val_loss: 0.7268 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.79455\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5783 - acc: 0.8208 - val_loss: 0.7093 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.79455\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5698 - acc: 0.8388 - val_loss: 0.7962 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.79455\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5765 - acc: 0.8271 - val_loss: 0.7404 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.79455\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5627 - acc: 0.8265 - val_loss: 0.7477 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.79455\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5531 - acc: 0.8331 - val_loss: 0.7420 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.79455\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5678 - acc: 0.8287 - val_loss: 0.7245 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.79455\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5521 - acc: 0.8367 - val_loss: 0.7500 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.79455\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5674 - acc: 0.8323 - val_loss: 0.7128 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.79455\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5576 - acc: 0.8301 - val_loss: 0.7007 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00095: val_acc improved from 0.79455 to 0.79950, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5476 - acc: 0.8460 - val_loss: 0.7171 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.79950\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5469 - acc: 0.8316 - val_loss: 0.7458 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.79950\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5348 - acc: 0.8448 - val_loss: 0.7132 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.79950\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5196 - acc: 0.8602 - val_loss: 0.7271 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.79950\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5286 - acc: 0.8355 - val_loss: 0.7212 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00100: val_acc improved from 0.79950 to 0.81436, saving model to src/trainingWeigths/best_second_try.hdf5\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5286 - acc: 0.8440 - val_loss: 0.7044 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.81436\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5171 - acc: 0.8571 - val_loss: 0.7253 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.81436\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5242 - acc: 0.8524 - val_loss: 0.7119 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.81436\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5197 - acc: 0.8541 - val_loss: 0.7693 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.81436\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5054 - acc: 0.8656 - val_loss: 0.7571 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.81436\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5041 - acc: 0.8658 - val_loss: 0.7682 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.81436\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4945 - acc: 0.8710 - val_loss: 0.7689 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.81436\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4959 - acc: 0.8633 - val_loss: 0.7660 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.81436\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4889 - acc: 0.8701 - val_loss: 0.7814 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.81436\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5041 - acc: 0.8629 - val_loss: 0.8160 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.81436\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5086 - acc: 0.8621 - val_loss: 0.7494 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.81436\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4904 - acc: 0.8710 - val_loss: 0.7496 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.81436\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4989 - acc: 0.8635 - val_loss: 0.7590 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.81436\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4928 - acc: 0.8698 - val_loss: 0.7085 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.81436\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4821 - acc: 0.8746 - val_loss: 0.8380 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.81436\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4732 - acc: 0.8728 - val_loss: 0.7783 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.81436\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4635 - acc: 0.8867 - val_loss: 0.7369 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.81436\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4662 - acc: 0.8848 - val_loss: 0.7391 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.81436\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4654 - acc: 0.8836 - val_loss: 0.7187 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.81436\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4770 - acc: 0.8708 - val_loss: 0.7600 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.81436\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4613 - acc: 0.8761 - val_loss: 0.7370 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.81436\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4565 - acc: 0.8918 - val_loss: 0.7600 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.81436\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4548 - acc: 0.8866 - val_loss: 0.7786 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.81436\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4523 - acc: 0.8878 - val_loss: 0.8557 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.81436\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4399 - acc: 0.8936 - val_loss: 0.7881 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.81436\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4509 - acc: 0.8938 - val_loss: 0.7585 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.81436\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4402 - acc: 0.8873 - val_loss: 0.7324 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.81436\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4458 - acc: 0.8807 - val_loss: 0.8913 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.81436\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4346 - acc: 0.8918 - val_loss: 0.8651 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.81436\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4289 - acc: 0.8969 - val_loss: 0.8059 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.81436\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4396 - acc: 0.8861 - val_loss: 0.7890 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.81436\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4349 - acc: 0.9029 - val_loss: 0.8811 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.81436\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4252 - acc: 0.8947 - val_loss: 0.8667 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.81436\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4284 - acc: 0.9080 - val_loss: 0.8679 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.81436\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4159 - acc: 0.9059 - val_loss: 0.8575 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.81436\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4058 - acc: 0.9134 - val_loss: 0.8653 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.81436\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4192 - acc: 0.9017 - val_loss: 0.8117 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.81436\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4037 - acc: 0.9083 - val_loss: 0.8464 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.81436\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4070 - acc: 0.9104 - val_loss: 0.7794 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.81436\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4078 - acc: 0.9131 - val_loss: 0.9023 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.81436\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4104 - acc: 0.9104 - val_loss: 0.8374 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.81436\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3914 - acc: 0.9230 - val_loss: 0.8763 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.81436\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3918 - acc: 0.9174 - val_loss: 0.8985 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.81436\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4072 - acc: 0.9101 - val_loss: 0.8695 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.81436\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3905 - acc: 0.9167 - val_loss: 0.8887 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.81436\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3841 - acc: 0.9188 - val_loss: 0.9440 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.81436\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3941 - acc: 0.9179 - val_loss: 0.8012 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.81436\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3912 - acc: 0.9171 - val_loss: 0.8006 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.81436\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3750 - acc: 0.9332 - val_loss: 0.8706 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.81436\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3838 - acc: 0.9279 - val_loss: 0.8428 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.81436\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load custom model\n",
    "def build_CNN_model(image_size=224):\n",
    "    #Input\n",
    "    inputs = Input(shape=(image_size,image_size,3,))\n",
    "    #BranchOne\n",
    "    model = Conv2D(filters=16,kernel_size=(3,3),activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(0.01),bias_regularizer=regularizers.l2(0.01))(inputs)\n",
    "    model = Conv2D(filters=32,kernel_size=(3,3),activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(0.01),bias_regularizer=regularizers.l2(0.01))(model)\n",
    "    model = MaxPooling2D(pool_size=(2,2))(model)    \n",
    "    model = Conv2D(filters=64,kernel_size=(3,3),activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(0.01),bias_regularizer=regularizers.l2(0.01))(model) \n",
    "    model = Conv2D(filters=128,kernel_size=(3,3),activation='relu',\n",
    "                   kernel_regularizer=regularizers.l2(0.01),bias_regularizer=regularizers.l2(0.01))(model)\n",
    "    model = MaxPooling2D(pool_size=(2,2))(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = Dense(124,activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    #Output\n",
    "    out = Dense(1, activation='sigmoid')(model)\n",
    "    # Compile Model\n",
    "    model = Model(inputs=[inputs], outputs=[out])\n",
    "    model.summary()\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    return model\n",
    "\n",
    "model = build_CNN_model()\n",
    "history = train_model(model,'leishmaniasis',150,save_as='second_try')\n",
    "export(model,'second_try')\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al probar regularización de kernel y bias se obtienen mejores, aunque el modelo sigue en sobreajuste. \n",
    "\n",
    "Para mejorar los resultados, se intentará utilizar redes convolucionales ya creadas y entrenadas para identificar caracteristicas de las imagenes. El objetivo es reutilizar los filtros que se han entrenado a partir de datasets más diversos y más grandes en el contexto del problema actual; y ver si al reutilizar estos filtros se obtienen mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "### Definición de función para crear modelo\n",
    "\n",
    "Primero, se crea una función que reciba capas convolucionales, las congele todas y agregue capas de clasificación al final. Se imprimirá la información de cada capa y se correrá en dos GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds model for transfer learning and fine tunning\n",
    "def build_transfer_learning_model(conv_layers):\n",
    "    # Freeze conv layers that are not going to be trained\n",
    "    for layer in conv_layers.layers[:]:\n",
    "        layer.trainable = False \n",
    "    # Print summary of the layers\n",
    "    for layer in conv_layers.layers:\n",
    "        print(layer, layer.trainable)    \n",
    "    # Create sequential model\n",
    "    model = Sequential()\n",
    "    # add conv layers to model\n",
    "    model.add(conv_layers)\n",
    "    # Add clasification layers to model\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    model.summary()\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se intentará realizar el entrenamiento con cuatro redes convolucionales creadas de keras applications. Se cargaran los pesos de image net. No se incluirá la parte de clasificación de estas redes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 215s 4us/step\n",
      "<keras.engine.input_layer.InputLayer object at 0x7efd7e8e7278> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd84102f28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7ed12780> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd8415ed30> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7ed02438> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7e8a2550> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd7e807860> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7e8167f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7e830978> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7dedab38> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd7dec5d30> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7deef550> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7de9c358> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7de9cf98> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd7de49d68> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7de5fcf8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7de05588> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7de1d438> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd7de30e10> False\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "vgg16_input (InputLayer)        (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 224, 224, 3)  0           vgg16_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 224, 224, 3)  0           vgg16_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 1)            21137729    lambda_17[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Concatenate)          (None, 1)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 21,137,729\n",
      "Trainable params: 6,423,041\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 27s 1s/step - loss: 0.5865 - acc: 0.7408 - val_loss: 0.5025 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76733, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 23s 878ms/step - loss: 0.4730 - acc: 0.7814 - val_loss: 0.4642 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76733 to 0.76980, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 27s 1s/step - loss: 0.4569 - acc: 0.7956 - val_loss: 0.4430 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76980 to 0.78713, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.4639 - acc: 0.7866 - val_loss: 0.4394 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78713 to 0.78713, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4370 - acc: 0.7964 - val_loss: 0.4713 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.78713\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.4194 - acc: 0.8199 - val_loss: 0.4197 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.78713 to 0.79950, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 28s 1s/step - loss: 0.4039 - acc: 0.8187 - val_loss: 0.4359 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79950\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3899 - acc: 0.8361 - val_loss: 0.4165 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.79950 to 0.80941, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3792 - acc: 0.8355 - val_loss: 0.4045 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80941\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3803 - acc: 0.8343 - val_loss: 0.4032 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.80941\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3676 - acc: 0.8442 - val_loss: 0.4149 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.80941\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3479 - acc: 0.8560 - val_loss: 0.3925 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.80941 to 0.82921, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3481 - acc: 0.8410 - val_loss: 0.3944 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82921\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3613 - acc: 0.8441 - val_loss: 0.3898 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82921\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3336 - acc: 0.8646 - val_loss: 0.3940 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.82921\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3328 - acc: 0.8677 - val_loss: 0.4055 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.82921\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3249 - acc: 0.8713 - val_loss: 0.3896 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.82921\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3200 - acc: 0.8734 - val_loss: 0.3768 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.82921 to 0.83663, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3249 - acc: 0.8644 - val_loss: 0.4001 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83663\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3141 - acc: 0.8659 - val_loss: 0.3849 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83663\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3042 - acc: 0.8731 - val_loss: 0.3929 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83663\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2935 - acc: 0.8791 - val_loss: 0.3752 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83663\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3004 - acc: 0.8740 - val_loss: 0.3690 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83663\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2895 - acc: 0.8903 - val_loss: 0.3767 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83663\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2935 - acc: 0.8744 - val_loss: 0.4071 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83663\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3077 - acc: 0.8690 - val_loss: 0.3675 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83663\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2824 - acc: 0.8854 - val_loss: 0.4112 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83663\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3146 - acc: 0.8623 - val_loss: 0.5269 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83663\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2952 - acc: 0.8740 - val_loss: 0.3621 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83663\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2686 - acc: 0.8855 - val_loss: 0.3689 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83663\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2617 - acc: 0.8938 - val_loss: 0.3706 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83663\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2704 - acc: 0.8906 - val_loss: 0.3538 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83663\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2493 - acc: 0.9044 - val_loss: 0.3579 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.83663 to 0.84158, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2584 - acc: 0.8945 - val_loss: 0.3502 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.84158 to 0.85396, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2453 - acc: 0.9002 - val_loss: 0.3629 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.85396\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2433 - acc: 0.9149 - val_loss: 0.3535 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.85396\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2448 - acc: 0.8948 - val_loss: 0.3604 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.85396\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2381 - acc: 0.9059 - val_loss: 0.3455 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.85396\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2451 - acc: 0.9011 - val_loss: 0.3826 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.85396\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2490 - acc: 0.8993 - val_loss: 0.3518 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.85396\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2426 - acc: 0.9036 - val_loss: 0.3570 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.85396\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2382 - acc: 0.9047 - val_loss: 0.3491 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.85396\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2248 - acc: 0.9086 - val_loss: 0.3558 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.85396\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2203 - acc: 0.9134 - val_loss: 0.3437 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.85396 to 0.86386, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2201 - acc: 0.9173 - val_loss: 0.3884 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.86386\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2167 - acc: 0.9182 - val_loss: 0.3394 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.86386\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2110 - acc: 0.9242 - val_loss: 0.3406 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.86386\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2107 - acc: 0.9218 - val_loss: 0.3715 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.86386\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2129 - acc: 0.9176 - val_loss: 0.3437 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.86386\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2041 - acc: 0.9272 - val_loss: 0.3461 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.86386\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2094 - acc: 0.9242 - val_loss: 0.3484 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.86386\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2171 - acc: 0.9158 - val_loss: 0.3714 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.86386\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2218 - acc: 0.9149 - val_loss: 0.3546 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.86386\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1972 - acc: 0.9252 - val_loss: 0.3408 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.86386\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2022 - acc: 0.9257 - val_loss: 0.3357 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.86386\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1906 - acc: 0.9320 - val_loss: 0.3633 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.86386\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1861 - acc: 0.9362 - val_loss: 0.3347 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.86386\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1961 - acc: 0.9273 - val_loss: 0.3428 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.86386\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1797 - acc: 0.9414 - val_loss: 0.3442 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.86386\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1904 - acc: 0.9245 - val_loss: 0.3355 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.86386 to 0.86386, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1812 - acc: 0.9318 - val_loss: 0.3297 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.86386\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1803 - acc: 0.9357 - val_loss: 0.3345 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.86386\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1930 - acc: 0.9279 - val_loss: 0.3633 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.86386\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1810 - acc: 0.9335 - val_loss: 0.3341 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.86386\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1705 - acc: 0.9399 - val_loss: 0.3450 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.86386\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1618 - acc: 0.9422 - val_loss: 0.3323 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.86386 to 0.86881, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1696 - acc: 0.9459 - val_loss: 0.3476 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.86881\n",
      "Epoch 68/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 30s 1s/step - loss: 0.1620 - acc: 0.9501 - val_loss: 0.3563 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.86881\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1665 - acc: 0.9399 - val_loss: 0.3434 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.86881\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1721 - acc: 0.9356 - val_loss: 0.4049 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.86881\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1716 - acc: 0.9339 - val_loss: 0.3482 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.86881\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1649 - acc: 0.9428 - val_loss: 0.3498 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.86881\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1583 - acc: 0.9416 - val_loss: 0.3470 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.86881\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1486 - acc: 0.9543 - val_loss: 0.3341 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.86881\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1488 - acc: 0.9516 - val_loss: 0.3330 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.86881\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1539 - acc: 0.9435 - val_loss: 0.3715 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.86881\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1484 - acc: 0.9516 - val_loss: 0.3317 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.86881\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1515 - acc: 0.9429 - val_loss: 0.3301 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.86881\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1478 - acc: 0.9531 - val_loss: 0.3381 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.86881\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1387 - acc: 0.9531 - val_loss: 0.3435 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.86881\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1350 - acc: 0.9564 - val_loss: 0.3436 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.86881\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1391 - acc: 0.9570 - val_loss: 0.3283 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.86881\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1428 - acc: 0.9567 - val_loss: 0.3422 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.86881\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1310 - acc: 0.9558 - val_loss: 0.3520 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.86881\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1318 - acc: 0.9579 - val_loss: 0.3292 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00085: val_acc improved from 0.86881 to 0.87129, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1305 - acc: 0.9621 - val_loss: 0.4002 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.87129\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1434 - acc: 0.9558 - val_loss: 0.3388 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.87129\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1391 - acc: 0.9492 - val_loss: 0.3654 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.87129\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1265 - acc: 0.9585 - val_loss: 0.3345 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.87129\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1184 - acc: 0.9636 - val_loss: 0.3740 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.87129\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1220 - acc: 0.9597 - val_loss: 0.3638 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.87129\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1252 - acc: 0.9603 - val_loss: 0.3422 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.87129\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1245 - acc: 0.9561 - val_loss: 0.3432 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.87129\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1193 - acc: 0.9660 - val_loss: 0.3459 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.87129\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1201 - acc: 0.9645 - val_loss: 0.3400 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.87129\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1343 - acc: 0.9534 - val_loss: 0.3418 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.87129\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1236 - acc: 0.9636 - val_loss: 0.3919 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.87129\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1134 - acc: 0.9663 - val_loss: 0.3714 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.87129\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1155 - acc: 0.9654 - val_loss: 0.3424 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.87129\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1057 - acc: 0.9646 - val_loss: 0.3313 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.87129\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1085 - acc: 0.9696 - val_loss: 0.3368 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.87129\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1133 - acc: 0.9633 - val_loss: 0.3475 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.87129\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1094 - acc: 0.9687 - val_loss: 0.3374 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00103: val_acc improved from 0.87129 to 0.87376, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1078 - acc: 0.9717 - val_loss: 0.3762 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.87376\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1195 - acc: 0.9661 - val_loss: 0.3658 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.87376\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1141 - acc: 0.9678 - val_loss: 0.3421 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.87376\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1153 - acc: 0.9603 - val_loss: 0.3539 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.87376\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1070 - acc: 0.9675 - val_loss: 0.3538 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.87376\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1076 - acc: 0.9687 - val_loss: 0.3436 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.87376\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1112 - acc: 0.9645 - val_loss: 0.3809 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.87376\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0995 - acc: 0.9702 - val_loss: 0.3487 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.87376\n",
      "Epoch 112/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 30s 1s/step - loss: 0.0980 - acc: 0.9714 - val_loss: 0.3809 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.87376\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0968 - acc: 0.9684 - val_loss: 0.3404 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.87376\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1100 - acc: 0.9654 - val_loss: 0.3797 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.87376\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1080 - acc: 0.9639 - val_loss: 0.3427 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.87376\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0927 - acc: 0.9769 - val_loss: 0.3537 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.87376\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0910 - acc: 0.9747 - val_loss: 0.3691 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.87376\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0883 - acc: 0.9729 - val_loss: 0.3571 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.87376\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0944 - acc: 0.9733 - val_loss: 0.3920 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.87376\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0989 - acc: 0.9687 - val_loss: 0.3463 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00120: val_acc improved from 0.87376 to 0.87624, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0968 - acc: 0.9699 - val_loss: 0.3819 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.87624\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0918 - acc: 0.9687 - val_loss: 0.3540 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.87624\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1007 - acc: 0.9688 - val_loss: 0.3492 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.87624\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0915 - acc: 0.9735 - val_loss: 0.3514 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.87624\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0909 - acc: 0.9753 - val_loss: 0.3621 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.87624\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0842 - acc: 0.9807 - val_loss: 0.3579 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.87624\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0883 - acc: 0.9727 - val_loss: 0.3681 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.87624\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0809 - acc: 0.9727 - val_loss: 0.3555 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.87624\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0854 - acc: 0.9759 - val_loss: 0.3758 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.87624\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0809 - acc: 0.9777 - val_loss: 0.3606 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.87624\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0772 - acc: 0.9765 - val_loss: 0.3785 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.87624\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0755 - acc: 0.9814 - val_loss: 0.3909 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.87624\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0805 - acc: 0.9763 - val_loss: 0.3622 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.87624\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0938 - acc: 0.9727 - val_loss: 0.3685 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.87624\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0810 - acc: 0.9688 - val_loss: 0.3945 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.87624\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0751 - acc: 0.9820 - val_loss: 0.3708 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00136: val_acc improved from 0.87624 to 0.87871, saving model to src/trainingWeigths/best_transfer_VGG16.hdf5\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0725 - acc: 0.9820 - val_loss: 0.3675 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.87871\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0718 - acc: 0.9801 - val_loss: 0.3613 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.87871\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0766 - acc: 0.9820 - val_loss: 0.3585 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.87871\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0673 - acc: 0.9838 - val_loss: 0.3701 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.87871\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0749 - acc: 0.9781 - val_loss: 0.3841 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.87871\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0667 - acc: 0.9856 - val_loss: 0.3626 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.87871\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0885 - acc: 0.9730 - val_loss: 0.3685 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.87871\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0866 - acc: 0.9735 - val_loss: 0.4023 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.87871\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0703 - acc: 0.9783 - val_loss: 0.3743 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.87871\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0763 - acc: 0.9769 - val_loss: 0.4200 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.87871\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0695 - acc: 0.9844 - val_loss: 0.4057 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.87871\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0766 - acc: 0.9747 - val_loss: 0.3725 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.87871\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0678 - acc: 0.9814 - val_loss: 0.3666 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.87871\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0653 - acc: 0.9805 - val_loss: 0.4149 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.87871\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv_layers = VGG16(weights='imagenet',include_top=False,input_shape=(image_size, image_size, 3))\n",
    "model = build_transfer_learning_model(conv_layers)\n",
    "history = train_model(model,'leishmaniasis',150,save_as='transfer_VGG16')\n",
    "export(model, 'transfer_VGG16')\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 343s 4us/step\n",
      "<keras.engine.input_layer.InputLayer object at 0x7efd7e8e63c8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cc23358> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd8411f9e8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd7e8e7128> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cbc7588> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cbf6828> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd7cb883c8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cb99710> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cbb25f8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cb49e10> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cb74828> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd7cb02ef0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cb1ee48> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cac9a20> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cadc8d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7caf0eb8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd7ca9dcc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7cab3c50> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7ca5c320> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7ca5c9e8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7ca09d68> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd7ca34780> False\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "vgg19_input (InputLayer)        (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 224, 224, 3)  0           vgg19_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 224, 224, 3)  0           vgg19_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 1)            26447425    lambda_19[0][0]                  \n",
      "                                                                 lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Concatenate)          (None, 1)            0           sequential_2[1][0]               \n",
      "                                                                 sequential_2[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 26,447,425\n",
      "Trainable params: 6,423,041\n",
      "Non-trainable params: 20,024,384\n",
      "__________________________________________________________________________________________________\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 25s 978ms/step - loss: 0.5383 - acc: 0.7522 - val_loss: 0.4639 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.77970, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 22s 865ms/step - loss: 0.4683 - acc: 0.7805 - val_loss: 0.4522 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.77970\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 27s 1s/step - loss: 0.4551 - acc: 0.7898 - val_loss: 0.4431 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77970 to 0.78218, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 28s 1s/step - loss: 0.4424 - acc: 0.8018 - val_loss: 0.4345 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78218 to 0.79950, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 28s 1s/step - loss: 0.4354 - acc: 0.8136 - val_loss: 0.4179 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79950\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4183 - acc: 0.8130 - val_loss: 0.4143 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79950\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.4041 - acc: 0.8205 - val_loss: 0.4142 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79950\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4133 - acc: 0.8157 - val_loss: 0.4059 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79950\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.3894 - acc: 0.8271 - val_loss: 0.3960 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79950\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3920 - acc: 0.8215 - val_loss: 0.3939 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79950\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3779 - acc: 0.8319 - val_loss: 0.4115 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79950\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3933 - acc: 0.8223 - val_loss: 0.3893 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79950\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3655 - acc: 0.8289 - val_loss: 0.3834 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.79950 to 0.81931, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3580 - acc: 0.8398 - val_loss: 0.3819 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.81931\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3698 - acc: 0.8380 - val_loss: 0.3791 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.81931 to 0.83168, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3599 - acc: 0.8443 - val_loss: 0.4142 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.83168\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3787 - acc: 0.8440 - val_loss: 0.4123 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83168\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3466 - acc: 0.8428 - val_loss: 0.3838 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83168\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3444 - acc: 0.8506 - val_loss: 0.3887 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83168\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3620 - acc: 0.8400 - val_loss: 0.3779 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83168\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3424 - acc: 0.8581 - val_loss: 0.3802 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83168\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3220 - acc: 0.8647 - val_loss: 0.3781 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83168\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3241 - acc: 0.8605 - val_loss: 0.3700 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83168\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3100 - acc: 0.8734 - val_loss: 0.3700 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.83168 to 0.84158, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3214 - acc: 0.8626 - val_loss: 0.3836 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.84158 to 0.84653, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3278 - acc: 0.8628 - val_loss: 0.3703 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.84653\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3072 - acc: 0.8764 - val_loss: 0.3687 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.84653 to 0.85644, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3042 - acc: 0.8773 - val_loss: 0.3776 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.85644\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2975 - acc: 0.8671 - val_loss: 0.3674 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.85644\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3117 - acc: 0.8674 - val_loss: 0.3844 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.85644\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3051 - acc: 0.8726 - val_loss: 0.3880 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.85644\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2985 - acc: 0.8815 - val_loss: 0.3617 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.85644\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2925 - acc: 0.8800 - val_loss: 0.3642 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.85644\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3209 - acc: 0.8563 - val_loss: 0.3717 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.85644\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2967 - acc: 0.8740 - val_loss: 0.3616 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.85644\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3106 - acc: 0.8641 - val_loss: 0.4144 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.85644\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2843 - acc: 0.8815 - val_loss: 0.3628 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.85644\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2843 - acc: 0.8848 - val_loss: 0.3608 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.85644\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.2765 - acc: 0.8845 - val_loss: 0.3548 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.85644\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2728 - acc: 0.8885 - val_loss: 0.3591 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.85644\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2707 - acc: 0.8851 - val_loss: 0.3697 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.85644 to 0.85644, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.2697 - acc: 0.8978 - val_loss: 0.3933 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.85644\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2758 - acc: 0.8845 - val_loss: 0.4073 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.85644\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2621 - acc: 0.8878 - val_loss: 0.3550 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.85644\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2544 - acc: 0.8953 - val_loss: 0.3581 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.85644\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2556 - acc: 0.8912 - val_loss: 0.3805 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.85644\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2547 - acc: 0.8906 - val_loss: 0.3593 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.85644\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2581 - acc: 0.8912 - val_loss: 0.3580 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.85644 to 0.85644, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2470 - acc: 0.9002 - val_loss: 0.3662 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.85644\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2375 - acc: 0.9075 - val_loss: 0.3649 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.85644\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2428 - acc: 0.9116 - val_loss: 0.3595 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00051: val_acc improved from 0.85644 to 0.85891, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2466 - acc: 0.9026 - val_loss: 0.3563 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.85891\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2325 - acc: 0.9080 - val_loss: 0.3671 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.85891\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2322 - acc: 0.9026 - val_loss: 0.3561 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.85891\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2440 - acc: 0.9053 - val_loss: 0.3607 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.85891\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2414 - acc: 0.9101 - val_loss: 0.3529 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.85891\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2460 - acc: 0.8960 - val_loss: 0.3712 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.85891\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.2826 - acc: 0.8818 - val_loss: 0.4196 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.85891\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2582 - acc: 0.8999 - val_loss: 0.3565 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.85891\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2434 - acc: 0.8924 - val_loss: 0.3676 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.85891\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2323 - acc: 0.9059 - val_loss: 0.3597 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.85891\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.2185 - acc: 0.9101 - val_loss: 0.3786 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.85891\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2314 - acc: 0.9098 - val_loss: 0.3596 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.85891\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2111 - acc: 0.9245 - val_loss: 0.3568 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.85891\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2213 - acc: 0.9138 - val_loss: 0.3606 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.85891\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2329 - acc: 0.9095 - val_loss: 0.3591 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.85891 to 0.86139, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.2104 - acc: 0.9212 - val_loss: 0.3567 - val_acc: 0.8540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00067: val_acc did not improve from 0.86139\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2167 - acc: 0.9158 - val_loss: 0.3593 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.86139\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2106 - acc: 0.9224 - val_loss: 0.3624 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00069: val_acc improved from 0.86139 to 0.86634, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2073 - acc: 0.9249 - val_loss: 0.3840 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.86634\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2349 - acc: 0.9047 - val_loss: 0.3563 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.86634\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2140 - acc: 0.9074 - val_loss: 0.4100 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.86634\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2397 - acc: 0.9008 - val_loss: 0.3548 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.86634\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2174 - acc: 0.9146 - val_loss: 0.4465 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.86634\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2151 - acc: 0.9092 - val_loss: 0.3623 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.86634\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2161 - acc: 0.9089 - val_loss: 0.3835 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.86634\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2000 - acc: 0.9249 - val_loss: 0.3765 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.86634\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2161 - acc: 0.9125 - val_loss: 0.4173 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.86634\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2118 - acc: 0.9095 - val_loss: 0.4182 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.86634\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1976 - acc: 0.9230 - val_loss: 0.3785 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.86634\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2011 - acc: 0.9222 - val_loss: 0.4051 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.86634\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2069 - acc: 0.9098 - val_loss: 0.4524 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.86634\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2129 - acc: 0.9194 - val_loss: 0.3541 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.86634\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1861 - acc: 0.9384 - val_loss: 0.3690 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.86634\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1943 - acc: 0.9228 - val_loss: 0.4481 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.86634\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2004 - acc: 0.9221 - val_loss: 0.3685 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.86634\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1909 - acc: 0.9303 - val_loss: 0.3586 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.86634\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1738 - acc: 0.9380 - val_loss: 0.3953 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.86634\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1723 - acc: 0.9386 - val_loss: 0.3677 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.86634\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1898 - acc: 0.9276 - val_loss: 0.3597 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.86634\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1976 - acc: 0.9153 - val_loss: 0.3600 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.86634\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1715 - acc: 0.9344 - val_loss: 0.3663 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.86634\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1870 - acc: 0.9327 - val_loss: 0.4362 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.86634\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1795 - acc: 0.9344 - val_loss: 0.3695 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00094: val_acc improved from 0.86634 to 0.86634, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1745 - acc: 0.9357 - val_loss: 0.3717 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.86634\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1731 - acc: 0.9297 - val_loss: 0.3700 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.86634\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1765 - acc: 0.9297 - val_loss: 0.3754 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.86634\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1933 - acc: 0.9206 - val_loss: 0.3654 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.86634\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1793 - acc: 0.9315 - val_loss: 0.3747 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.86634\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1667 - acc: 0.9384 - val_loss: 0.3964 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.86634\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1708 - acc: 0.9384 - val_loss: 0.3869 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.86634\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2059 - acc: 0.9218 - val_loss: 0.3699 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00102: val_acc improved from 0.86634 to 0.86881, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1818 - acc: 0.9338 - val_loss: 0.4340 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.86881\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1919 - acc: 0.9203 - val_loss: 0.3661 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.86881\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 28s 1s/step - loss: 0.1569 - acc: 0.9447 - val_loss: 0.3786 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.86881\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1629 - acc: 0.9369 - val_loss: 0.3749 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.86881\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1530 - acc: 0.9468 - val_loss: 0.3677 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.86881\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1513 - acc: 0.9459 - val_loss: 0.4264 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.86881\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1740 - acc: 0.9309 - val_loss: 0.3711 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.86881\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1684 - acc: 0.9360 - val_loss: 0.3653 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.86881\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1445 - acc: 0.9483 - val_loss: 0.3842 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.86881\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1468 - acc: 0.9510 - val_loss: 0.3753 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.86881\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 28s 1s/step - loss: 0.1507 - acc: 0.9456 - val_loss: 0.3670 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.86881\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1487 - acc: 0.9456 - val_loss: 0.3710 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.86881\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1398 - acc: 0.9492 - val_loss: 0.3747 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.86881\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1593 - acc: 0.9410 - val_loss: 0.3895 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.86881\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1464 - acc: 0.9519 - val_loss: 0.3826 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.86881\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1386 - acc: 0.9525 - val_loss: 0.3657 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.86881\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1460 - acc: 0.9474 - val_loss: 0.4098 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.86881\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1525 - acc: 0.9414 - val_loss: 0.3726 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.86881\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1439 - acc: 0.9501 - val_loss: 0.3627 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.86881\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1368 - acc: 0.9543 - val_loss: 0.4044 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.86881\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1542 - acc: 0.9398 - val_loss: 0.3745 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.86881\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1373 - acc: 0.9501 - val_loss: 0.3782 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00124: val_acc improved from 0.86881 to 0.86881, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1349 - acc: 0.9537 - val_loss: 0.4186 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.86881\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1788 - acc: 0.9306 - val_loss: 0.4068 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.86881\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1345 - acc: 0.9513 - val_loss: 0.3910 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.86881\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1459 - acc: 0.9444 - val_loss: 0.4169 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.86881\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1417 - acc: 0.9519 - val_loss: 0.3812 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.86881\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1380 - acc: 0.9501 - val_loss: 0.3889 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.86881\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1413 - acc: 0.9498 - val_loss: 0.3937 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.86881\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1603 - acc: 0.9294 - val_loss: 0.3690 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.86881\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1370 - acc: 0.9411 - val_loss: 0.3921 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.86881\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1238 - acc: 0.9555 - val_loss: 0.3842 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.86881\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1408 - acc: 0.9495 - val_loss: 0.3850 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.86881\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1367 - acc: 0.9468 - val_loss: 0.3778 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.86881\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1287 - acc: 0.9504 - val_loss: 0.4012 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.86881\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1334 - acc: 0.9510 - val_loss: 0.3730 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00138: val_acc improved from 0.86881 to 0.87624, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1233 - acc: 0.9561 - val_loss: 0.3791 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.87624\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1257 - acc: 0.9567 - val_loss: 0.3800 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.87624\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1276 - acc: 0.9570 - val_loss: 0.3661 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.87624\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1227 - acc: 0.9537 - val_loss: 0.3768 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.87624\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1220 - acc: 0.9525 - val_loss: 0.3797 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.87624\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1193 - acc: 0.9525 - val_loss: 0.3892 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.87624\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1250 - acc: 0.9567 - val_loss: 0.3771 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00145: val_acc improved from 0.87624 to 0.88366, saving model to src/trainingWeigths/best_transfer_VGG19.hdf5\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1234 - acc: 0.9564 - val_loss: 0.3889 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.88366\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1166 - acc: 0.9633 - val_loss: 0.3725 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.88366\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1207 - acc: 0.9492 - val_loss: 0.4976 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.88366\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1588 - acc: 0.9279 - val_loss: 0.4687 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.88366\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.1402 - acc: 0.9422 - val_loss: 0.4589 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.88366\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv_layers = VGG19(weights='imagenet',include_top=False,input_shape=(image_size, image_size, 3))\n",
    "model = build_transfer_learning_model(conv_layers)\n",
    "history = train_model(model,'leishmaniasis',150,save_as='transfer_VGG19')\n",
    "export(model, 'transfer_VGG19')\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icesi/anaconda3/lib/python3.6/site-packages/keras_applications/resnet50.py:263: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7efd7efde940> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7efd7efdeba8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7efdec88> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd840a2860> False\n",
      "<keras.layers.core.Activation object at 0x7efd7efdedd8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7efd7ed81dd8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5fd96c18> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5fd479b0> False\n",
      "<keras.layers.core.Activation object at 0x7efd5fd36cf8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5fcd35f8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5fca4780> False\n",
      "<keras.layers.core.Activation object at 0x7efd5fca4828> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5fbc34e0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5fb976a0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5fbfdcf8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5fae1da0> False\n",
      "<keras.layers.merge.Add object at 0x7efd5fafec50> False\n",
      "<keras.layers.core.Activation object at 0x7efd5fa04860> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f9d6f98> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f9f1668> False\n",
      "<keras.layers.core.Activation object at 0x7efd5f9b5ef0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f9145f8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f8d1f28> False\n",
      "<keras.layers.core.Activation object at 0x7efd5f8ec550> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f805ba8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f7d54a8> False\n",
      "<keras.layers.merge.Add object at 0x7efd5f7d5da0> False\n",
      "<keras.layers.core.Activation object at 0x7efd5f71d2e8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f6c1978> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f6d71d0> False\n",
      "<keras.layers.core.Activation object at 0x7efd5f69ba58> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f66f9e8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f5c3208> False\n",
      "<keras.layers.core.Activation object at 0x7efd5f5c3940> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f573e48> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f4c9390> False\n",
      "<keras.layers.merge.Add object at 0x7efd5f4e0908> False\n",
      "<keras.layers.core.Activation object at 0x7efd5f44b7b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f414eb8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f4305c0> False\n",
      "<keras.layers.core.Activation object at 0x7efd5f3f2e48> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f348f28> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f31f6d8> False\n",
      "<keras.layers.core.Activation object at 0x7efd5f31fb70> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f2b7b00> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f2206d8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f20ac88> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f170d30> False\n",
      "<keras.layers.merge.Add object at 0x7efd5f10ebe0> False\n",
      "<keras.layers.core.Activation object at 0x7efd5f0957f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5f065ef0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5f0065f8> False\n",
      "<keras.layers.core.Activation object at 0x7efd5efc7e80> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5efa6588> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5ef62e80> False\n",
      "<keras.layers.core.Activation object at 0x7efd5ef004e0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5ee99b38> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5ee6f438> False\n",
      "<keras.layers.merge.Add object at 0x7efd5ee6fd30> False\n",
      "<keras.layers.core.Activation object at 0x7efd5ed86668> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5ed86470> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5ed714a8> False\n",
      "<keras.layers.core.Activation object at 0x7efd5ed31a58> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5ec86978> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5ec5e198> False\n",
      "<keras.layers.core.Activation object at 0x7efd5ec5e8d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5ebf7940> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5eb4e748> False\n",
      "<keras.layers.merge.Add object at 0x7efd5eb4ecc0> False\n",
      "<keras.layers.core.Activation object at 0x7efd5eaeae80> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5eab4eb8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5ea53550> False\n",
      "<keras.layers.core.Activation object at 0x7efd5ea16dd8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5e9e8e48> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5e942668> False\n",
      "<keras.layers.core.Activation object at 0x7efd5e942cc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5e8d8a90> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5e8acc18> False\n",
      "<keras.layers.merge.Add object at 0x7efd5e842668> False\n",
      "<keras.layers.core.Activation object at 0x7efd5dfc7b38> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5dfc7c50> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5dfb0940> False\n",
      "<keras.layers.core.Activation object at 0x7efd5df6edd8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5dedbe80> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5de9aa58> False\n",
      "<keras.layers.core.Activation object at 0x7efd5deb2550> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5ddc6748> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5dda16a0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5dda1208> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5dc917f0> False\n",
      "<keras.layers.merge.Add object at 0x7efd5dc91ef0> False\n",
      "<keras.layers.core.Activation object at 0x7efd840e4da0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd8413da20> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd8412af98> False\n",
      "<keras.layers.core.Activation object at 0x7efd840cff98> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd8410f160> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f02eaba51d0> False\n",
      "<keras.layers.core.Activation object at 0x7efd840110f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5fdd3518> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd84102b38> False\n",
      "<keras.layers.merge.Add object at 0x7efd7efde0b8> False\n",
      "<keras.layers.core.Activation object at 0x7efd7c07ccc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd7c3b32b0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd7e8e6898> False\n",
      "<keras.layers.core.Activation object at 0x7efd7e8ee160> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5d87f780> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5d857048> False\n",
      "<keras.layers.core.Activation object at 0x7efd5d857940> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5d7e8fd0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5d76e860> False\n",
      "<keras.layers.merge.Add object at 0x7efd5d73dbe0> False\n",
      "<keras.layers.core.Activation object at 0x7efd5d6d6780> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5d69ff60> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5d63d710> False\n",
      "<keras.layers.core.Activation object at 0x7efd5d62e390> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5d5d6f60> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5d5abc50> False\n",
      "<keras.layers.core.Activation object at 0x7efd5d541668> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5d4d89b0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5d4ae320> False\n",
      "<keras.layers.merge.Add object at 0x7efd5d4aeb38> False\n",
      "<keras.layers.core.Activation object at 0x7efd5d3c95f8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5d398940> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5d3ae630> False\n",
      "<keras.layers.core.Activation object at 0x7efd5d373be0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5d2c9f60> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5d24f828> False\n",
      "<keras.layers.core.Activation object at 0x7efd5d2a1be0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5d1ca518> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5d185e80> False\n",
      "<keras.layers.merge.Add object at 0x7efd5d1a36a0> False\n",
      "<keras.layers.core.Activation object at 0x7efd5d128c18> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5d128f28> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5d094630> False\n",
      "<keras.layers.core.Activation object at 0x7efd5d06d748> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5cfc0978> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5cf96320> False\n",
      "<keras.layers.core.Activation object at 0x7efd5cf1b5f8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5cec2518> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5ce819b0> False\n",
      "<keras.layers.merge.Add object at 0x7efd5ce9b668> False\n",
      "<keras.layers.core.Activation object at 0x7efd5ce1f978> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5ce1fa90> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5cd87198> False\n",
      "<keras.layers.core.Activation object at 0x7efd5cd6e2b0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5cd304e0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5cce5e80> False\n",
      "<keras.layers.core.Activation object at 0x7efd5cc836a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5cc26ba8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5cbfa9e8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5cbfa518> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5ca7c630> False\n",
      "<keras.layers.merge.Add object at 0x7efd5cae3e80> False\n",
      "<keras.layers.core.Activation object at 0x7efd5ca03b38> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5c9d6cc0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5c9f2518> False\n",
      "<keras.layers.core.Activation object at 0x7efd5c9b5ef0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5c91a8d0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5c8f0198> False\n",
      "<keras.layers.core.Activation object at 0x7efd5c8f0a90> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5c81ee80> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5c7d9978> False\n",
      "<keras.layers.merge.Add object at 0x7efd5c7f61d0> False\n",
      "<keras.layers.core.Activation object at 0x7efd5c7788d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5c7789e8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5c6e20b8> False\n",
      "<keras.layers.core.Activation object at 0x7efd5c64d208> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5c6084e0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5c5c5cf8> False\n",
      "<keras.layers.core.Activation object at 0x7efd5c5e27f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7efd5c4fcb00> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7efd5c4d3470> False\n",
      "<keras.layers.merge.Add object at 0x7efd5c4d3d68> False\n",
      "<keras.layers.core.Activation object at 0x7efd5c417390> False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "resnet50_input (InputLayer)     (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 224, 224, 3)  0           resnet50_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 224, 224, 3)  0           resnet50_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 1)            49278337    lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Concatenate)          (None, 1)            0           sequential_3[1][0]               \n",
      "                                                                 sequential_3[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 49,278,337\n",
      "Trainable params: 25,690,625\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7949 - acc: 0.7384 - val_loss: 0.6303 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_transfer_ResNet50.hdf5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 25s 975ms/step - loss: 0.3758 - acc: 0.8346 - val_loss: 0.5868 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3138 - acc: 0.8704 - val_loss: 0.5561 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2490 - acc: 0.8978 - val_loss: 0.5531 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2107 - acc: 0.9209 - val_loss: 0.5572 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1686 - acc: 0.9323 - val_loss: 0.5835 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76238\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1451 - acc: 0.9498 - val_loss: 0.5889 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76238\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1088 - acc: 0.9636 - val_loss: 0.5483 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76238\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0859 - acc: 0.9694 - val_loss: 0.5527 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76238\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0780 - acc: 0.9735 - val_loss: 0.5504 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76238\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0699 - acc: 0.9790 - val_loss: 0.5512 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76238\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0648 - acc: 0.9808 - val_loss: 0.6129 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76238\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0481 - acc: 0.9841 - val_loss: 0.7185 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_transfer_ResNet50.hdf5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0513 - acc: 0.9841 - val_loss: 0.6023 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76238\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0441 - acc: 0.9862 - val_loss: 0.6858 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76238\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0279 - acc: 0.9919 - val_loss: 0.6339 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76238\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0201 - acc: 0.9946 - val_loss: 0.7407 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76238\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0210 - acc: 0.9946 - val_loss: 0.7555 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76238\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0215 - acc: 0.9940 - val_loss: 0.7005 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76238\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0179 - acc: 0.9976 - val_loss: 0.6058 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76238\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0209 - acc: 0.9946 - val_loss: 0.7585 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76238\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0197 - acc: 0.9934 - val_loss: 0.7615 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76238\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0172 - acc: 0.9958 - val_loss: 0.8090 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76238\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0096 - acc: 0.9982 - val_loss: 0.8147 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76238\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0232 - acc: 0.9937 - val_loss: 0.6549 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76238\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0170 - acc: 0.9952 - val_loss: 0.6336 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76238\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0205 - acc: 0.9943 - val_loss: 0.5520 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76238\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0177 - acc: 0.9940 - val_loss: 0.7535 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76238\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0179 - acc: 0.9970 - val_loss: 0.6958 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76238\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0109 - acc: 0.9961 - val_loss: 0.6743 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76238\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0104 - acc: 0.9976 - val_loss: 0.6648 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76238\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0075 - acc: 0.9988 - val_loss: 0.7774 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76238\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.9500 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76238\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0058 - acc: 0.9988 - val_loss: 1.0349 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.76238\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0086 - acc: 0.9982 - val_loss: 0.9237 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.76238\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0073 - acc: 0.9982 - val_loss: 1.0530 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.76238\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.9605 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.76238\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.9914 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.76238\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.9216 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.76238\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.9559 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.76238\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0062 - acc: 0.9994 - val_loss: 0.9016 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.76238\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0075 - acc: 0.9994 - val_loss: 0.6626 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.76238\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0060 - acc: 0.9976 - val_loss: 0.7965 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.76238\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0143 - acc: 0.9943 - val_loss: 0.5986 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.76238\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0055 - acc: 0.9988 - val_loss: 0.6603 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.76238\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.8703 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.76238\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0113 - acc: 0.9967 - val_loss: 0.8888 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.76238\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0172 - acc: 0.9958 - val_loss: 0.5873 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.76238\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0075 - acc: 0.9988 - val_loss: 0.9057 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.76238\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0126 - acc: 0.9970 - val_loss: 0.9189 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.76238\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0069 - acc: 0.9994 - val_loss: 0.8635 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.76238\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0052 - acc: 0.9982 - val_loss: 1.0212 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.76238\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0059 - acc: 0.9982 - val_loss: 1.0860 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.76238\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0045 - acc: 0.9988 - val_loss: 1.0449 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.76238\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0050 - acc: 0.9994 - val_loss: 0.9122 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.76238\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0070 - acc: 0.9967 - val_loss: 0.7869 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.76238\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.9240 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.76238\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0235 - acc: 0.9898 - val_loss: 0.7940 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.76238\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0096 - acc: 0.9967 - val_loss: 0.8961 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.76238\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0084 - acc: 0.9976 - val_loss: 1.0484 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.76238\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0064 - acc: 0.9973 - val_loss: 1.0905 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.76238\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0098 - acc: 0.9955 - val_loss: 1.2479 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.76238\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0047 - acc: 0.9994 - val_loss: 1.0651 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.76238\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0104 - acc: 0.9967 - val_loss: 1.0915 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.76238\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0061 - acc: 0.9982 - val_loss: 0.9332 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.76238\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0063 - acc: 0.9982 - val_loss: 1.1440 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.76238\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0068 - acc: 0.9982 - val_loss: 1.2298 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.76238\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0043 - acc: 0.9982 - val_loss: 0.9888 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.76238\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0032 - acc: 0.9994 - val_loss: 0.9172 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.76238\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0055 - acc: 0.9982 - val_loss: 1.0984 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.76238\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0043 - acc: 0.9988 - val_loss: 1.1925 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.76238\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0025 - acc: 0.9994 - val_loss: 1.0930 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.76238\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0038 - acc: 0.9994 - val_loss: 1.1515 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.76238\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0027 - acc: 0.9994 - val_loss: 1.0165 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.76238\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0116 - acc: 0.9955 - val_loss: 1.2864 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.76238\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0035 - acc: 0.9982 - val_loss: 1.2501 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.76238\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0052 - acc: 0.9988 - val_loss: 1.5471 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.76238\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0034 - acc: 0.9994 - val_loss: 1.5456 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.76238\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0036 - acc: 0.9988 - val_loss: 1.4824 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.76238\n",
      "Epoch 80/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 30s 1s/step - loss: 0.0033 - acc: 1.0000 - val_loss: 1.3976 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.76238\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0050 - acc: 0.9988 - val_loss: 1.2183 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.76238\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0043 - acc: 0.9982 - val_loss: 1.4057 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.76238\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0021 - acc: 1.0000 - val_loss: 1.5250 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.76238\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0046 - acc: 0.9979 - val_loss: 1.3941 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.76238\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0040 - acc: 0.9994 - val_loss: 1.2732 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.76238\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0016 - acc: 0.9994 - val_loss: 1.3590 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.76238\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.5268 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.76238\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0042 - acc: 0.9982 - val_loss: 1.4128 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.76238\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0129 - acc: 0.9955 - val_loss: 1.5926 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.76238\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0131 - acc: 0.9943 - val_loss: 1.4313 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.76238\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0141 - acc: 0.9931 - val_loss: 1.6637 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.76238\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0121 - acc: 0.9970 - val_loss: 1.7600 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.76238\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0055 - acc: 0.9988 - val_loss: 1.5279 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.76238\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0026 - acc: 0.9994 - val_loss: 1.4867 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.76238\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0042 - acc: 0.9988 - val_loss: 1.5256 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.76238\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0024 - acc: 0.9994 - val_loss: 1.5925 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.76238\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0036 - acc: 0.9994 - val_loss: 1.3659 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.76238\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0106 - acc: 0.9973 - val_loss: 1.2837 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.76238\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0055 - acc: 0.9982 - val_loss: 1.2985 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.76238\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0028 - acc: 0.9982 - val_loss: 1.4241 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.76238\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0071 - acc: 0.9982 - val_loss: 1.4169 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.76238\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0021 - acc: 1.0000 - val_loss: 1.4582 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.76238\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0026 - acc: 0.9988 - val_loss: 1.4377 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.76238\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.4472 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.76238\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0144 - acc: 0.9943 - val_loss: 1.1138 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.76238\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0038 - acc: 0.9988 - val_loss: 1.0330 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.76238\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0045 - acc: 0.9988 - val_loss: 1.2124 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.76238\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.1349 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.76238\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0017 - acc: 1.0000 - val_loss: 1.1494 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.76238\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0016 - acc: 1.0000 - val_loss: 1.2125 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.76238\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0011 - acc: 1.0000 - val_loss: 1.1496 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.76238\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0020 - acc: 1.0000 - val_loss: 1.2046 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.76238\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0035 - acc: 0.9994 - val_loss: 1.1645 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.76238\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0036 - acc: 0.9988 - val_loss: 1.1582 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.76238\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0051 - acc: 0.9982 - val_loss: 1.2816 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.76238\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0046 - acc: 0.9988 - val_loss: 1.3214 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.76238\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0025 - acc: 0.9994 - val_loss: 1.1935 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.76238\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0043 - acc: 0.9973 - val_loss: 1.3526 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.76238\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0077 - acc: 0.9970 - val_loss: 1.1176 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.76238\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0110 - acc: 0.9982 - val_loss: 1.1873 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.76238\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0034 - acc: 1.0000 - val_loss: 1.2905 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.76238\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0033 - acc: 0.9994 - val_loss: 1.1110 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.76238\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0046 - acc: 0.9994 - val_loss: 1.0846 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.76238\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0022 - acc: 1.0000 - val_loss: 1.0040 - val_acc: 0.7624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00124: val_acc did not improve from 0.76238\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0162 - acc: 0.9937 - val_loss: 1.0146 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.76238\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0067 - acc: 0.9988 - val_loss: 1.0075 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.76238\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0035 - acc: 0.9994 - val_loss: 1.0625 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.76238\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.9288 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.76238\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0132 - acc: 0.9952 - val_loss: 0.9969 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.76238\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0056 - acc: 0.9967 - val_loss: 1.2526 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.76238\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.8455 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.76238\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0090 - acc: 0.9973 - val_loss: 1.0020 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.76238\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.9855 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.76238\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0214 - acc: 0.9910 - val_loss: 0.7645 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.76238\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0048 - acc: 0.9994 - val_loss: 0.9714 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.76238\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0080 - acc: 0.9967 - val_loss: 0.9114 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.76238\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0113 - acc: 0.9932 - val_loss: 1.1291 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.76238\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0245 - acc: 0.9946 - val_loss: 0.5490 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.76238\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0081 - acc: 0.9988 - val_loss: 0.6473 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.76238\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0174 - acc: 0.9934 - val_loss: 0.6399 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.76238\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.7931 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.76238\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.9150 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.76238\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0069 - acc: 0.9994 - val_loss: 0.8005 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.76238\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 6.1999e-04 - acc: 1.0000 - val_loss: 0.9332 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.76238\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.6603 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.76238\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0064 - acc: 0.9970 - val_loss: 0.8244 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.76238\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.8397 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.76238\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.9295 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.76238\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0013 - acc: 0.9994 - val_loss: 0.9747 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.76238\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.0080 - acc: 0.9955 - val_loss: 0.7283 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.76238\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv_layers = ResNet50(weights='imagenet',include_top=False,input_shape=(image_size, image_size, 3))\n",
    "model = build_transfer_learning_model(conv_layers)\n",
    "history = train_model(model,'leishmaniasis',150,save_as='transfer_ResNet50')\n",
    "export(model, 'transfer_ResNet50')\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f1106aff710> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f110b2146d8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f1106aff048> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f110742b780> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f11073ea128> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f1106b59358> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f1106b55278> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f1106b4a048> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f1107b61d68> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f1106ba36a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f1107e47be0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f1107e4e8d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f1107e4b9b0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f1106b31f28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f1107327d68> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f11072cd780> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f11072dce10> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f11072f4898> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f110729b898> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f11072b3828> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f1107244ef0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f1107273c18> False\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "vgg19_input (InputLayer)        (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_45 (Lambda)              (None, 224, 224, 3)  0           vgg19_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 224, 224, 3)  0           vgg19_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_23 (Sequential)      (None, 1)            26447425    lambda_45[0][0]                  \n",
      "                                                                 lambda_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Concatenate)          (None, 1)            0           sequential_23[1][0]              \n",
      "                                                                 sequential_23[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 26,447,425\n",
      "Trainable params: 6,423,041\n",
      "Non-trainable params: 20,024,384\n",
      "__________________________________________________________________________________________________\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 27s 1s/step - loss: 0.5184 - acc: 0.7600 - val_loss: 0.4962 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76485, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 22s 850ms/step - loss: 0.5080 - acc: 0.7479 - val_loss: 0.4825 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76485\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4872 - acc: 0.7604 - val_loss: 0.4822 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76485\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4799 - acc: 0.7613 - val_loss: 0.4683 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76485 to 0.76980, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4738 - acc: 0.7640 - val_loss: 0.4633 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.76980 to 0.77228, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4724 - acc: 0.7664 - val_loss: 0.4627 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.77228 to 0.78465, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4694 - acc: 0.7775 - val_loss: 0.4576 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.78465\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4660 - acc: 0.7796 - val_loss: 0.4542 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.78465\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4584 - acc: 0.7862 - val_loss: 0.4509 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.78465\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4621 - acc: 0.7709 - val_loss: 0.4498 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.78465\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4456 - acc: 0.7865 - val_loss: 0.4492 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.78465\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4608 - acc: 0.7796 - val_loss: 0.4473 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.78465 to 0.78960, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4432 - acc: 0.7865 - val_loss: 0.4455 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.78960\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4462 - acc: 0.7847 - val_loss: 0.4505 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.78960 to 0.80941, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4422 - acc: 0.7800 - val_loss: 0.4411 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80941\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4416 - acc: 0.7905 - val_loss: 0.4419 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.80941\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4476 - acc: 0.7736 - val_loss: 0.4494 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.80941\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4477 - acc: 0.7766 - val_loss: 0.4632 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.80941\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4343 - acc: 0.7956 - val_loss: 0.4466 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.80941\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4316 - acc: 0.7938 - val_loss: 0.4421 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.80941\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4330 - acc: 0.7904 - val_loss: 0.4380 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.80941\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4312 - acc: 0.7911 - val_loss: 0.4368 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.80941\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4273 - acc: 0.7976 - val_loss: 0.4431 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.80941\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4336 - acc: 0.7980 - val_loss: 0.4347 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.80941\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4241 - acc: 0.7958 - val_loss: 0.4371 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.80941\n",
      "Epoch 26/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 0.4219 - acc: 0.8058 - val_loss: 0.4402 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.80941\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4157 - acc: 0.8009 - val_loss: 0.4394 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.80941\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4268 - acc: 0.7862 - val_loss: 0.4361 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.80941\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4193 - acc: 0.8003 - val_loss: 0.4401 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.80941\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4052 - acc: 0.8118 - val_loss: 0.4322 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.80941\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4175 - acc: 0.7988 - val_loss: 0.4429 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.80941\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4249 - acc: 0.7965 - val_loss: 0.4740 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.80941\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4162 - acc: 0.8112 - val_loss: 0.4317 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.80941\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4191 - acc: 0.7974 - val_loss: 0.4334 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.80941 to 0.81188, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4137 - acc: 0.8109 - val_loss: 0.4312 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.81188\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4128 - acc: 0.8115 - val_loss: 0.4341 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.81188\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4070 - acc: 0.8040 - val_loss: 0.4374 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.81188\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4092 - acc: 0.8103 - val_loss: 0.4319 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.81188\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4089 - acc: 0.8103 - val_loss: 0.4275 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.81188\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4040 - acc: 0.8055 - val_loss: 0.4280 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.81188\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3945 - acc: 0.8082 - val_loss: 0.4321 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.81188\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3923 - acc: 0.8231 - val_loss: 0.4268 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.81188\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4070 - acc: 0.8064 - val_loss: 0.4293 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.81188\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4051 - acc: 0.8106 - val_loss: 0.4275 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.81188\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4009 - acc: 0.8142 - val_loss: 0.4285 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.81188\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3949 - acc: 0.8124 - val_loss: 0.4304 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.81188 to 0.81436, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3984 - acc: 0.8094 - val_loss: 0.4307 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.81436\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4043 - acc: 0.8037 - val_loss: 0.4331 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.81436\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3923 - acc: 0.8274 - val_loss: 0.4466 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.81436\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3995 - acc: 0.8073 - val_loss: 0.4294 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.81436\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3985 - acc: 0.8124 - val_loss: 0.4336 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.81436\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4063 - acc: 0.8106 - val_loss: 0.4302 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.81436\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3943 - acc: 0.8124 - val_loss: 0.4282 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.81436\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3883 - acc: 0.8180 - val_loss: 0.4269 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.81436\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3906 - acc: 0.8151 - val_loss: 0.4447 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.81436\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4066 - acc: 0.8067 - val_loss: 0.4266 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.81436\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3882 - acc: 0.8184 - val_loss: 0.4531 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.81436\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3991 - acc: 0.8150 - val_loss: 0.4255 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.81436\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3838 - acc: 0.8220 - val_loss: 0.4300 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.81436\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3844 - acc: 0.8217 - val_loss: 0.4266 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.81436\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3810 - acc: 0.8118 - val_loss: 0.4231 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.81436\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3853 - acc: 0.8202 - val_loss: 0.4227 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00062: val_acc improved from 0.81436 to 0.81683, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3818 - acc: 0.8284 - val_loss: 0.4382 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.81683\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3872 - acc: 0.8217 - val_loss: 0.4240 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.81683 to 0.82178, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3829 - acc: 0.8205 - val_loss: 0.4244 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.82178\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3882 - acc: 0.8235 - val_loss: 0.4228 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.82178\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3878 - acc: 0.8220 - val_loss: 0.4307 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.82178\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3773 - acc: 0.8271 - val_loss: 0.4211 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.82178\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3757 - acc: 0.8290 - val_loss: 0.4231 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.82178\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3695 - acc: 0.8289 - val_loss: 0.4248 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00070: val_acc improved from 0.82178 to 0.82426, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3827 - acc: 0.8263 - val_loss: 0.4353 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.82426\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3912 - acc: 0.8160 - val_loss: 0.4242 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00072: val_acc improved from 0.82426 to 0.83416, saving model to src/trainingWeigths/best_transfer_InceptionV3.h5\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3707 - acc: 0.8238 - val_loss: 0.4316 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.83416\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3901 - acc: 0.8164 - val_loss: 0.4241 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.83416\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3944 - acc: 0.8068 - val_loss: 0.5042 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.83416\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3883 - acc: 0.8163 - val_loss: 0.4311 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.83416\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3750 - acc: 0.8277 - val_loss: 0.4370 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.83416\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3669 - acc: 0.8316 - val_loss: 0.4374 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.83416\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3706 - acc: 0.8259 - val_loss: 0.4208 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.83416\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3651 - acc: 0.8286 - val_loss: 0.4280 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.83416\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3768 - acc: 0.8248 - val_loss: 0.4373 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.83416\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3738 - acc: 0.8229 - val_loss: 0.4272 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.83416\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3644 - acc: 0.8313 - val_loss: 0.4295 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.83416\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3688 - acc: 0.8298 - val_loss: 0.4236 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.83416\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3752 - acc: 0.8313 - val_loss: 0.4279 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.83416\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3782 - acc: 0.8206 - val_loss: 0.4244 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.83416\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3661 - acc: 0.8319 - val_loss: 0.4238 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.83416\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3835 - acc: 0.8192 - val_loss: 0.4569 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.83416\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3671 - acc: 0.8251 - val_loss: 0.4256 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.83416\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3534 - acc: 0.8400 - val_loss: 0.4252 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.83416\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3610 - acc: 0.8296 - val_loss: 0.4253 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.83416\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3649 - acc: 0.8326 - val_loss: 0.4284 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.83416\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3555 - acc: 0.8406 - val_loss: 0.4256 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.83416\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3578 - acc: 0.8368 - val_loss: 0.4283 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.83416\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3621 - acc: 0.8377 - val_loss: 0.4255 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.83416\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3464 - acc: 0.8412 - val_loss: 0.4416 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.83416\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3608 - acc: 0.8377 - val_loss: 0.4246 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.83416\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3580 - acc: 0.8367 - val_loss: 0.4421 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.83416\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3625 - acc: 0.8232 - val_loss: 0.4258 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.83416\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3636 - acc: 0.8320 - val_loss: 0.4292 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.83416\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3594 - acc: 0.8286 - val_loss: 0.4238 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.83416\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3548 - acc: 0.8379 - val_loss: 0.4383 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.83416\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3651 - acc: 0.8250 - val_loss: 0.4220 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.83416\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3663 - acc: 0.8329 - val_loss: 0.4307 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.83416\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3459 - acc: 0.8457 - val_loss: 0.4255 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.83416\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3444 - acc: 0.8476 - val_loss: 0.4352 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.83416\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3411 - acc: 0.8433 - val_loss: 0.4243 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.83416\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3470 - acc: 0.8410 - val_loss: 0.4272 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.83416\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3499 - acc: 0.8442 - val_loss: 0.4274 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.83416\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3743 - acc: 0.8277 - val_loss: 0.4655 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.83416\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3609 - acc: 0.8242 - val_loss: 0.4245 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.83416\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3490 - acc: 0.8311 - val_loss: 0.4248 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.83416\n",
      "Epoch 113/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 0.3611 - acc: 0.8382 - val_loss: 0.4404 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.83416\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3701 - acc: 0.8162 - val_loss: 0.4328 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.83416\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3479 - acc: 0.8427 - val_loss: 0.4254 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.83416\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3503 - acc: 0.8347 - val_loss: 0.4336 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.83416\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3498 - acc: 0.8389 - val_loss: 0.4227 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.83416\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3585 - acc: 0.8349 - val_loss: 0.4261 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.83416\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3404 - acc: 0.8371 - val_loss: 0.4268 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.83416\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3476 - acc: 0.8391 - val_loss: 0.4442 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.83416\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3522 - acc: 0.8358 - val_loss: 0.4270 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.83416\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3467 - acc: 0.8358 - val_loss: 0.4257 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.83416\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3448 - acc: 0.8445 - val_loss: 0.4259 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.83416\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3438 - acc: 0.8373 - val_loss: 0.4234 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.83416\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3422 - acc: 0.8458 - val_loss: 0.4752 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.83416\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3423 - acc: 0.8446 - val_loss: 0.4287 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.83416\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3269 - acc: 0.8476 - val_loss: 0.4294 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.83416\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3463 - acc: 0.8503 - val_loss: 0.4404 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.83416\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3569 - acc: 0.8391 - val_loss: 0.4435 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.83416\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3394 - acc: 0.8413 - val_loss: 0.4297 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.83416\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3496 - acc: 0.8388 - val_loss: 0.4312 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.83416\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3525 - acc: 0.8343 - val_loss: 0.4218 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.83416\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3476 - acc: 0.8428 - val_loss: 0.4280 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.83416\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3602 - acc: 0.8331 - val_loss: 0.4318 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.83416\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3385 - acc: 0.8472 - val_loss: 0.4365 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.83416\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3355 - acc: 0.8418 - val_loss: 0.4217 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.83416\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3400 - acc: 0.8452 - val_loss: 0.4206 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.83416\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3314 - acc: 0.8442 - val_loss: 0.4228 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.83416\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3277 - acc: 0.8382 - val_loss: 0.4688 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.83416\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3361 - acc: 0.8497 - val_loss: 0.4242 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.83416\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3304 - acc: 0.8529 - val_loss: 0.4232 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.83416\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3293 - acc: 0.8539 - val_loss: 0.4235 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.83416\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3201 - acc: 0.8559 - val_loss: 0.4267 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.83416\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3340 - acc: 0.8418 - val_loss: 0.4275 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.83416\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3399 - acc: 0.8413 - val_loss: 0.4307 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.83416\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3340 - acc: 0.8434 - val_loss: 0.4713 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.83416\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3409 - acc: 0.8436 - val_loss: 0.4261 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.83416\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3399 - acc: 0.8413 - val_loss: 0.4235 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.83416\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3208 - acc: 0.8436 - val_loss: 0.4522 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.83416\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3445 - acc: 0.8476 - val_loss: 0.4234 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.83416\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_transfer_learning_model(conv_layers)\n",
    "history = train_model(model,'leishmaniasis',150,save_as='transfer_InceptionV3')\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "A continuación se intentará realizar fine tuning con VGG19, que fue la red que mejor desempeñó en la predicción. Para esto se defina la función de construcción de modelo de fine tuning, congelando todas las capas menos las ultimas 5 (4 capas convolucionales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds model for transfer learning and fine tunning\n",
    "def build_fine_tuning_model(conv_layers):\n",
    "    # Freeze conv layers that are not going to be trained\n",
    "    for layer in conv_layers.layers[:-5]:\n",
    "        layer.trainable = False \n",
    "    # Print summary of the layers\n",
    "    for layer in conv_layers.layers:\n",
    "        print(layer, layer.trainable)    \n",
    "    # Create sequential model\n",
    "    model = Sequential()\n",
    "    # add conv layers to model\n",
    "    model.add(conv_layers)\n",
    "    # Add clasification layers to model\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    model.summary()\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar historia\n",
    "\n",
    "A partir de este punto se exportan las historias para su posterior \n",
    "analisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_history(history, save_as,epochs):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    file = open('src/history/history' + save_as+'.txt','w')\n",
    "    file.write('acc,val_acc'+'\\n')\n",
    "    for i in range(epochs):\n",
    "        file.write(str(acc[i])+','+str(val_acc[i])+'\\n')\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f10fcdac3c8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fcdac1d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fde56d68> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f10fd61b6a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fddde7b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fcd43f60> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f10fd5c1e10> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd5d6da0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd5fb518> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd5954a8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd5a9e80> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f10fd554898> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd565828> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd57d940> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd516f28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd4c0940> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f10fd4d0fd0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd4e7c50> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd495a58> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd4aa9e8> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f10fd456860> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f10fd468dd8> True\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "vgg19_input (InputLayer)        (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_47 (Lambda)              (None, 224, 224, 3)  0           vgg19_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_48 (Lambda)              (None, 224, 224, 3)  0           vgg19_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_24 (Sequential)      (None, 1)            26447425    lambda_47[0][0]                  \n",
      "                                                                 lambda_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Concatenate)          (None, 1)            0           sequential_24[1][0]              \n",
      "                                                                 sequential_24[2][0]              \n",
      "==================================================================================================\n",
      "Total params: 26,447,425\n",
      "Trainable params: 15,862,273\n",
      "Non-trainable params: 10,585,152\n",
      "__________________________________________________________________________________________________\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5848 - acc: 0.7373 - val_loss: 0.5018 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.5130 - acc: 0.7463 - val_loss: 0.4672 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4702 - acc: 0.7700 - val_loss: 0.4328 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.79455, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4254 - acc: 0.7853 - val_loss: 0.4580 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.79455 to 0.80941, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4097 - acc: 0.8148 - val_loss: 0.3907 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.80941 to 0.82673, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3941 - acc: 0.8256 - val_loss: 0.4259 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82673\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3648 - acc: 0.8493 - val_loss: 0.3789 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.82673 to 0.84406, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3253 - acc: 0.8598 - val_loss: 0.3450 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.84406 to 0.85644, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2887 - acc: 0.8719 - val_loss: 0.3813 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85644\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2767 - acc: 0.8882 - val_loss: 0.3881 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.85644\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2685 - acc: 0.8896 - val_loss: 0.3324 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.85644 to 0.85891, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2251 - acc: 0.9080 - val_loss: 0.4115 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85891\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2670 - acc: 0.8930 - val_loss: 0.3729 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.85891\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2265 - acc: 0.9149 - val_loss: 0.3553 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.85891 to 0.86139, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1864 - acc: 0.9249 - val_loss: 0.3630 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.86139\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1675 - acc: 0.9296 - val_loss: 0.4050 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.86139 to 0.87376, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1621 - acc: 0.9387 - val_loss: 0.3763 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.87376 to 0.88614, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1652 - acc: 0.9323 - val_loss: 0.3559 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.88614\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1412 - acc: 0.9390 - val_loss: 0.3902 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.88614 to 0.89356, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1178 - acc: 0.9558 - val_loss: 0.4634 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.89356\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1272 - acc: 0.9453 - val_loss: 0.3561 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.89356\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0818 - acc: 0.9675 - val_loss: 0.4024 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.89356\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0646 - acc: 0.9741 - val_loss: 0.6686 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.89356\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0816 - acc: 0.9699 - val_loss: 0.4670 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.89356\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0873 - acc: 0.9681 - val_loss: 0.3522 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.89356 to 0.89604, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0773 - acc: 0.9729 - val_loss: 0.3663 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.89604 to 0.91089, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0503 - acc: 0.9820 - val_loss: 0.4580 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.91089\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0353 - acc: 0.9832 - val_loss: 0.5409 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.91089\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0850 - acc: 0.9684 - val_loss: 0.6657 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.91089\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0872 - acc: 0.9696 - val_loss: 0.3619 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.91089\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0490 - acc: 0.9787 - val_loss: 0.3611 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.91089\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0472 - acc: 0.9844 - val_loss: 0.4958 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.91089\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0379 - acc: 0.9874 - val_loss: 0.5715 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.91089\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0565 - acc: 0.9763 - val_loss: 0.3894 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.91089\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0412 - acc: 0.9847 - val_loss: 0.6157 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.91089\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0584 - acc: 0.9795 - val_loss: 0.4238 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.91089\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0462 - acc: 0.9850 - val_loss: 0.5177 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.91089 to 0.91337, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0284 - acc: 0.9883 - val_loss: 0.7981 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.91337\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0638 - acc: 0.9793 - val_loss: 0.5091 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.91337\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0347 - acc: 0.9898 - val_loss: 0.4836 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.91337\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0218 - acc: 0.9916 - val_loss: 0.5210 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.91337\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0231 - acc: 0.9916 - val_loss: 0.6004 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.91337 to 0.91337, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0238 - acc: 0.9898 - val_loss: 0.5530 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.91337\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0142 - acc: 0.9952 - val_loss: 0.6167 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.91337\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0392 - acc: 0.9850 - val_loss: 0.6396 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.91337\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0761 - acc: 0.9727 - val_loss: 0.4485 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.91337\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0406 - acc: 0.9844 - val_loss: 0.4458 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.91337\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0171 - acc: 0.9940 - val_loss: 0.5138 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.91337\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0142 - acc: 0.9952 - val_loss: 0.5472 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.91337\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0120 - acc: 0.9970 - val_loss: 0.6575 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.91337\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0569 - acc: 0.9838 - val_loss: 0.4443 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.91337\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0358 - acc: 0.9886 - val_loss: 0.6193 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.91337\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0151 - acc: 0.9958 - val_loss: 0.6084 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.91337\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0101 - acc: 0.9958 - val_loss: 0.4995 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.91337\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0091 - acc: 0.9970 - val_loss: 0.5603 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.91337\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0067 - acc: 0.9970 - val_loss: 0.5333 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.91337\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0128 - acc: 0.9952 - val_loss: 0.5871 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.91337\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0153 - acc: 0.9940 - val_loss: 0.6665 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.91337\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0146 - acc: 0.9946 - val_loss: 0.6828 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.91337\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0229 - acc: 0.9910 - val_loss: 0.6202 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.91337\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0548 - acc: 0.9862 - val_loss: 0.4410 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.91337\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0343 - acc: 0.9868 - val_loss: 0.5271 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.91337\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0275 - acc: 0.9898 - val_loss: 0.4386 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.91337\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0139 - acc: 0.9964 - val_loss: 0.6569 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.91337\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0235 - acc: 0.9910 - val_loss: 0.4725 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00065: val_acc improved from 0.91337 to 0.91584, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0136 - acc: 0.9958 - val_loss: 0.5455 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.91584\n",
      "Epoch 67/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.0096 - acc: 0.9946 - val_loss: 0.5354 - val_acc: 0.9257\n",
      "\n",
      "Epoch 00067: val_acc improved from 0.91584 to 0.92574, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0097 - acc: 0.9952 - val_loss: 0.6772 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.92574\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.7272 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.92574\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0233 - acc: 0.9892 - val_loss: 0.4599 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.92574\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0121 - acc: 0.9940 - val_loss: 0.7607 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.92574\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0125 - acc: 0.9970 - val_loss: 0.6726 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.92574\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.5986 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.92574\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0026 - acc: 0.9988 - val_loss: 0.6603 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.92574\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0042 - acc: 0.9982 - val_loss: 0.8789 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.92574\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0214 - acc: 0.9910 - val_loss: 0.6505 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.92574\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0150 - acc: 0.9946 - val_loss: 0.7093 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.92574\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0058 - acc: 0.9988 - val_loss: 0.6892 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.92574\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0188 - acc: 0.9940 - val_loss: 0.6782 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.92574\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0405 - acc: 0.9856 - val_loss: 0.4762 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.92574\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0269 - acc: 0.9928 - val_loss: 0.5078 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.92574\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0113 - acc: 0.9982 - val_loss: 0.5129 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.92574\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0106 - acc: 0.9976 - val_loss: 0.5961 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.92574\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0067 - acc: 0.9970 - val_loss: 0.5286 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.92574\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0059 - acc: 0.9982 - val_loss: 0.5317 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.92574\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.5520 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.92574\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.5942 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.92574\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.6361 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.92574\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0102 - acc: 0.9964 - val_loss: 0.5655 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.92574\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0177 - acc: 0.9952 - val_loss: 0.7590 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.92574\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0148 - acc: 0.9964 - val_loss: 0.5545 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.92574\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0196 - acc: 0.9970 - val_loss: 0.5424 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.92574\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.4856 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.92574\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0199 - acc: 0.9925 - val_loss: 0.7791 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.92574\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0240 - acc: 0.9928 - val_loss: 0.4995 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.92574\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0141 - acc: 0.9940 - val_loss: 0.5826 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.92574\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0183 - acc: 0.9910 - val_loss: 0.6061 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.92574\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0189 - acc: 0.9928 - val_loss: 0.5807 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.92574\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0175 - acc: 0.9913 - val_loss: 0.7424 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.92574\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0350 - acc: 0.9916 - val_loss: 0.5316 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.92574\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0216 - acc: 0.9916 - val_loss: 0.6413 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.92574\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0126 - acc: 0.9946 - val_loss: 0.4755 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.92574\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.4997 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.92574\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0081 - acc: 0.9961 - val_loss: 0.5034 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.92574\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0314 - acc: 0.9871 - val_loss: 0.7712 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.92574\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0370 - acc: 0.9889 - val_loss: 0.4651 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.92574\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0075 - acc: 0.9988 - val_loss: 0.5379 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.92574\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0049 - acc: 0.9982 - val_loss: 0.5445 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.92574\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.6185 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.92574\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.6117 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.92574\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0105 - acc: 0.9949 - val_loss: 0.6491 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.92574\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0173 - acc: 0.9910 - val_loss: 0.5633 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.92574\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0218 - acc: 0.9940 - val_loss: 0.5542 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.92574\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.4763 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.92574\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0060 - acc: 0.9970 - val_loss: 0.5505 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.92574\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.5179 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.92574\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.5085 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.92574\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 7.7098e-04 - acc: 1.0000 - val_loss: 0.5183 - val_acc: 0.9282\n",
      "\n",
      "Epoch 00118: val_acc improved from 0.92574 to 0.92822, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7966e-04 - acc: 1.0000 - val_loss: 0.5466 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00119: val_acc improved from 0.92822 to 0.93069, saving model to src/trainingWeigths/best_fine_tuning_VGG19.h5\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.5103e-04 - acc: 1.0000 - val_loss: 0.5598 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.93069\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.7905 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.93069\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0067 - acc: 0.9976 - val_loss: 0.6832 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.93069\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.6159 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.93069\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.5590 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.93069\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.5433 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.93069\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 9.7347e-04 - acc: 1.0000 - val_loss: 0.7712 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.93069\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.6359e-04 - acc: 1.0000 - val_loss: 0.7141 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.93069\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4158e-04 - acc: 1.0000 - val_loss: 0.7116 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.93069\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 5.5302e-05 - acc: 1.0000 - val_loss: 0.7020 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.93069\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.7262e-05 - acc: 1.0000 - val_loss: 0.7016 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.93069\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.9225e-05 - acc: 1.0000 - val_loss: 0.7223 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.93069\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.7671e-05 - acc: 1.0000 - val_loss: 0.7249 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.93069\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.7119e-05 - acc: 1.0000 - val_loss: 0.7275 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.93069\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8815e-05 - acc: 1.0000 - val_loss: 0.7330 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.93069\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4412e-05 - acc: 1.0000 - val_loss: 0.7438 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.93069\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.4093e-05 - acc: 1.0000 - val_loss: 0.7387 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.93069\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7160e-05 - acc: 1.0000 - val_loss: 0.7459 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.93069\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.8285e-06 - acc: 1.0000 - val_loss: 0.7492 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.93069\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5303e-05 - acc: 1.0000 - val_loss: 0.7573 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.93069\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.1041e-04 - acc: 1.0000 - val_loss: 0.7241 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.93069\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.6526e-04 - acc: 1.0000 - val_loss: 0.8371 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.93069\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 4.8600e-05 - acc: 1.0000 - val_loss: 0.7862 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.93069\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.6941e-05 - acc: 1.0000 - val_loss: 0.7981 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.93069\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.3837e-05 - acc: 1.0000 - val_loss: 0.7831 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.93069\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2001e-05 - acc: 1.0000 - val_loss: 0.7818 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.93069\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6599e-05 - acc: 1.0000 - val_loss: 0.7830 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.93069\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 9.7978e-06 - acc: 1.0000 - val_loss: 0.7864 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.93069\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 9.7837e-06 - acc: 1.0000 - val_loss: 0.7913 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.93069\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2659e-05 - acc: 1.0000 - val_loss: 0.7939 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.93069\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.4705e-05 - acc: 1.0000 - val_loss: 0.7955 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.93069\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 150\n",
    "save_as = 'fine_tuning_VGG19'\n",
    "# Train model\n",
    "conv_layers = VGG19(weights='imagenet',include_top=False,input_shape=(224, 224, 3))\n",
    "model = build_fine_tuning_model(conv_layers)\n",
    "history = train_model(model,'leishmaniasis',epochs,save_as=save_as)\n",
    "# Export history\n",
    "export_history(history, save_as,epochs)\n",
    "# Plot accuracy\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG19 Sequential API para erxportar a Android"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import get_file \n",
    "\n",
    "# Builds model for transfer learning and fine tunning\n",
    "def build_VGG19_fine_tuning_model(last_trainable_layers):\n",
    "    model = Sequential()\n",
    "    # block 1\n",
    "    model.add(Conv2D(64, (3, 3),activation='relu',padding='same',name='block1_conv1',input_shape=(224,224,3)))\n",
    "    model.add(Conv2D(64, (3, 3),activation='relu',padding='same',name='block1_conv2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3, 3),activation='relu',padding='same',name='block2_conv1'))\n",
    "    model.add(Conv2D(128, (3, 3),activation='relu',padding='same',name='block2_conv2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv1'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv2'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv3'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv4'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv1'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv2'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv3'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv4'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n",
    "    # Block 5\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv1'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv2'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv3'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv4'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))\n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        cache_subdir='models',\n",
    "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
    "    model.load_weights(weights_path)\n",
    "    # Freeze conv layers that are not going to be trained\n",
    "    for layer in model.layers[:-last_trainable_layers]:\n",
    "        layer.trainable = False \n",
    "    # Print summary of the layers\n",
    "    for layer in model.layers:\n",
    "        print(layer, layer.trainable)  \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1,activation='sigmoid'))    \n",
    "    model.summary()\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.convolutional.Conv2D object at 0x7fc8cc121d30> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8cc121a90> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fc8ee1a00b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8cc1731d0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8effefbe0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fc8effd1b70> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8effdab00> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8eff8b358> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8eff8bef0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8effb6c18> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fc8eff62630> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8eff714a8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8eff0ad30> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8eff1ecc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8efec66d8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fc8efedbdd8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8efef3f98> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8efe9c7f0> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8efeb1780> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7fc8efe42e10> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7fc8efe72b70> True\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "block1_conv1_input (InputLayer) (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 224, 224, 3)  0           block1_conv1_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 224, 224, 3)  0           block1_conv1_input[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 1)            26447425    lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Concatenate)           (None, 1)            0           sequential_7[1][0]               \n",
      "                                                                 sequential_7[2][0]               \n",
      "==================================================================================================\n",
      "Total params: 26,447,425\n",
      "Trainable params: 15,862,273\n",
      "Non-trainable params: 10,585,152\n",
      "__________________________________________________________________________________________________\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.5865 - acc: 0.7475 - val_loss: 0.5096 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5010 - acc: 0.7487 - val_loss: 0.4603 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76485, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4638 - acc: 0.7829 - val_loss: 0.4245 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76485 to 0.78465, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4369 - acc: 0.7988 - val_loss: 0.4271 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78465 to 0.79950, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3994 - acc: 0.8201 - val_loss: 0.4078 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79950\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4039 - acc: 0.8208 - val_loss: 0.3934 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.79950 to 0.83911, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3798 - acc: 0.8388 - val_loss: 0.4042 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.83911\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3728 - acc: 0.8404 - val_loss: 0.3738 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.83911 to 0.85644, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3685 - acc: 0.8431 - val_loss: 0.3606 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.85644\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3148 - acc: 0.8731 - val_loss: 0.4521 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.85644\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3253 - acc: 0.8689 - val_loss: 0.3159 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.85644\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2659 - acc: 0.8851 - val_loss: 0.3863 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85644\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2879 - acc: 0.8614 - val_loss: 0.3433 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.85644\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2357 - acc: 0.8989 - val_loss: 0.3762 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.85644\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2154 - acc: 0.9128 - val_loss: 0.3830 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.85644 to 0.85891, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1944 - acc: 0.9113 - val_loss: 0.3416 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.85891 to 0.86386, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1830 - acc: 0.9192 - val_loss: 0.3514 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.86386 to 0.87624, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1531 - acc: 0.9368 - val_loss: 0.4629 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.87624\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1899 - acc: 0.9162 - val_loss: 0.5297 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.87624\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.1591 - acc: 0.9329 - val_loss: 0.5123 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87624\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1472 - acc: 0.9420 - val_loss: 0.4127 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.87624 to 0.88861, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1105 - acc: 0.9522 - val_loss: 0.4124 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88861\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1289 - acc: 0.9462 - val_loss: 0.3516 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.88861 to 0.90099, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1118 - acc: 0.9537 - val_loss: 0.3337 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.90099\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0865 - acc: 0.9687 - val_loss: 0.5098 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.90099\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.0782 - acc: 0.9702 - val_loss: 0.3971 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.90099\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0699 - acc: 0.9733 - val_loss: 0.4771 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.90099\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0646 - acc: 0.9747 - val_loss: 0.5072 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.90099\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0865 - acc: 0.9681 - val_loss: 0.6137 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.90099\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.0922 - acc: 0.9699 - val_loss: 0.4109 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.90099\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0728 - acc: 0.9705 - val_loss: 0.4014 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.90099 to 0.90842, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0504 - acc: 0.9820 - val_loss: 0.4327 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.90842\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0550 - acc: 0.9747 - val_loss: 0.6339 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.90842\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0537 - acc: 0.9763 - val_loss: 0.4706 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.90842\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.0590 - acc: 0.9769 - val_loss: 0.5388 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.90842\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0578 - acc: 0.9759 - val_loss: 0.5753 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.90842\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0956 - acc: 0.9684 - val_loss: 0.5736 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.90842\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0808 - acc: 0.9793 - val_loss: 0.4358 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.90842\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0408 - acc: 0.9853 - val_loss: 0.4806 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.90842\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0398 - acc: 0.9868 - val_loss: 0.4976 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.90842\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0302 - acc: 0.9880 - val_loss: 0.5318 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.90842\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0444 - acc: 0.9820 - val_loss: 0.5254 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.90842\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.0275 - acc: 0.9904 - val_loss: 0.6071 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.90842\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0255 - acc: 0.9910 - val_loss: 0.5657 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.90842\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0301 - acc: 0.9928 - val_loss: 0.6399 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.90842\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0324 - acc: 0.9850 - val_loss: 0.5045 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.90842 to 0.91832, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0418 - acc: 0.9853 - val_loss: 0.5452 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.91832\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0452 - acc: 0.9850 - val_loss: 0.5224 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.91832\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0790 - acc: 0.9708 - val_loss: 0.4138 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.91832\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0411 - acc: 0.9862 - val_loss: 0.4268 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.91832\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0283 - acc: 0.9910 - val_loss: 0.4653 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.91832\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0148 - acc: 0.9958 - val_loss: 0.4864 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.91832\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0221 - acc: 0.9910 - val_loss: 0.4707 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.91832\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0140 - acc: 0.9958 - val_loss: 0.6115 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.91832\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0156 - acc: 0.9958 - val_loss: 0.6602 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.91832\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0239 - acc: 0.9922 - val_loss: 0.6431 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.91832\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0181 - acc: 0.9928 - val_loss: 0.5520 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.91832\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0098 - acc: 0.9964 - val_loss: 0.6257 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.91832\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0192 - acc: 0.9946 - val_loss: 0.5858 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.91832\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0335 - acc: 0.9922 - val_loss: 0.5098 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.91832\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0133 - acc: 0.9964 - val_loss: 0.5325 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.91832 to 0.92079, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0197 - acc: 0.9913 - val_loss: 0.5678 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.92079\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0285 - acc: 0.9916 - val_loss: 0.5499 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.92079\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.0132 - acc: 0.9940 - val_loss: 0.5673 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.92079\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0102 - acc: 0.9976 - val_loss: 0.5870 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.92079\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0113 - acc: 0.9958 - val_loss: 0.5733 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.92079\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0160 - acc: 0.9949 - val_loss: 0.7667 - val_acc: 0.8787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00067: val_acc did not improve from 0.92079\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0333 - acc: 0.9868 - val_loss: 0.6475 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.92079\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0186 - acc: 0.9934 - val_loss: 0.7426 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.92079\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0816 - acc: 0.9687 - val_loss: 0.5435 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.92079\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0507 - acc: 0.9856 - val_loss: 0.5558 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.92079\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0187 - acc: 0.9928 - val_loss: 0.5236 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.92079\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0112 - acc: 0.9952 - val_loss: 0.5500 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.92079\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0137 - acc: 0.9946 - val_loss: 0.7087 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.92079\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.6343 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.92079\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0077 - acc: 0.9970 - val_loss: 0.5678 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.92079\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0076 - acc: 0.9970 - val_loss: 0.5574 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.92079\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0597 - acc: 0.9859 - val_loss: 0.4867 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.92079\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0708 - acc: 0.9777 - val_loss: 0.4960 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.92079\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0209 - acc: 0.9934 - val_loss: 0.4795 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.92079\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.5534 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.92079\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0220 - acc: 0.9952 - val_loss: 0.5229 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.92079\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0371 - acc: 0.9856 - val_loss: 0.4109 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.92079\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.5653 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.92079\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0096 - acc: 0.9958 - val_loss: 0.6319 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.92079\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0143 - acc: 0.9946 - val_loss: 0.6610 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.92079\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0176 - acc: 0.9934 - val_loss: 0.5526 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.92079\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.5990 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00088: val_acc improved from 0.92079 to 0.92079, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.6442 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.92079\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.5890 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.92079\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0056 - acc: 0.9988 - val_loss: 0.6244 - val_acc: 0.9257\n",
      "\n",
      "Epoch 00091: val_acc improved from 0.92079 to 0.92574, saving model to src/trainingWeigths/best_fine_tuning_VGG19_4.h5\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0047 - acc: 0.9970 - val_loss: 0.6472 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.92574\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0088 - acc: 0.9976 - val_loss: 0.6611 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.92574\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0027 - acc: 0.9994 - val_loss: 0.7455 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.92574\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.8074 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.92574\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0091 - acc: 0.9964 - val_loss: 0.7006 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.92574\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0060 - acc: 0.9976 - val_loss: 0.8634 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.92574\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0114 - acc: 0.9976 - val_loss: 0.6842 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.92574\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7144 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.92574\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.8819 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.92574\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0391 - acc: 0.9922 - val_loss: 0.4522 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.92574\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0166 - acc: 0.9970 - val_loss: 0.4611 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.92574\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0074 - acc: 0.9988 - val_loss: 0.5618 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.92574\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0141 - acc: 0.9946 - val_loss: 0.5177 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.92574\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0136 - acc: 0.9952 - val_loss: 0.6112 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.92574\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.6491 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.92574\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0135 - acc: 0.9958 - val_loss: 0.8825 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.92574\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0113 - acc: 0.9964 - val_loss: 0.8357 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.92574\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0085 - acc: 0.9970 - val_loss: 0.8418 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.92574\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0156 - acc: 0.9952 - val_loss: 0.6264 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.92574\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0145 - acc: 0.9964 - val_loss: 0.6994 - val_acc: 0.9084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00111: val_acc did not improve from 0.92574\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0206 - acc: 0.9964 - val_loss: 0.6073 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.92574\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0166 - acc: 0.9958 - val_loss: 0.6821 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.92574\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0322 - acc: 0.9904 - val_loss: 0.5147 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.92574\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0138 - acc: 0.9964 - val_loss: 0.6350 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.92574\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0093 - acc: 0.9970 - val_loss: 0.6267 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.92574\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0540 - acc: 0.9847 - val_loss: 0.6084 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.92574\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0240 - acc: 0.9904 - val_loss: 0.5969 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.92574\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0115 - acc: 0.9970 - val_loss: 0.6633 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.92574\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0456 - acc: 0.9868 - val_loss: 0.5573 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.92574\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0312 - acc: 0.9868 - val_loss: 0.5552 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.92574\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0057 - acc: 0.9988 - val_loss: 0.7537 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.92574\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0072 - acc: 0.9970 - val_loss: 0.6544 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.92574\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0037 - acc: 0.9982 - val_loss: 0.6264 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.92574\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.6095 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.92574\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.6915 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.92574\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.5937 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.92574\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0039 - acc: 0.9976 - val_loss: 0.5843 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.92574\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.6092e-04 - acc: 1.0000 - val_loss: 0.6206 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.92574\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.0723e-04 - acc: 1.0000 - val_loss: 0.6351 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.92574\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0012 - acc: 0.9994 - val_loss: 0.6813 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.92574\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.7540 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.92574\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.0454e-04 - acc: 1.0000 - val_loss: 0.7558 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.92574\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.8371e-04 - acc: 1.0000 - val_loss: 0.7429 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.92574\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0046 - acc: 0.9970 - val_loss: 0.7333 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.92574\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0243 - acc: 0.9910 - val_loss: 0.6702 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.92574\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0039 - acc: 0.9994 - val_loss: 0.7210 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.92574\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.8204 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.92574\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.4146e-04 - acc: 1.0000 - val_loss: 0.7766 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.92574\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0032 - acc: 0.9988 - val_loss: 0.7895 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.92574\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0099 - acc: 0.9976 - val_loss: 0.7872 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.92574\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0317 - acc: 0.9898 - val_loss: 0.7121 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.92574\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0667 - acc: 0.9795 - val_loss: 0.4775 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.92574\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0267 - acc: 0.9910 - val_loss: 0.6009 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.92574\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0076 - acc: 0.9970 - val_loss: 0.6586 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.92574\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0096 - acc: 0.9964 - val_loss: 0.6557 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.92574\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0073 - acc: 0.9982 - val_loss: 0.6198 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.92574\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0033 - acc: 0.9994 - val_loss: 0.7296 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.92574\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0108 - acc: 0.9964 - val_loss: 0.6806 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.92574\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.8312 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.92574\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 150\n",
    "save_as = 'fine_tuning_VGG19_5'\n",
    "model = build_VGG19_fine_tuning_model(5)\n",
    "history = train_model(model,'leishmaniasis',epochs,save_as=save_as)\n",
    "# Export history\n",
    "export_history(history, save_as,epochs)\n",
    "# Plot accuracy\n",
    "plt = get_plt(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar especificidady sensibilidad\n",
    "\n",
    "Procedemos a evaluar la sensibilidad y especirficidad del ultimo modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 404 images belonging to 2 classes.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.80      0.84        96\n",
      "        1.0       0.94      0.96      0.95       308\n",
      "\n",
      "avg / total       0.92      0.93      0.92       404\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Instanciate model\n",
    "model = load_model('src/trainingWeigths/best_fine_tuning_VGG19_5.h5')\n",
    "# Create data flow for test images\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.0)\n",
    "generator = test_datagen.flow_from_directory(\n",
    "        'src/leishmaniasis/test',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=1,\n",
    "        class_mode='binary',\n",
    "        shuffle=False)  # keep data in same order as labels\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "for i in range(404):\n",
    "    x, y = generator.next()\n",
    "    yz = model.predict(x)\n",
    "    if(yz[0][0]<0.5):\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)\n",
    "    y_true.append(y[0])\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas para evaluaciòn de 100 imagenes (Comparaciòn con expertos)\n",
    "\n",
    "Se realizo una prueba con 100 imagenes seleccionadas aleatoriamente del conjunto de prueba. A partir de estas 100 imagenes se espera comparar que tan bien desempeña el modelo en relación a expertos que realizan activamente diagnostico de leishmaniasis cutanea. El grupo de expertos esta compuesto por 5 medicos del cideim.\n",
    "\n",
    "A continuación se carga el modelo con mayor exactitud hasta el momento (fine_tuning_vgg19) y se obtienen las metricas asociadas a las predicciones de este modelo.\n",
    "\n",
    "### evaluar exactitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38378087258839516, 0.95]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# Instanciate model\n",
    "model = load_model('src/trainingWeigths/best_fine_tuning_VGG19_5.h5')\n",
    "# Create data flow for test images\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.0)\n",
    "generator = test_datagen.flow_from_directory(\n",
    "        'src/leishmaniasis_test',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=16,\n",
    "        class_mode='binary',\n",
    "        shuffle=False)  # keep data in same order as labels\n",
    "model.evaluate_generator(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar sensibilidad y especificidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 2 classes.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.83      0.89        24\n",
      "        1.0       0.95      0.99      0.97        76\n",
      "\n",
      "avg / total       0.95      0.95      0.95       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Instanciate model\n",
    "model = load_model('src/trainingWeigths/best_fine_tuning_VGG19_5.h5')\n",
    "# Create data flow for test images\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.0)\n",
    "generator = test_datagen.flow_from_directory(\n",
    "        'src/leishmaniasis_test',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=1,\n",
    "        class_mode='binary',\n",
    "        shuffle=False)  # keep data in same order as labels\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "for i in range(100):\n",
    "    x, y = generator.next()\n",
    "    yz = model.predict(x)\n",
    "    if(yz[0][0]<0.5):\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        y_pred.append(1)\n",
    "    y_true.append(y[0])\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  4]\n",
      " [ 1 75]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(y_true,y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Pretty print\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "class_names  = ['Negative','Positive']\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks,class_names)\n",
    "plt.yticks(tick_marks,class_names)\n",
    "s = [['TN','FP'],['FN','TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i,str(s[i][j])+ \" = \"+ str(cm[i][j]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning + Regularización + Dropout\n",
    "\n",
    "Con el fin de combatir el sobre ajuste que se ha presentado a lo large del experimento, se  agrega regularización l2 en la últimas capas convolucionales (las capas que se entrenarán), y se prueban diferentes valores de dropout con el fin de maximizar el error de validación.\n",
    "\n",
    "primero, definimos una nueva función pra construir el modelo, recibiendo por parametro el valor de regularización y dropout, esto con el fin de tratar diferentes valores en un solo experimento.\n",
    "\n",
    "**nota:** se entrenan las últimas cinco capas convolucionales dado que experimentos anteriores demuestran que es el nùmero adecuado de capas a entrenar para el problema en cuestión. \n",
    "\n",
    "### primer intento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds model for transfer learning and fine tunning\n",
    "#  PARAMS\n",
    "#     dropout: dropout value for dropout layers\n",
    "#     regularization: regularization \n",
    "def build_VGG19_fine_tuning_model(dropout, regularization):\n",
    "    model = Sequential()\n",
    "    # block 1\n",
    "    model.add(Conv2D(64, (3, 3),activation='relu',padding='same',name='block1_conv1',input_shape=(224,224,3)))\n",
    "    model.add(Conv2D(64, (3, 3),activation='relu',padding='same',name='block1_conv2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3, 3),activation='relu',padding='same',name='block2_conv1'))\n",
    "    model.add(Conv2D(128, (3, 3),activation='relu',padding='same',name='block2_conv2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv1'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv2'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv3'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv4'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv1'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv2'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv3'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv4'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n",
    "    # Block 5\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv1',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv2',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv3',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv4',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))\n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        cache_subdir='models',\n",
    "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
    "    model.load_weights(weights_path)\n",
    "    # Freeze conv layers that are not going to be trained\n",
    "    for layer in model.layers[:-5]:\n",
    "        layer.trainable = False \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1,activation='sigmoid'))    \n",
    "    # Multigpu model to speed up\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    return model  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo, definimos los diferentes niveles de regularización y dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "regularization = [0.001, 0.05, 0.01, 0.5, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tercero, deifnimos la función de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model\n",
    "#     PARAMS:\n",
    "#        optimizer: optimizer to train model\n",
    "#        model: model to be trained\n",
    "#        dataset: dataset to train modelo \n",
    "#        epoochs: number of epochs to train model\n",
    "#        save_as: name to save best weigths\n",
    "#     RETURNS\n",
    "#        history: vaules for acc and val_acc for each epoch\n",
    "def train_model(optimizer,model,dataset,epochs,save_as,image_size=224,batch_size=64):\n",
    "    # Data generator to  rescale training images\n",
    "    # Data Augmentation: Horizontal and vertical flips\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255.0,\n",
    "                                      vertical_flip=True,\n",
    "                                      horizontal_flip=True)\n",
    "    # Data generator to rescale test images\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255.0)\n",
    "    # Data flow training images\n",
    "    train_flow = train_datagen.flow_from_directory(\n",
    "        directory='src/'+dataset+'/training',  \n",
    "        target_size=(image_size, image_size),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    # Data flow test images\n",
    "    test_flow = test_datagen.flow_from_directory(\n",
    "        directory='src/'+dataset+'/test',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['acc'])\n",
    "    # Create check point call back to store best validation weigths\n",
    "    bestWeigthsPath='src/trainingWeigths/best_' + save_as+'.h5'\n",
    "    checkpoint = ModelCheckpoint(bestWeigthsPath, monitor='val_acc',save_weights_only=False, verbose=1, save_best_only=True, mode='max')\n",
    "    # Run experiment\n",
    "    history = model.fit_generator(\n",
    "            generator=train_flow,\n",
    "            epochs=epochs,\n",
    "            validation_data=test_flow,\n",
    "            callbacks=[checkpoint],\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuarto, definimos función para exportar historia de cada entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_history(history, save_as,epochs):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    file = open('src/history/history' + save_as+'.txt','w')\n",
    "    file.write('acc,val_acc'+'\\n')\n",
    "    for i in range(epochs):\n",
    "        file.write(str(acc[i])+','+str(val_acc[i])+'\\n')\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quinto, ejecutamos el experimento para cada combinación de dropout y regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment: dropout=0.1 regularization0.001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 27s 1s/step - loss: 1.3617 - acc: 0.7538 - val_loss: 1.2647 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75990, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2210 - acc: 0.7625 - val_loss: 1.1465 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75990 to 0.77475, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.1094 - acc: 0.7811 - val_loss: 1.0638 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77475 to 0.79703, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.0193 - acc: 0.8046 - val_loss: 1.0187 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79703\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.9867 - acc: 0.8016 - val_loss: 0.9555 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.79703 to 0.81931, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.8923 - acc: 0.8352 - val_loss: 0.9086 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.81931\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.8406 - acc: 0.8572 - val_loss: 0.8729 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.81931\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.8434 - acc: 0.8337 - val_loss: 0.9078 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.81931 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.8026 - acc: 0.8575 - val_loss: 0.7978 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.84158\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.7221 - acc: 0.8797 - val_loss: 0.9801 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.84158\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.7311 - acc: 0.8791 - val_loss: 0.8550 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.84158 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6717 - acc: 0.8930 - val_loss: 0.8194 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.84158 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6526 - acc: 0.8994 - val_loss: 0.7565 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.85396 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6410 - acc: 0.9083 - val_loss: 0.7612 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.86634 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.6050 - acc: 0.9140 - val_loss: 0.7424 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.86634\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5802 - acc: 0.9317 - val_loss: 0.7609 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86634\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5489 - acc: 0.9408 - val_loss: 0.7862 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.86634 to 0.86881, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5394 - acc: 0.9372 - val_loss: 0.7537 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.86881 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5089 - acc: 0.9549 - val_loss: 0.7778 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.87624 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.5496 - acc: 0.9303 - val_loss: 0.7760 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87624\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5120 - acc: 0.9432 - val_loss: 0.7538 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87624\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4693 - acc: 0.9612 - val_loss: 0.7527 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.87624 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4779 - acc: 0.9642 - val_loss: 0.7803 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.89356\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4592 - acc: 0.9633 - val_loss: 0.7409 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.89356 to 0.89604, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4821 - acc: 0.9546 - val_loss: 0.7465 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.89604\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4683 - acc: 0.9615 - val_loss: 0.7925 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.89604\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4402 - acc: 0.9747 - val_loss: 0.7773 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.89604\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4188 - acc: 0.9753 - val_loss: 0.8284 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.89604\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4292 - acc: 0.9666 - val_loss: 0.8123 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.89604\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4191 - acc: 0.9699 - val_loss: 0.8406 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.89604\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.4008 - acc: 0.9799 - val_loss: 0.8458 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.89604\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3868 - acc: 0.9856 - val_loss: 0.7825 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.89604\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4007 - acc: 0.9801 - val_loss: 1.0109 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.89604\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3885 - acc: 0.9832 - val_loss: 0.7211 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.89604 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3978 - acc: 0.9787 - val_loss: 0.7393 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.89851 to 0.90347, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3909 - acc: 0.9838 - val_loss: 0.7442 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.90347\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3596 - acc: 0.9910 - val_loss: 0.7534 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.90347 to 0.91089, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3692 - acc: 0.9880 - val_loss: 0.8922 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.91089\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3803 - acc: 0.9789 - val_loss: 0.7767 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.91089\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3816 - acc: 0.9814 - val_loss: 0.8078 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.91089\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3595 - acc: 0.9892 - val_loss: 0.7927 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.91089\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4036 - acc: 0.9723 - val_loss: 0.7799 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.91089\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3882 - acc: 0.9814 - val_loss: 0.7180 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.91089\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.3607 - acc: 0.9892 - val_loss: 0.8276 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.91089\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3501 - acc: 0.9934 - val_loss: 0.7800 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.91089\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3314 - acc: 0.9958 - val_loss: 0.7880 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.91089\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3286 - acc: 0.9970 - val_loss: 0.8456 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.91089\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3339 - acc: 0.9940 - val_loss: 0.8783 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.91089\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3479 - acc: 0.9844 - val_loss: 0.7093 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.91089\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3580 - acc: 0.9793 - val_loss: 0.9059 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.91089\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4221 - acc: 0.9573 - val_loss: 0.8508 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.91089\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3562 - acc: 0.9856 - val_loss: 0.8148 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.91089\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3538 - acc: 0.9826 - val_loss: 0.9962 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.91089\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3448 - acc: 0.9862 - val_loss: 0.7831 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.91089\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.3516 - acc: 0.9898 - val_loss: 0.7540 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.91089\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3280 - acc: 0.9940 - val_loss: 0.8542 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.91089\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3408 - acc: 0.9874 - val_loss: 0.9702 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.91089\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3518 - acc: 0.9844 - val_loss: 0.7827 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.91089\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3175 - acc: 0.9964 - val_loss: 0.8757 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.91089\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3263 - acc: 0.9916 - val_loss: 0.9042 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.91089\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3253 - acc: 0.9892 - val_loss: 0.8315 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.91089\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3032 - acc: 0.9982 - val_loss: 0.7797 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.91089\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3043 - acc: 0.9976 - val_loss: 0.8767 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.91089\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2990 - acc: 0.9976 - val_loss: 0.9524 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.91089\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2929 - acc: 1.0000 - val_loss: 0.8268 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.91089\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2899 - acc: 0.9994 - val_loss: 0.8935 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.91089\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2886 - acc: 0.9994 - val_loss: 0.8045 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.91089\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2896 - acc: 0.9982 - val_loss: 1.0691 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.91089\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3014 - acc: 0.9898 - val_loss: 1.0532 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.91089\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3056 - acc: 0.9883 - val_loss: 0.8935 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.91089\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3558 - acc: 0.9729 - val_loss: 1.0704 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.91089\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3449 - acc: 0.9753 - val_loss: 0.7544 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.91089\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3034 - acc: 0.9934 - val_loss: 0.8487 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.91089\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2949 - acc: 0.9955 - val_loss: 0.8149 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.91089\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3192 - acc: 0.9874 - val_loss: 0.9053 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.91089\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.2999 - acc: 0.9946 - val_loss: 0.7930 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.91089\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2937 - acc: 0.9952 - val_loss: 0.8448 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.91089\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2877 - acc: 0.9970 - val_loss: 0.7908 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.91089\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2796 - acc: 0.9994 - val_loss: 0.8560 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.91089\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2778 - acc: 0.9994 - val_loss: 0.8456 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.91089\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2773 - acc: 0.9982 - val_loss: 0.8930 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.91089\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2843 - acc: 0.9964 - val_loss: 0.8916 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.91089\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 36s 1s/step - loss: 0.2823 - acc: 0.9946 - val_loss: 0.8344 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.91089\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2891 - acc: 0.9934 - val_loss: 0.8412 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.91089\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2936 - acc: 0.9913 - val_loss: 0.7499 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.91089\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2812 - acc: 0.9946 - val_loss: 0.7375 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.91089\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2759 - acc: 0.9958 - val_loss: 0.8218 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.91089\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2786 - acc: 0.9964 - val_loss: 0.9156 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.91089\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2802 - acc: 0.9916 - val_loss: 0.8937 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.91089\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2964 - acc: 0.9898 - val_loss: 0.8125 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.91089\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2858 - acc: 0.9916 - val_loss: 0.8711 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91089\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2986 - acc: 0.9904 - val_loss: 0.6706 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91089\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2842 - acc: 0.9904 - val_loss: 0.7818 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.91089\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2761 - acc: 0.9946 - val_loss: 0.6913 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.91089\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2877 - acc: 0.9895 - val_loss: 1.0104 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.91089\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2917 - acc: 0.9892 - val_loss: 0.8693 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91089\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2694 - acc: 0.9946 - val_loss: 0.7372 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00097: val_acc improved from 0.91089 to 0.91337, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2637 - acc: 0.9970 - val_loss: 0.8701 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.91337\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2628 - acc: 0.9976 - val_loss: 0.7692 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91337\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2596 - acc: 0.9976 - val_loss: 0.7548 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.91337\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2705 - acc: 0.9934 - val_loss: 0.8015 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.91337\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2663 - acc: 0.9952 - val_loss: 0.7662 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.91337\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2651 - acc: 0.9928 - val_loss: 0.8363 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.91337\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2586 - acc: 0.9988 - val_loss: 0.7805 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.91337\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2608 - acc: 0.9976 - val_loss: 0.8025 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.91337\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2584 - acc: 0.9946 - val_loss: 0.9849 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.91337\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2585 - acc: 0.9958 - val_loss: 0.9689 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.91337\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2551 - acc: 0.9976 - val_loss: 0.8872 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.91337\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2481 - acc: 0.9988 - val_loss: 0.9231 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.91337\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2523 - acc: 0.9946 - val_loss: 0.8899 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.91337\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2803 - acc: 0.9859 - val_loss: 0.8617 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.91337\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3044 - acc: 0.9769 - val_loss: 0.8702 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.91337\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2810 - acc: 0.9847 - val_loss: 0.7244 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.91337\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2664 - acc: 0.9898 - val_loss: 0.8106 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.91337\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2621 - acc: 0.9916 - val_loss: 0.8073 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.91337\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2656 - acc: 0.9916 - val_loss: 0.7983 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.91337\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2614 - acc: 0.9910 - val_loss: 0.7987 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.91337\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2785 - acc: 0.9856 - val_loss: 1.1098 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.91337\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2650 - acc: 0.9922 - val_loss: 0.8502 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.91337\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2527 - acc: 0.9940 - val_loss: 0.8526 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.91337\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2499 - acc: 0.9958 - val_loss: 0.7359 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.91337\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2506 - acc: 0.9970 - val_loss: 0.9258 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.91337\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2455 - acc: 0.9964 - val_loss: 0.8010 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.91337\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2467 - acc: 0.9958 - val_loss: 0.8536 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.91337\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3072 - acc: 0.9781 - val_loss: 0.7510 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.91337\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2549 - acc: 0.9946 - val_loss: 0.6905 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.91337\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2493 - acc: 0.9934 - val_loss: 0.7173 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00127: val_acc improved from 0.91337 to 0.91832, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2476 - acc: 0.9964 - val_loss: 0.7567 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.91832\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2370 - acc: 1.0000 - val_loss: 0.8318 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.91832\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2389 - acc: 0.9982 - val_loss: 0.8165 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.91832\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2373 - acc: 0.9982 - val_loss: 0.7606 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.91832\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2331 - acc: 0.9994 - val_loss: 0.8851 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.91832\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2308 - acc: 0.9994 - val_loss: 0.7914 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.91832\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2287 - acc: 1.0000 - val_loss: 0.8530 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.91832\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2278 - acc: 0.9988 - val_loss: 0.8602 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.91832\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2269 - acc: 0.9988 - val_loss: 0.8419 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.91832\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2248 - acc: 1.0000 - val_loss: 0.7735 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.91832\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2229 - acc: 1.0000 - val_loss: 0.7842 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.91832\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2219 - acc: 1.0000 - val_loss: 0.8017 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.91832\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2203 - acc: 1.0000 - val_loss: 0.7931 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.91832\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2190 - acc: 1.0000 - val_loss: 0.7988 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.91832\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2177 - acc: 1.0000 - val_loss: 0.8024 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.91832\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2165 - acc: 1.0000 - val_loss: 0.8024 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.91832\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2152 - acc: 1.0000 - val_loss: 0.7959 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00144: val_acc improved from 0.91832 to 0.91832, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001.h5\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2140 - acc: 1.0000 - val_loss: 0.7980 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.91832\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2128 - acc: 1.0000 - val_loss: 0.8062 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.91832\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2116 - acc: 1.0000 - val_loss: 0.8095 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.91832\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2105 - acc: 1.0000 - val_loss: 0.8001 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.91832\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2099 - acc: 1.0000 - val_loss: 0.7841 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.91832\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2082 - acc: 1.0000 - val_loss: 0.7901 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.91832\n",
      "experiment: dropout=0.1 regularization0.05\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 25.0879 - acc: 0.7084 - val_loss: 14.5889 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.5666 - acc: 0.7466 - val_loss: 11.2931 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 10.7933 - acc: 0.7610 - val_loss: 10.3782 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 10.1382 - acc: 0.7639 - val_loss: 9.9124 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 9.7881 - acc: 0.7646 - val_loss: 9.6628 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 9.5466 - acc: 0.7586 - val_loss: 9.3973 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 9.3198 - acc: 0.7664 - val_loss: 9.2168 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.76238 to 0.76485, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 9.1490 - acc: 0.7751 - val_loss: 9.1458 - val_acc: 0.7228\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76485\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 9.0289 - acc: 0.7643 - val_loss: 8.9272 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.76485 to 0.77228, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.8937 - acc: 0.7672 - val_loss: 8.8126 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.77228 to 0.77723, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.7893 - acc: 0.7721 - val_loss: 8.7129 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.77723\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.7171 - acc: 0.7541 - val_loss: 8.6278 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.77723\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.5904 - acc: 0.7748 - val_loss: 8.5632 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.77723 to 0.77970, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.5017 - acc: 0.7883 - val_loss: 8.4491 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.77970 to 0.80446, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.4196 - acc: 0.7848 - val_loss: 8.3748 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80446\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.3503 - acc: 0.7925 - val_loss: 8.2959 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.80446\n",
      "Epoch 17/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 8.2728 - acc: 0.8049 - val_loss: 8.2230 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.80446 to 0.80693, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.2014 - acc: 0.8060 - val_loss: 8.1590 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.80693\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.1357 - acc: 0.8097 - val_loss: 8.0991 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.80693\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.0835 - acc: 0.8022 - val_loss: 8.0637 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.80693\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.0196 - acc: 0.7965 - val_loss: 8.1089 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.80693\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.9802 - acc: 0.7953 - val_loss: 7.9429 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.80693\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.9053 - acc: 0.8108 - val_loss: 7.9198 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.80693 to 0.80693, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.8770 - acc: 0.8165 - val_loss: 7.8420 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.80693 to 0.81931, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.8295 - acc: 0.8073 - val_loss: 7.7967 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.81931 to 0.82178, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.7665 - acc: 0.8204 - val_loss: 7.7505 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.82178 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.7354 - acc: 0.8057 - val_loss: 7.7129 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83663\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.6831 - acc: 0.8099 - val_loss: 7.6587 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83663\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.6428 - acc: 0.8212 - val_loss: 7.6252 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83663\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.6196 - acc: 0.8068 - val_loss: 7.6111 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83663\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.5685 - acc: 0.8121 - val_loss: 7.5507 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83663\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.5202 - acc: 0.8151 - val_loss: 7.5268 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83663\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.5045 - acc: 0.8104 - val_loss: 7.5213 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83663\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.4488 - acc: 0.8295 - val_loss: 7.4635 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83663\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.4126 - acc: 0.8277 - val_loss: 7.4118 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83663\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.3909 - acc: 0.8109 - val_loss: 7.3781 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.83663\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.3447 - acc: 0.8259 - val_loss: 7.3354 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.83663 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.2979 - acc: 0.8373 - val_loss: 7.2986 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.83911\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.2742 - acc: 0.8313 - val_loss: 7.3287 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83911\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.2362 - acc: 0.8406 - val_loss: 7.2484 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.83911\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.2165 - acc: 0.8340 - val_loss: 7.2116 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83911\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.2159 - acc: 0.8253 - val_loss: 7.1971 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.83911\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.1726 - acc: 0.8326 - val_loss: 7.1506 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.83911\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.1232 - acc: 0.8311 - val_loss: 7.1161 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.83911\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.0855 - acc: 0.8458 - val_loss: 7.0909 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.83911\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.0600 - acc: 0.8391 - val_loss: 7.0732 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.83911\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.0223 - acc: 0.8422 - val_loss: 7.0382 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.83911\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.9909 - acc: 0.8464 - val_loss: 7.0465 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.83911\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.9890 - acc: 0.8386 - val_loss: 6.9896 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.83911\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.9351 - acc: 0.8622 - val_loss: 6.9700 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.83911\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.9418 - acc: 0.8494 - val_loss: 6.9507 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.83911\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.8940 - acc: 0.8491 - val_loss: 6.9247 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.83911\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.8861 - acc: 0.8431 - val_loss: 6.9403 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.83911\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.8668 - acc: 0.8394 - val_loss: 6.8774 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.83911\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.8258 - acc: 0.8584 - val_loss: 6.8540 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.83911\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.7960 - acc: 0.8647 - val_loss: 6.8459 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.83911\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.7579 - acc: 0.8674 - val_loss: 6.8097 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.83911\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.7345 - acc: 0.8740 - val_loss: 6.7981 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.83911\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.7186 - acc: 0.8605 - val_loss: 6.7645 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.83911 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.7280 - acc: 0.8471 - val_loss: 6.7390 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84158\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.6699 - acc: 0.8725 - val_loss: 6.7213 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84158\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.6488 - acc: 0.8737 - val_loss: 6.7142 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84158\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.6435 - acc: 0.8634 - val_loss: 6.6801 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.84158 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.6233 - acc: 0.8611 - val_loss: 6.6707 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84901\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.5959 - acc: 0.8710 - val_loss: 6.6575 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84901\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.5599 - acc: 0.8833 - val_loss: 6.6343 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.84901 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.5529 - acc: 0.8753 - val_loss: 6.5945 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.85644\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.5265 - acc: 0.8749 - val_loss: 6.5953 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.85644\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.5205 - acc: 0.8665 - val_loss: 6.5913 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.85644\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.4989 - acc: 0.8722 - val_loss: 6.5476 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.85644\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.4627 - acc: 0.8884 - val_loss: 6.5426 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.85644\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.4812 - acc: 0.8716 - val_loss: 6.5176 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.85644\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.4258 - acc: 0.8990 - val_loss: 6.4969 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.85644\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.4226 - acc: 0.8821 - val_loss: 6.4876 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.85644\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.3895 - acc: 0.8855 - val_loss: 6.4995 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.85644\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.3603 - acc: 0.8990 - val_loss: 6.4653 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.85644\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.3416 - acc: 0.8963 - val_loss: 6.4389 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.85644\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.3118 - acc: 0.9014 - val_loss: 6.4189 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.85644\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.3200 - acc: 0.8903 - val_loss: 6.4309 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.85644\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.2691 - acc: 0.9107 - val_loss: 6.4046 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.85644 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2657 - acc: 0.8987 - val_loss: 6.3711 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.86634\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.2600 - acc: 0.8933 - val_loss: 6.3885 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.86634\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2484 - acc: 0.8966 - val_loss: 6.3381 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.86634\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.2414 - acc: 0.8938 - val_loss: 6.3876 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.86634\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2085 - acc: 0.9092 - val_loss: 6.3115 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.86634\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2005 - acc: 0.9032 - val_loss: 6.3152 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.86634\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.1487 - acc: 0.9269 - val_loss: 6.2853 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.86634\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.1356 - acc: 0.9206 - val_loss: 6.2859 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.86634\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.1214 - acc: 0.9167 - val_loss: 6.2619 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.86634\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.1154 - acc: 0.9113 - val_loss: 6.2630 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.86634\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0951 - acc: 0.9069 - val_loss: 6.2225 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.86634\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.1088 - acc: 0.9053 - val_loss: 6.2220 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.86634\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0647 - acc: 0.9248 - val_loss: 6.2140 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.86634\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0507 - acc: 0.9215 - val_loss: 6.2499 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.86634\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.0544 - acc: 0.9113 - val_loss: 6.2017 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.86634\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.0130 - acc: 0.9332 - val_loss: 6.1906 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.86634\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0082 - acc: 0.9185 - val_loss: 6.3645 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.86634\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.0583 - acc: 0.8882 - val_loss: 6.1717 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.86634\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.9967 - acc: 0.9155 - val_loss: 6.1470 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.86634\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.9735 - acc: 0.9203 - val_loss: 6.1357 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.86634\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.9400 - acc: 0.9327 - val_loss: 6.1242 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.86634\n",
      "Epoch 102/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 34s 1s/step - loss: 5.9101 - acc: 0.9414 - val_loss: 6.1158 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.86634\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.8947 - acc: 0.9435 - val_loss: 6.1487 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.86634\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.9064 - acc: 0.9282 - val_loss: 6.1466 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.86634\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.8777 - acc: 0.9368 - val_loss: 6.1099 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.86634\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.8683 - acc: 0.9329 - val_loss: 6.0764 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.86634\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.8667 - acc: 0.9285 - val_loss: 6.0589 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.86634\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.8624 - acc: 0.9314 - val_loss: 6.0331 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.86634\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.8277 - acc: 0.9432 - val_loss: 6.0853 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.86634\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.8701 - acc: 0.9131 - val_loss: 6.0138 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.86634\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.8196 - acc: 0.9335 - val_loss: 6.0451 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.86634\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 5.7986 - acc: 0.9323 - val_loss: 6.0565 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.86634\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.7789 - acc: 0.9471 - val_loss: 6.0269 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.86634\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.7673 - acc: 0.9438 - val_loss: 5.9975 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.86634\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.7671 - acc: 0.9432 - val_loss: 5.9782 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.86634\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.7576 - acc: 0.9348 - val_loss: 5.9784 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.86634\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.7132 - acc: 0.9576 - val_loss: 5.9797 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.86634\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.7605 - acc: 0.9285 - val_loss: 6.0272 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.86634\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.7530 - acc: 0.9236 - val_loss: 5.9447 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.86634\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.7011 - acc: 0.9465 - val_loss: 5.9886 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.86634\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 5.7036 - acc: 0.9372 - val_loss: 5.9261 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.86634\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.6640 - acc: 0.9513 - val_loss: 5.9195 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.86634\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.6589 - acc: 0.9513 - val_loss: 5.9138 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.86634\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 5.6469 - acc: 0.9549 - val_loss: 5.8975 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.86634\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.6447 - acc: 0.9495 - val_loss: 5.8812 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.86634\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.6163 - acc: 0.9558 - val_loss: 5.8909 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.86634\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.6049 - acc: 0.9570 - val_loss: 5.8865 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.86634\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.6061 - acc: 0.9528 - val_loss: 5.8986 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.86634\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 5.5715 - acc: 0.9663 - val_loss: 5.8454 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.86634\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 5.5528 - acc: 0.9681 - val_loss: 5.8432 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.86634\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.5616 - acc: 0.9591 - val_loss: 5.8799 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.86634\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.5431 - acc: 0.9663 - val_loss: 5.8439 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.86634\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.5569 - acc: 0.9546 - val_loss: 5.9430 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.86634\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.5129 - acc: 0.9699 - val_loss: 5.8190 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00134: val_acc improved from 0.86634 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.5057 - acc: 0.9663 - val_loss: 5.9253 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.86634\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.5294 - acc: 0.9513 - val_loss: 5.8722 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.86634\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.5261 - acc: 0.9495 - val_loss: 5.7660 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00137: val_acc improved from 0.86634 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.5243 - acc: 0.9435 - val_loss: 5.8612 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.87129\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.5356 - acc: 0.9384 - val_loss: 5.7707 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.87129\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.4813 - acc: 0.9609 - val_loss: 5.7529 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.87129\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.4712 - acc: 0.9630 - val_loss: 5.7751 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.87129\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.4595 - acc: 0.9684 - val_loss: 5.7322 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.87129\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.4350 - acc: 0.9687 - val_loss: 5.7410 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.87129\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.4379 - acc: 0.9676 - val_loss: 5.7499 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.87129\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.4048 - acc: 0.9807 - val_loss: 5.7563 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.87129\n",
      "Epoch 146/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 36s 1s/step - loss: 5.4049 - acc: 0.9657 - val_loss: 5.7350 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.87129\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.3822 - acc: 0.9783 - val_loss: 5.7171 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00147: val_acc improved from 0.87129 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.05.h5\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 5.4099 - acc: 0.9594 - val_loss: 5.7142 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.88119\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.3986 - acc: 0.9621 - val_loss: 5.7026 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.88119\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 5.3677 - acc: 0.9717 - val_loss: 5.7229 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.88119\n",
      "experiment: dropout=0.1 regularization0.01\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 39s 2s/step - loss: 6.2940 - acc: 0.7215 - val_loss: 4.4128 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 3.6639 - acc: 0.7666 - val_loss: 3.2560 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 3.1160 - acc: 0.7642 - val_loss: 2.9443 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.8435 - acc: 0.7639 - val_loss: 2.7475 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.6983 - acc: 0.7421 - val_loss: 2.6208 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.76238 to 0.76733, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.5704 - acc: 0.7688 - val_loss: 2.5259 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76733 to 0.78218, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.4794 - acc: 0.7859 - val_loss: 2.4306 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.78218 to 0.80198, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.4179 - acc: 0.7736 - val_loss: 2.3884 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80198\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.3619 - acc: 0.7802 - val_loss: 2.3399 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80198\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.3142 - acc: 0.7880 - val_loss: 2.3008 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.80198 to 0.80198, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.2629 - acc: 0.8091 - val_loss: 2.2292 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.80198 to 0.81436, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.2121 - acc: 0.8169 - val_loss: 2.2015 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.81436\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.2090 - acc: 0.8079 - val_loss: 2.2035 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.81436\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 2.1660 - acc: 0.8207 - val_loss: 2.1491 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.81436\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.1436 - acc: 0.8121 - val_loss: 2.1264 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.81436\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.1236 - acc: 0.8151 - val_loss: 2.0980 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.81436 to 0.81931, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.0976 - acc: 0.8190 - val_loss: 2.1006 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.81931\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.0667 - acc: 0.8208 - val_loss: 2.0578 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.81931 to 0.82426, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.0717 - acc: 0.8253 - val_loss: 2.0492 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.82426 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 2.0365 - acc: 0.8317 - val_loss: 2.0629 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83663\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.0251 - acc: 0.8319 - val_loss: 2.0371 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83663\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.9848 - acc: 0.8412 - val_loss: 1.9965 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.83663 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.9673 - acc: 0.8487 - val_loss: 1.9709 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.84158 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.9611 - acc: 0.8422 - val_loss: 1.9852 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.84406\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.9464 - acc: 0.8446 - val_loss: 1.9487 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.84406\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.9422 - acc: 0.8424 - val_loss: 1.9506 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.84406 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.9208 - acc: 0.8425 - val_loss: 1.9537 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.84901\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.8917 - acc: 0.8565 - val_loss: 1.9328 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.84901\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.9022 - acc: 0.8578 - val_loss: 1.9100 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.84901\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.8607 - acc: 0.8686 - val_loss: 1.9136 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.84901 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.8543 - acc: 0.8536 - val_loss: 1.8850 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.85149\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.8426 - acc: 0.8632 - val_loss: 1.9897 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.85149\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.8369 - acc: 0.8608 - val_loss: 1.8793 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.85149\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.8283 - acc: 0.8596 - val_loss: 1.8729 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.85149\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.8030 - acc: 0.8749 - val_loss: 1.8761 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.85149\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.7714 - acc: 0.8803 - val_loss: 1.8894 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.85149\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.7714 - acc: 0.8842 - val_loss: 1.8521 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.85149 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.7837 - acc: 0.8737 - val_loss: 1.8413 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.85149\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.7498 - acc: 0.8900 - val_loss: 1.8225 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.85149\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.7511 - acc: 0.8815 - val_loss: 1.8216 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.85149\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.7667 - acc: 0.8680 - val_loss: 1.8233 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.85149\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.7096 - acc: 0.8953 - val_loss: 1.8453 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.85149 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.7107 - acc: 0.8879 - val_loss: 1.8255 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.85644 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.6879 - acc: 0.9029 - val_loss: 1.7832 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.85891 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.6694 - acc: 0.9068 - val_loss: 1.8136 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.87129\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.6506 - acc: 0.9074 - val_loss: 1.8507 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.87129\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.6865 - acc: 0.8989 - val_loss: 1.7841 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87129\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.6543 - acc: 0.9080 - val_loss: 1.7929 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.87129\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.6243 - acc: 0.9149 - val_loss: 1.7965 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.87129\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.6295 - acc: 0.9128 - val_loss: 1.8235 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87129\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.6466 - acc: 0.9019 - val_loss: 1.7740 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.87129\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5976 - acc: 0.9197 - val_loss: 1.7626 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.87129\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5709 - acc: 0.9320 - val_loss: 1.8656 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.87129\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5892 - acc: 0.9224 - val_loss: 1.8834 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.87129\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.6151 - acc: 0.9037 - val_loss: 1.7759 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87129\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.5585 - acc: 0.9416 - val_loss: 1.7451 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87129\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.5382 - acc: 0.9384 - val_loss: 1.7819 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.87129\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5269 - acc: 0.9345 - val_loss: 1.8078 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87129\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5048 - acc: 0.9486 - val_loss: 1.8346 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.87129\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.5195 - acc: 0.9426 - val_loss: 1.7920 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.87129\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.5345 - acc: 0.9285 - val_loss: 1.8352 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.87129\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5681 - acc: 0.9200 - val_loss: 1.7843 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.87129\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5495 - acc: 0.9252 - val_loss: 1.7869 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.87129\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5404 - acc: 0.9237 - val_loss: 1.7336 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.87129\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5038 - acc: 0.9386 - val_loss: 1.7717 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00065: val_acc improved from 0.87129 to 0.88366, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5151 - acc: 0.9270 - val_loss: 1.7177 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.88366\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4722 - acc: 0.9489 - val_loss: 1.8375 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.88366\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4730 - acc: 0.9420 - val_loss: 1.8381 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.88366\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4754 - acc: 0.9432 - val_loss: 1.7739 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.88366\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4503 - acc: 0.9561 - val_loss: 1.7342 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.88366\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4493 - acc: 0.9573 - val_loss: 1.8061 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.88366\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4181 - acc: 0.9618 - val_loss: 1.8000 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.88366\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4308 - acc: 0.9615 - val_loss: 1.9473 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.88366\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4643 - acc: 0.9392 - val_loss: 1.7124 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.88366\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4337 - acc: 0.9537 - val_loss: 1.7672 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.88366\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4114 - acc: 0.9627 - val_loss: 1.9597 - val_acc: 0.8564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00076: val_acc did not improve from 0.88366\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.4128 - acc: 0.9615 - val_loss: 1.7813 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.88366\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4004 - acc: 0.9639 - val_loss: 1.8065 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.88366\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3978 - acc: 0.9579 - val_loss: 1.8408 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.88366\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3941 - acc: 0.9594 - val_loss: 1.7675 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.88366\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3846 - acc: 0.9663 - val_loss: 1.8264 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.88366\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.3900 - acc: 0.9621 - val_loss: 1.9521 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.88366\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4039 - acc: 0.9579 - val_loss: 2.0263 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.88366\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3843 - acc: 0.9618 - val_loss: 1.8145 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.88366\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3646 - acc: 0.9675 - val_loss: 1.8554 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.88366\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3451 - acc: 0.9765 - val_loss: 1.8950 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.88366\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.3530 - acc: 0.9714 - val_loss: 1.8550 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.88366\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3898 - acc: 0.9567 - val_loss: 1.7714 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.88366\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3464 - acc: 0.9717 - val_loss: 1.8828 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.88366\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3469 - acc: 0.9700 - val_loss: 1.9883 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.88366\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3437 - acc: 0.9714 - val_loss: 1.9255 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.88366\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3385 - acc: 0.9721 - val_loss: 1.8053 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.88366\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3122 - acc: 0.9868 - val_loss: 1.8702 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00093: val_acc improved from 0.88366 to 0.88614, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3065 - acc: 0.9856 - val_loss: 1.8399 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.88614\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3267 - acc: 0.9687 - val_loss: 1.8322 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.88614\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3882 - acc: 0.9477 - val_loss: 1.6897 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.88614\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3695 - acc: 0.9471 - val_loss: 1.6994 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.88614\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3183 - acc: 0.9757 - val_loss: 1.8038 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.88614\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3029 - acc: 0.9826 - val_loss: 1.7510 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00099: val_acc improved from 0.88614 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2871 - acc: 0.9844 - val_loss: 1.8261 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89851\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2835 - acc: 0.9874 - val_loss: 1.8818 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89851\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2811 - acc: 0.9844 - val_loss: 1.8251 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89851\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.2826 - acc: 0.9844 - val_loss: 1.8452 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89851\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2738 - acc: 0.9859 - val_loss: 1.8455 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89851\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2717 - acc: 0.9850 - val_loss: 1.8575 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89851\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2771 - acc: 0.9807 - val_loss: 1.8545 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89851\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2955 - acc: 0.9745 - val_loss: 1.7767 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.89851\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 1.2735 - acc: 0.9820 - val_loss: 1.8568 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.89851\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.2599 - acc: 0.9880 - val_loss: 1.8475 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.89851\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3099 - acc: 0.9681 - val_loss: 1.8142 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.89851\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2671 - acc: 0.9832 - val_loss: 1.8255 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.89851\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2484 - acc: 0.9871 - val_loss: 1.7787 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.89851\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.2529 - acc: 0.9823 - val_loss: 1.8662 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.89851\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2505 - acc: 0.9826 - val_loss: 1.8338 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.89851\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2522 - acc: 0.9850 - val_loss: 1.9040 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.89851\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2600 - acc: 0.9772 - val_loss: 1.7752 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.89851\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2524 - acc: 0.9826 - val_loss: 1.8321 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.89851\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2915 - acc: 0.9651 - val_loss: 1.7043 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.89851\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2502 - acc: 0.9795 - val_loss: 1.7647 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.89851\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2284 - acc: 0.9871 - val_loss: 1.9951 - val_acc: 0.8713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00120: val_acc did not improve from 0.89851\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2690 - acc: 0.9733 - val_loss: 1.7608 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.89851\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2268 - acc: 0.9904 - val_loss: 1.8834 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.89851\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2204 - acc: 0.9895 - val_loss: 1.7884 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.89851\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2275 - acc: 0.9853 - val_loss: 1.7246 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.89851\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2132 - acc: 0.9916 - val_loss: 1.8084 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.89851\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2109 - acc: 0.9904 - val_loss: 1.8214 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.89851\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2042 - acc: 0.9898 - val_loss: 1.7944 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.89851\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2107 - acc: 0.9841 - val_loss: 1.7665 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00128: val_acc improved from 0.89851 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2228 - acc: 0.9811 - val_loss: 1.6905 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00129: val_acc improved from 0.89851 to 0.90099, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.01.h5\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2012 - acc: 0.9910 - val_loss: 1.8144 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90099\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2156 - acc: 0.9850 - val_loss: 1.7667 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90099\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.2115 - acc: 0.9850 - val_loss: 1.7776 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90099\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.1849 - acc: 0.9952 - val_loss: 1.7300 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90099\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.1795 - acc: 0.9964 - val_loss: 1.7827 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90099\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.1815 - acc: 0.9934 - val_loss: 1.7437 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90099\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.1801 - acc: 0.9928 - val_loss: 1.7616 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90099\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.1734 - acc: 0.9952 - val_loss: 1.7896 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90099\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.1987 - acc: 0.9829 - val_loss: 1.7304 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90099\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2511 - acc: 0.9651 - val_loss: 1.6552 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90099\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.2122 - acc: 0.9751 - val_loss: 1.6653 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90099\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.1810 - acc: 0.9898 - val_loss: 1.7142 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90099\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.1723 - acc: 0.9916 - val_loss: 1.7659 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90099\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.1843 - acc: 0.9850 - val_loss: 1.7555 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90099\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.1822 - acc: 0.9874 - val_loss: 1.7345 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90099\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.1686 - acc: 0.9892 - val_loss: 1.6967 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90099\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.1617 - acc: 0.9916 - val_loss: 1.7459 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90099\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.1545 - acc: 0.9958 - val_loss: 1.7728 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90099\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.1501 - acc: 0.9958 - val_loss: 1.6981 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90099\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.1470 - acc: 0.9958 - val_loss: 1.7906 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90099\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.1579 - acc: 0.9880 - val_loss: 1.7181 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90099\n",
      "experiment: dropout=0.1 regularization0.5\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 39s 1s/step - loss: 233.8133 - acc: 0.7204 - val_loss: 129.6259 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.5.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 112.9233 - acc: 0.7598 - val_loss: 102.7594 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 99.1038 - acc: 0.7595 - val_loss: 96.1579 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.5.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 94.4540 - acc: 0.7639 - val_loss: 92.8639 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 91.7434 - acc: 0.7610 - val_loss: 90.6168 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 89.7503 - acc: 0.7580 - val_loss: 88.8680 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76238\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 88.1557 - acc: 0.7639 - val_loss: 87.4227 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76238\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 86.8077 - acc: 0.7669 - val_loss: 86.1742 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76238\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 85.6363 - acc: 0.7639 - val_loss: 85.0631 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76238\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 84.5841 - acc: 0.7610 - val_loss: 84.0677 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76238\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 83.6369 - acc: 0.7566 - val_loss: 83.1622 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76238\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 82.7573 - acc: 0.7625 - val_loss: 82.3204 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76238\n",
      "Epoch 13/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 35s 1s/step - loss: 81.9580 - acc: 0.7595 - val_loss: 81.5405 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76238\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 81.1979 - acc: 0.7566 - val_loss: 80.8112 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76238\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 80.4831 - acc: 0.7625 - val_loss: 80.1271 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76238\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 79.8152 - acc: 0.7625 - val_loss: 79.4807 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76238\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 79.1829 - acc: 0.7654 - val_loss: 78.8635 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76238\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 78.5858 - acc: 0.7595 - val_loss: 78.2826 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76238\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 78.0187 - acc: 0.7580 - val_loss: 77.7229 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76238\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 77.4654 - acc: 0.7610 - val_loss: 77.1938 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76238\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 76.9403 - acc: 0.7639 - val_loss: 76.6759 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76238\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 76.4433 - acc: 0.7595 - val_loss: 76.1830 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76238\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 75.9643 - acc: 0.7610 - val_loss: 75.7099 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76238\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 75.4928 - acc: 0.7639 - val_loss: 75.2526 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76238\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 75.0385 - acc: 0.7669 - val_loss: 74.8124 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76238\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 74.6047 - acc: 0.7654 - val_loss: 74.3875 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76238\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 74.1918 - acc: 0.7639 - val_loss: 73.9728 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76238\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 73.7858 - acc: 0.7625 - val_loss: 73.5725 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76238\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 73.3895 - acc: 0.7639 - val_loss: 73.1849 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76238\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 73.0066 - acc: 0.7654 - val_loss: 72.8074 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76238\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 72.6252 - acc: 0.7669 - val_loss: 72.4406 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76238\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 72.2783 - acc: 0.7595 - val_loss: 72.0837 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76238\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 71.9233 - acc: 0.7595 - val_loss: 71.7364 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76238\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 71.5740 - acc: 0.7654 - val_loss: 71.3988 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.76238\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 71.2432 - acc: 0.7625 - val_loss: 71.0676 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.76238\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 70.9157 - acc: 0.7654 - val_loss: 70.7449 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.76238\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 70.5950 - acc: 0.7639 - val_loss: 70.4302 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.76238\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 70.2895 - acc: 0.7625 - val_loss: 70.1231 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.76238\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 69.9883 - acc: 0.7595 - val_loss: 69.8225 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.76238\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 69.6797 - acc: 0.7654 - val_loss: 69.5277 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.76238\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 69.3897 - acc: 0.7669 - val_loss: 69.2418 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.76238\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 69.1103 - acc: 0.7610 - val_loss: 68.9578 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.76238\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 68.8298 - acc: 0.7625 - val_loss: 68.6816 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.76238\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 68.5534 - acc: 0.7625 - val_loss: 68.4113 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.76238\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 68.2829 - acc: 0.7639 - val_loss: 68.1456 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.76238\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 68.0184 - acc: 0.7684 - val_loss: 67.8860 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.76238\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 67.7645 - acc: 0.7639 - val_loss: 67.6298 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.76238\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 67.5123 - acc: 0.7625 - val_loss: 67.3802 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.76238\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 67.2653 - acc: 0.7654 - val_loss: 67.1343 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.76238\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 67.0209 - acc: 0.7625 - val_loss: 66.8911 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.76238\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 66.7914 - acc: 0.7580 - val_loss: 66.6537 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.76238\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 42s 2s/step - loss: 66.5507 - acc: 0.7566 - val_loss: 66.4202 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.76238\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 39s 1s/step - loss: 66.3114 - acc: 0.7654 - val_loss: 66.1902 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.76238\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 66.0827 - acc: 0.7639 - val_loss: 65.9651 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.76238\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 65.8648 - acc: 0.7566 - val_loss: 65.7437 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.76238\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 65.6414 - acc: 0.7639 - val_loss: 65.5236 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.76238\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 65.4304 - acc: 0.7595 - val_loss: 65.3090 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.76238\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 65.2117 - acc: 0.7610 - val_loss: 65.0973 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.76238\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 64.9994 - acc: 0.7625 - val_loss: 64.8885 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.76238\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 64.7891 - acc: 0.7654 - val_loss: 64.6829 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.76238\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 64.5899 - acc: 0.7610 - val_loss: 64.4804 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.76238\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 64.3909 - acc: 0.7625 - val_loss: 64.2823 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.76238\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 64.1896 - acc: 0.7625 - val_loss: 64.0843 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.76238\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 63.9944 - acc: 0.7610 - val_loss: 63.8907 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.76238\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 63.8025 - acc: 0.7639 - val_loss: 63.6996 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.76238\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 63.6227 - acc: 0.7566 - val_loss: 63.5114 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.76238\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 63.4280 - acc: 0.7625 - val_loss: 63.3256 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.76238\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 63.2496 - acc: 0.7580 - val_loss: 63.1424 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.76238\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 63.0559 - acc: 0.7639 - val_loss: 62.9611 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.76238\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.8783 - acc: 0.7639 - val_loss: 62.7831 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.76238\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.7026 - acc: 0.7639 - val_loss: 62.6065 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.76238\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 62.5348 - acc: 0.7580 - val_loss: 62.4346 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.76238\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.3544 - acc: 0.7639 - val_loss: 62.2610 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.76238\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.1810 - acc: 0.7625 - val_loss: 62.0913 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.76238\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.0102 - acc: 0.7654 - val_loss: 61.9239 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.76238\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 61.8482 - acc: 0.7610 - val_loss: 61.7587 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.76238\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 61.6923 - acc: 0.7580 - val_loss: 61.5973 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.76238\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 61.5283 - acc: 0.7566 - val_loss: 61.4339 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.76238\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 61.3590 - acc: 0.7654 - val_loss: 61.2739 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.76238\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 61.2060 - acc: 0.7595 - val_loss: 61.1171 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.76238\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 61.0434 - acc: 0.7654 - val_loss: 60.9607 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.76238\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 60.8843 - acc: 0.7669 - val_loss: 60.8063 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.76238\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 60.7319 - acc: 0.7669 - val_loss: 60.6545 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.76238\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 60.5870 - acc: 0.7625 - val_loss: 60.5032 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.76238\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 60.4356 - acc: 0.7639 - val_loss: 60.3545 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.76238\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 60.2857 - acc: 0.7654 - val_loss: 60.2068 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.76238\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 60.1423 - acc: 0.7610 - val_loss: 60.0610 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.76238\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 59.9945 - acc: 0.7625 - val_loss: 59.9169 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.76238\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 59.8465 - acc: 0.7654 - val_loss: 59.7743 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.76238\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 59.7079 - acc: 0.7625 - val_loss: 59.6335 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.76238\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 59.5744 - acc: 0.7595 - val_loss: 59.4938 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.76238\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 59.4324 - acc: 0.7610 - val_loss: 59.3554 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.76238\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 59.2964 - acc: 0.7610 - val_loss: 59.2198 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.76238\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 59.1587 - acc: 0.7595 - val_loss: 59.0831 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.76238\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 59.0205 - acc: 0.7639 - val_loss: 58.9491 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.76238\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 58.8942 - acc: 0.7580 - val_loss: 58.8180 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.76238\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 58.7593 - acc: 0.7625 - val_loss: 58.6851 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.76238\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 58.6267 - acc: 0.7625 - val_loss: 58.5552 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.76238\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 58.5040 - acc: 0.7595 - val_loss: 58.4262 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.76238\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 58.3675 - acc: 0.7610 - val_loss: 58.2987 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.76238\n",
      "Epoch 101/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 35s 1s/step - loss: 58.2416 - acc: 0.7639 - val_loss: 58.1724 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.76238\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 58.1166 - acc: 0.7610 - val_loss: 58.0475 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.76238\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 57.9898 - acc: 0.7639 - val_loss: 57.9232 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.76238\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 57.8669 - acc: 0.7654 - val_loss: 57.8007 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.76238\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 57.7450 - acc: 0.7639 - val_loss: 57.6789 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.76238\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 57.6317 - acc: 0.7551 - val_loss: 57.5591 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.76238\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 57.5040 - acc: 0.7654 - val_loss: 57.4393 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.76238\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 57.3921 - acc: 0.7610 - val_loss: 57.3209 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.76238\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 57.2654 - acc: 0.7639 - val_loss: 57.2030 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.76238\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 57.1509 - acc: 0.7639 - val_loss: 57.0866 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.76238\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 57.0412 - acc: 0.7580 - val_loss: 56.9719 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.76238\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 56.9197 - acc: 0.7639 - val_loss: 56.8575 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.76238\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 56.8089 - acc: 0.7639 - val_loss: 56.7455 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.76238\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 56.7000 - acc: 0.7595 - val_loss: 56.6325 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.76238\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 56.5851 - acc: 0.7625 - val_loss: 56.5208 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.76238\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 56.4667 - acc: 0.7654 - val_loss: 56.4103 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.76238\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 56.3618 - acc: 0.7625 - val_loss: 56.3010 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.76238\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 56.2525 - acc: 0.7625 - val_loss: 56.1927 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.76238\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 56.1496 - acc: 0.7580 - val_loss: 56.0851 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.76238\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 56.0375 - acc: 0.7639 - val_loss: 55.9786 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.76238\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 55.9345 - acc: 0.7610 - val_loss: 55.8725 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.76238\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 55.8359 - acc: 0.7551 - val_loss: 55.7685 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.76238\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 55.7305 - acc: 0.7580 - val_loss: 55.6646 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.76238\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 55.6189 - acc: 0.7654 - val_loss: 55.5608 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.76238\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 55.5168 - acc: 0.7610 - val_loss: 55.4581 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.76238\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 55.4126 - acc: 0.7625 - val_loss: 55.3570 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.76238\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 55.3158 - acc: 0.7610 - val_loss: 55.2560 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.76238\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 55.2209 - acc: 0.7580 - val_loss: 55.1560 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.76238\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 55.1115 - acc: 0.7610 - val_loss: 55.0566 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.76238\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 55.0054 - acc: 0.7669 - val_loss: 54.9586 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.76238\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 54.9051 - acc: 0.7669 - val_loss: 54.8604 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.76238\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 54.8223 - acc: 0.7595 - val_loss: 54.7634 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.76238\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 54.7250 - acc: 0.7580 - val_loss: 54.6676 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.76238\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 54.6200 - acc: 0.7639 - val_loss: 54.5717 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.76238\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 54.5377 - acc: 0.7580 - val_loss: 54.4771 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.76238\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 54.4365 - acc: 0.7610 - val_loss: 54.3832 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.76238\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 54.3446 - acc: 0.7580 - val_loss: 54.2897 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.76238\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 54.2407 - acc: 0.7669 - val_loss: 54.1972 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.76238\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 54.1613 - acc: 0.7580 - val_loss: 54.1054 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.76238\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 54.0658 - acc: 0.7610 - val_loss: 54.0140 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.76238\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 53.9735 - acc: 0.7625 - val_loss: 53.9236 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.76238\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 53.8896 - acc: 0.7580 - val_loss: 53.8338 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.76238\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 53.7961 - acc: 0.7595 - val_loss: 53.7443 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.76238\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 53.7054 - acc: 0.7654 - val_loss: 53.6562 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.76238\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 53.6150 - acc: 0.7639 - val_loss: 53.5675 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.76238\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 53.5308 - acc: 0.7610 - val_loss: 53.4801 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.76238\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 53.4428 - acc: 0.7610 - val_loss: 53.3932 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.76238\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 53.3596 - acc: 0.7610 - val_loss: 53.3075 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.76238\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 53.2759 - acc: 0.7566 - val_loss: 53.2214 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.76238\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 53.1904 - acc: 0.7595 - val_loss: 53.1373 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.76238\n",
      "experiment: dropout=0.1 regularization0.1\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 49.2401 - acc: 0.7140 - val_loss: 28.0385 - val_acc: 0.4703\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.47030, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 24.1158 - acc: 0.7451 - val_loss: 21.6768 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.47030 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 20.8289 - acc: 0.7566 - val_loss: 20.0733 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 19.6807 - acc: 0.7639 - val_loss: 19.3121 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 19.0505 - acc: 0.7648 - val_loss: 18.7902 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 18.6096 - acc: 0.7625 - val_loss: 18.3917 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 18.2482 - acc: 0.7595 - val_loss: 18.1089 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76238\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 17.9653 - acc: 0.7566 - val_loss: 17.9787 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76238\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 17.7524 - acc: 0.7595 - val_loss: 17.5913 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76238\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 17.4959 - acc: 0.7595 - val_loss: 17.3753 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76238\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 17.2916 - acc: 0.7610 - val_loss: 17.1731 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76238\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 17.0977 - acc: 0.7654 - val_loss: 17.0015 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76238\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 16.9286 - acc: 0.7610 - val_loss: 16.8311 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76238\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 16.7619 - acc: 0.7639 - val_loss: 16.6761 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76238\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 16.6528 - acc: 0.7619 - val_loss: 16.5770 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76238\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 16.4936 - acc: 0.7551 - val_loss: 16.4061 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76238\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 16.3479 - acc: 0.7684 - val_loss: 16.2781 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76238\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 16.2353 - acc: 0.7595 - val_loss: 16.1601 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76238\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 16.1088 - acc: 0.7724 - val_loss: 16.0518 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.76238 to 0.78960, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 16.0101 - acc: 0.7757 - val_loss: 15.9474 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.78960\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 15.9074 - acc: 0.7697 - val_loss: 15.8345 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.78960\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 15.7983 - acc: 0.7745 - val_loss: 15.7349 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.78960 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 15.7223 - acc: 0.7808 - val_loss: 15.6438 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.79950\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 15.6014 - acc: 0.7799 - val_loss: 15.5538 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.79950 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 15.5340 - acc: 0.7790 - val_loss: 15.4700 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.81683\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 15.4242 - acc: 0.8000 - val_loss: 15.4047 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.81683\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 15.3382 - acc: 0.8006 - val_loss: 15.2903 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.81683\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 15.2589 - acc: 0.8021 - val_loss: 15.2064 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.81683\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 15.1792 - acc: 0.8088 - val_loss: 15.1329 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.81683\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 15.0976 - acc: 0.8037 - val_loss: 15.0809 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81683\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 15.0238 - acc: 0.8001 - val_loss: 14.9785 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.81683\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.9549 - acc: 0.7971 - val_loss: 14.9099 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.81683\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.8720 - acc: 0.8111 - val_loss: 14.8505 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.81683\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.7934 - acc: 0.8231 - val_loss: 14.7606 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.81683\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.7347 - acc: 0.8040 - val_loss: 14.7242 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.81683\n",
      "Epoch 36/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 35s 1s/step - loss: 14.6860 - acc: 0.8136 - val_loss: 14.6565 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.81683 to 0.82673, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.6135 - acc: 0.8076 - val_loss: 14.5664 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.82673\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.5372 - acc: 0.8172 - val_loss: 14.5104 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.82673 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.4865 - acc: 0.8133 - val_loss: 14.4497 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83663\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.4361 - acc: 0.8111 - val_loss: 14.3964 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.83663\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.3677 - acc: 0.8211 - val_loss: 14.3319 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83663\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.3037 - acc: 0.8163 - val_loss: 14.2692 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.83663\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.2474 - acc: 0.8131 - val_loss: 14.2286 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.83663\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.1827 - acc: 0.8250 - val_loss: 14.1631 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.83663\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.1446 - acc: 0.8132 - val_loss: 14.1172 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.83663 to 0.84653, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.0766 - acc: 0.8157 - val_loss: 14.0565 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84653\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.0375 - acc: 0.8118 - val_loss: 14.0128 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.84653\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.9826 - acc: 0.8217 - val_loss: 13.9481 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.84653\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.9541 - acc: 0.8016 - val_loss: 13.9165 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.84653\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 13.8792 - acc: 0.8208 - val_loss: 13.8571 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.84653\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.8283 - acc: 0.8220 - val_loss: 13.8230 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.84653\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.7947 - acc: 0.8247 - val_loss: 13.7605 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.84653\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.7404 - acc: 0.8235 - val_loss: 13.7403 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.84653\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.6846 - acc: 0.8289 - val_loss: 13.6810 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.84653\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.6476 - acc: 0.8199 - val_loss: 13.6207 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.84653\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.6030 - acc: 0.8208 - val_loss: 13.5788 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.84653\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.5469 - acc: 0.8436 - val_loss: 13.5361 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.84653\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.5066 - acc: 0.8317 - val_loss: 13.4936 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.84653\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.4735 - acc: 0.8232 - val_loss: 13.4621 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84653\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.4209 - acc: 0.8334 - val_loss: 13.4336 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84653\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.3845 - acc: 0.8274 - val_loss: 13.3653 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84653\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.3343 - acc: 0.8445 - val_loss: 13.3438 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84653\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.2963 - acc: 0.8368 - val_loss: 13.2785 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84653\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.2604 - acc: 0.8248 - val_loss: 13.2431 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84653\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.2401 - acc: 0.8166 - val_loss: 13.2352 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84653\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.1771 - acc: 0.8419 - val_loss: 13.1743 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.84653\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.1280 - acc: 0.8431 - val_loss: 13.1306 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.84653\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.0974 - acc: 0.8379 - val_loss: 13.1023 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.84653\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.0690 - acc: 0.8392 - val_loss: 13.0913 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.84653\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.0259 - acc: 0.8448 - val_loss: 13.0354 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.84653\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.9926 - acc: 0.8406 - val_loss: 12.9811 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.84653\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.9544 - acc: 0.8371 - val_loss: 12.9516 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.84653\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.9055 - acc: 0.8457 - val_loss: 12.9048 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.84653\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.8789 - acc: 0.8467 - val_loss: 12.8778 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00074: val_acc improved from 0.84653 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.8411 - acc: 0.8446 - val_loss: 12.8418 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.85396\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.8495 - acc: 0.8238 - val_loss: 12.8171 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.85396\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.7992 - acc: 0.8307 - val_loss: 12.8138 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.85396\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.7493 - acc: 0.8482 - val_loss: 12.7453 - val_acc: 0.8416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00078: val_acc did not improve from 0.85396\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.7097 - acc: 0.8476 - val_loss: 12.7152 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.85396\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.6900 - acc: 0.8383 - val_loss: 12.7117 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.85396\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.6578 - acc: 0.8430 - val_loss: 12.7327 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.85396\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.6454 - acc: 0.8368 - val_loss: 12.6334 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.85396\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.5816 - acc: 0.8512 - val_loss: 12.5940 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.85396\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.5607 - acc: 0.8469 - val_loss: 12.5706 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.85396\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.5535 - acc: 0.8337 - val_loss: 12.5349 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.85396\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.5051 - acc: 0.8422 - val_loss: 12.5149 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.85396\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.4706 - acc: 0.8484 - val_loss: 12.4902 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.85396\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.4572 - acc: 0.8419 - val_loss: 12.4530 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.85396\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.3961 - acc: 0.8605 - val_loss: 12.4156 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.85396\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.3710 - acc: 0.8563 - val_loss: 12.3915 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.85396\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.3367 - acc: 0.8611 - val_loss: 12.4151 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.85396\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.3167 - acc: 0.8545 - val_loss: 12.3493 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.85396\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.2904 - acc: 0.8527 - val_loss: 12.3287 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.85396\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.2520 - acc: 0.8662 - val_loss: 12.2815 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.85396\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.2109 - acc: 0.8755 - val_loss: 12.2532 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.85396\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.2055 - acc: 0.8632 - val_loss: 12.2280 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.85396\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.1650 - acc: 0.8638 - val_loss: 12.2295 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.85396\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 12.1498 - acc: 0.8638 - val_loss: 12.2115 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.85396\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.1480 - acc: 0.8515 - val_loss: 12.1796 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.85396\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.0999 - acc: 0.8596 - val_loss: 12.1301 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.85396\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.0588 - acc: 0.8686 - val_loss: 12.1138 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.85396\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.0429 - acc: 0.8578 - val_loss: 12.0737 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.85396\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.0355 - acc: 0.8461 - val_loss: 12.0482 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.85396\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.9795 - acc: 0.8683 - val_loss: 12.1011 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.85396\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.9736 - acc: 0.8572 - val_loss: 11.9868 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.85396\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.9336 - acc: 0.8707 - val_loss: 11.9733 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.85396\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.9017 - acc: 0.8791 - val_loss: 11.9379 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.85396\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.8735 - acc: 0.8818 - val_loss: 11.9293 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.85396\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.9028 - acc: 0.8449 - val_loss: 11.9408 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.85396\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.8390 - acc: 0.8728 - val_loss: 11.8725 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.85396\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.8154 - acc: 0.8692 - val_loss: 11.8657 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.85396\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.7945 - acc: 0.8659 - val_loss: 11.8869 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.85396\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.7755 - acc: 0.8689 - val_loss: 11.8087 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.85396\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.7505 - acc: 0.8731 - val_loss: 11.8148 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.85396\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.7293 - acc: 0.8713 - val_loss: 11.7805 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.85396\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.7082 - acc: 0.8665 - val_loss: 11.7532 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.85396\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.6637 - acc: 0.8780 - val_loss: 11.7307 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.85396\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.6545 - acc: 0.8830 - val_loss: 11.7293 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.85396\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.6265 - acc: 0.8782 - val_loss: 11.6698 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.85396\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.6075 - acc: 0.8692 - val_loss: 11.6610 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.85396\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.5720 - acc: 0.8848 - val_loss: 11.6307 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.85396\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.5557 - acc: 0.8851 - val_loss: 11.6179 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.85396\n",
      "Epoch 123/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 11.6283 - acc: 0.8290 - val_loss: 11.5928 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.85396\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.5346 - acc: 0.8734 - val_loss: 11.5958 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.85396\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.5139 - acc: 0.8749 - val_loss: 11.5465 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.85396\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.4992 - acc: 0.8719 - val_loss: 11.5213 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.85396\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.4589 - acc: 0.8750 - val_loss: 11.5077 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.85396\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.4478 - acc: 0.8731 - val_loss: 11.4964 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.85396\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.4239 - acc: 0.8837 - val_loss: 11.4657 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.85396\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.3723 - acc: 0.8963 - val_loss: 11.4618 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.85396\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.3484 - acc: 0.8971 - val_loss: 11.4719 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.85396\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.3425 - acc: 0.8876 - val_loss: 11.4102 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.85396\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.3520 - acc: 0.8755 - val_loss: 11.6182 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.85396\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.3186 - acc: 0.8881 - val_loss: 11.3844 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.85396\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.2885 - acc: 0.8963 - val_loss: 11.3686 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00135: val_acc improved from 0.85396 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.1.h5\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.2788 - acc: 0.8915 - val_loss: 11.3305 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.85644\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.2570 - acc: 0.8831 - val_loss: 11.3223 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.85644\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.2119 - acc: 0.9065 - val_loss: 11.2968 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.85644\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.2081 - acc: 0.8941 - val_loss: 11.2824 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.85644\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.1935 - acc: 0.8924 - val_loss: 11.2928 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.85644\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.1791 - acc: 0.8912 - val_loss: 11.2798 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.85644\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 11.1603 - acc: 0.8960 - val_loss: 11.2273 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.85644\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.1439 - acc: 0.8900 - val_loss: 11.2079 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.85644\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.1170 - acc: 0.8978 - val_loss: 11.1972 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.85644\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.1044 - acc: 0.8981 - val_loss: 11.1663 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.85644\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.0748 - acc: 0.8948 - val_loss: 11.1737 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.85644\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.0772 - acc: 0.8915 - val_loss: 11.1388 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.85644\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.0259 - acc: 0.9158 - val_loss: 11.1178 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.85644\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.0113 - acc: 0.9080 - val_loss: 11.1202 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.85644\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 10.9980 - acc: 0.9089 - val_loss: 11.1025 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.85644\n",
      "experiment: dropout=0.2 regularization0.001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4111 - acc: 0.7487 - val_loss: 1.1043 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.0502 - acc: 0.7583 - val_loss: 0.9747 - val_acc: 0.7574\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.9224 - acc: 0.7703 - val_loss: 0.8922 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.78713, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.8685 - acc: 0.7833 - val_loss: 0.8184 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78713 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.8179 - acc: 0.7997 - val_loss: 0.8578 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79950\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7826 - acc: 0.8174 - val_loss: 0.7604 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.79950 to 0.80941, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7561 - acc: 0.8171 - val_loss: 0.7375 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.80941\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7249 - acc: 0.8295 - val_loss: 0.7151 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.80941 to 0.81931, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7041 - acc: 0.8274 - val_loss: 0.6980 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.81931 to 0.82426, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6902 - acc: 0.8280 - val_loss: 0.6975 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.82426 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6724 - acc: 0.8361 - val_loss: 0.6626 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.83663 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6646 - acc: 0.8448 - val_loss: 0.7036 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.85644\n",
      "Epoch 13/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.6471 - acc: 0.8452 - val_loss: 0.7197 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.85644\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6256 - acc: 0.8602 - val_loss: 0.6484 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.85644\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6130 - acc: 0.8614 - val_loss: 0.6487 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85644\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5912 - acc: 0.8764 - val_loss: 0.6249 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.85644\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6058 - acc: 0.8591 - val_loss: 0.6540 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85644\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5629 - acc: 0.8815 - val_loss: 0.7363 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85644\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5784 - acc: 0.8734 - val_loss: 0.6349 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.85644 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5552 - acc: 0.8774 - val_loss: 0.6385 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85891\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5390 - acc: 0.8822 - val_loss: 0.6277 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.85891 to 0.86139, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5179 - acc: 0.8921 - val_loss: 0.5914 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.86139 to 0.86881, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5091 - acc: 0.8969 - val_loss: 0.6401 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86881\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4931 - acc: 0.8975 - val_loss: 0.6458 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.86881\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4736 - acc: 0.9125 - val_loss: 0.5937 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86881\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4639 - acc: 0.9186 - val_loss: 0.6003 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.86881 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4623 - acc: 0.9098 - val_loss: 0.6092 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.87129 to 0.87376, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4476 - acc: 0.9248 - val_loss: 0.6234 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87376\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4222 - acc: 0.9275 - val_loss: 0.6310 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87376\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4100 - acc: 0.9386 - val_loss: 0.6702 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87376\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4342 - acc: 0.9149 - val_loss: 0.6807 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87376\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4527 - acc: 0.9167 - val_loss: 0.5846 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.87376 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3993 - acc: 0.9362 - val_loss: 0.7669 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88119\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3910 - acc: 0.9393 - val_loss: 0.6378 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88119\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3671 - acc: 0.9471 - val_loss: 0.6932 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88119\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3897 - acc: 0.9398 - val_loss: 0.8553 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88119\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4154 - acc: 0.9245 - val_loss: 0.6985 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88119\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3645 - acc: 0.9510 - val_loss: 0.6573 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88119\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3470 - acc: 0.9510 - val_loss: 0.6334 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.88119 to 0.89109, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3290 - acc: 0.9627 - val_loss: 0.6589 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89109\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3250 - acc: 0.9618 - val_loss: 0.7804 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89109\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3576 - acc: 0.9504 - val_loss: 0.7330 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89109\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3577 - acc: 0.9444 - val_loss: 0.7620 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89109\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3354 - acc: 0.9570 - val_loss: 0.7091 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.89109 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3244 - acc: 0.9627 - val_loss: 0.7330 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89356\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2980 - acc: 0.9705 - val_loss: 0.7499 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89356\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2931 - acc: 0.9705 - val_loss: 0.7522 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00047: val_acc improved from 0.89356 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3259 - acc: 0.9612 - val_loss: 0.7032 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89356\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3215 - acc: 0.9618 - val_loss: 0.7508 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89356\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3118 - acc: 0.9681 - val_loss: 0.6644 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89356\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2828 - acc: 0.9735 - val_loss: 0.7317 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89356\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3069 - acc: 0.9624 - val_loss: 0.7269 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89356\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3536 - acc: 0.9492 - val_loss: 0.6354 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89356\n",
      "Epoch 54/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.3122 - acc: 0.9633 - val_loss: 0.6363 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89356\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2954 - acc: 0.9705 - val_loss: 0.8393 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89356\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2903 - acc: 0.9714 - val_loss: 0.6676 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89356\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2623 - acc: 0.9862 - val_loss: 0.7290 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.89356 to 0.89604, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2551 - acc: 0.9868 - val_loss: 0.7826 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00058: val_acc improved from 0.89604 to 0.89604, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2564 - acc: 0.9844 - val_loss: 0.7410 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.89604 to 0.90347, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2575 - acc: 0.9832 - val_loss: 0.7482 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90347\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2505 - acc: 0.9856 - val_loss: 0.7977 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90347\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2486 - acc: 0.9856 - val_loss: 0.9329 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90347\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2598 - acc: 0.9807 - val_loss: 0.8812 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90347\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2449 - acc: 0.9880 - val_loss: 0.8697 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90347\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2492 - acc: 0.9868 - val_loss: 0.8423 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90347\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2564 - acc: 0.9844 - val_loss: 0.9501 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90347\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3024 - acc: 0.9645 - val_loss: 0.6224 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90347\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2840 - acc: 0.9741 - val_loss: 0.6442 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.90347 to 0.91089, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2487 - acc: 0.9859 - val_loss: 0.6845 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.91089\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2444 - acc: 0.9877 - val_loss: 0.7050 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.91089\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2479 - acc: 0.9868 - val_loss: 0.9206 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.91089\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2429 - acc: 0.9874 - val_loss: 0.8253 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.91089\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2378 - acc: 0.9862 - val_loss: 0.8204 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.91089\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2593 - acc: 0.9811 - val_loss: 0.7587 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.91089\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2674 - acc: 0.9769 - val_loss: 0.7618 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.91089\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2401 - acc: 0.9868 - val_loss: 0.8102 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.91089\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2306 - acc: 0.9880 - val_loss: 0.8008 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.91089\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2345 - acc: 0.9904 - val_loss: 0.7168 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.91089\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2424 - acc: 0.9838 - val_loss: 0.8860 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.91089\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2238 - acc: 0.9916 - val_loss: 0.8272 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.91089\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2206 - acc: 0.9934 - val_loss: 0.7735 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.91089 to 0.91089, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2246 - acc: 0.9880 - val_loss: 0.7710 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.91089 to 0.91584, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001.h5\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2174 - acc: 0.9940 - val_loss: 0.8859 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.91584\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2434 - acc: 0.9838 - val_loss: 0.8622 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.91584\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2579 - acc: 0.9795 - val_loss: 0.7280 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.91584\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2302 - acc: 0.9898 - val_loss: 0.7709 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.91584\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2159 - acc: 0.9940 - val_loss: 0.8563 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.91584\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2087 - acc: 0.9964 - val_loss: 0.8148 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.91584\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2297 - acc: 0.9874 - val_loss: 0.8334 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.91584\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2174 - acc: 0.9916 - val_loss: 0.8642 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.91584\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2189 - acc: 0.9904 - val_loss: 0.8668 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91584\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2245 - acc: 0.9898 - val_loss: 0.9653 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91584\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2170 - acc: 0.9904 - val_loss: 0.8492 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.91584\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2146 - acc: 0.9934 - val_loss: 0.7694 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.91584\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2146 - acc: 0.9916 - val_loss: 0.9198 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.91584\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2260 - acc: 0.9868 - val_loss: 0.9256 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91584\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2285 - acc: 0.9880 - val_loss: 0.9039 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.91584\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2453 - acc: 0.9801 - val_loss: 0.9923 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.91584\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2307 - acc: 0.9874 - val_loss: 0.7058 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91584\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2194 - acc: 0.9928 - val_loss: 0.8451 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.91584\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2116 - acc: 0.9889 - val_loss: 0.7644 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.91584\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2039 - acc: 0.9958 - val_loss: 0.7999 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.91584\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2004 - acc: 0.9970 - val_loss: 0.9421 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.91584\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2009 - acc: 0.9958 - val_loss: 0.8642 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.91584\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2036 - acc: 0.9940 - val_loss: 0.9543 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.91584\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2166 - acc: 0.9880 - val_loss: 0.9586 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.91584\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2197 - acc: 0.9856 - val_loss: 0.8914 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.91584\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2244 - acc: 0.9874 - val_loss: 0.9197 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.91584\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2754 - acc: 0.9675 - val_loss: 0.8037 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.91584\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2471 - acc: 0.9771 - val_loss: 0.6917 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.91584\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2077 - acc: 0.9946 - val_loss: 0.7270 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.91584\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2079 - acc: 0.9916 - val_loss: 0.7781 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.91584\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2052 - acc: 0.9922 - val_loss: 0.8351 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.91584\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2031 - acc: 0.9940 - val_loss: 0.8061 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.91584\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1967 - acc: 0.9958 - val_loss: 0.9652 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.91584\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1980 - acc: 0.9958 - val_loss: 0.9183 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.91584\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1966 - acc: 0.9952 - val_loss: 0.8501 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.91584\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1978 - acc: 0.9940 - val_loss: 0.8676 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.91584\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1927 - acc: 0.9961 - val_loss: 0.8797 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.91584\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2031 - acc: 0.9940 - val_loss: 0.9215 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.91584\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2030 - acc: 0.9922 - val_loss: 0.8783 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.91584\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2079 - acc: 0.9940 - val_loss: 0.8488 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.91584\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2173 - acc: 0.9865 - val_loss: 0.7896 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.91584\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2230 - acc: 0.9835 - val_loss: 0.6915 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.91584\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2569 - acc: 0.9718 - val_loss: 0.8512 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.91584\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2428 - acc: 0.9793 - val_loss: 0.7118 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.91584\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1942 - acc: 0.9970 - val_loss: 0.7607 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.91584\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1982 - acc: 0.9958 - val_loss: 0.7057 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.91584\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2046 - acc: 0.9916 - val_loss: 0.7322 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.91584\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1899 - acc: 0.9976 - val_loss: 0.7621 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.91584\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1861 - acc: 0.9988 - val_loss: 0.7154 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.91584\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1870 - acc: 0.9988 - val_loss: 0.8280 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.91584\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1952 - acc: 0.9952 - val_loss: 0.7396 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.91584\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1900 - acc: 0.9964 - val_loss: 0.7478 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.91584\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1880 - acc: 0.9973 - val_loss: 0.8174 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.91584\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1899 - acc: 0.9952 - val_loss: 0.7163 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.91584\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1884 - acc: 0.9976 - val_loss: 0.8381 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.91584\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1884 - acc: 0.9970 - val_loss: 0.8090 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.91584\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1835 - acc: 0.9976 - val_loss: 0.7841 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.91584\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1795 - acc: 1.0000 - val_loss: 0.7833 - val_acc: 0.9084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00140: val_acc did not improve from 0.91584\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1809 - acc: 0.9976 - val_loss: 0.8022 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.91584\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1819 - acc: 0.9970 - val_loss: 0.7985 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.91584\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1777 - acc: 1.0000 - val_loss: 0.8877 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.91584\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1847 - acc: 0.9964 - val_loss: 0.9079 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.91584\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1862 - acc: 0.9940 - val_loss: 0.9296 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.91584\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1838 - acc: 0.9958 - val_loss: 0.8577 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.91584\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1772 - acc: 0.9988 - val_loss: 0.8960 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.91584\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1757 - acc: 0.9988 - val_loss: 0.9449 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.91584\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1797 - acc: 0.9982 - val_loss: 0.9242 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.91584\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1755 - acc: 0.9982 - val_loss: 0.8676 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.91584\n",
      "experiment: dropout=0.2 regularization0.05\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 26.7607 - acc: 0.7416 - val_loss: 15.9465 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 13.6236 - acc: 0.7514 - val_loss: 12.0545 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.4229 - acc: 0.7639 - val_loss: 10.9154 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 10.6327 - acc: 0.7556 - val_loss: 10.3678 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 10.1785 - acc: 0.7654 - val_loss: 9.9889 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 9.8670 - acc: 0.7640 - val_loss: 9.7354 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76238 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 9.6457 - acc: 0.7805 - val_loss: 9.5440 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79950\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 9.4514 - acc: 0.7877 - val_loss: 9.3434 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79950\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 9.2981 - acc: 0.7956 - val_loss: 9.2531 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79950\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 9.2019 - acc: 0.7735 - val_loss: 9.1349 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79950\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 9.0711 - acc: 0.7877 - val_loss: 8.9886 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79950\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 8.9840 - acc: 0.7811 - val_loss: 8.9476 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79950\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.8743 - acc: 0.7932 - val_loss: 8.8224 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.79950 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.7735 - acc: 0.8144 - val_loss: 8.7161 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.79950 to 0.80941, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.7040 - acc: 0.7944 - val_loss: 8.6756 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80941\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.6171 - acc: 0.8142 - val_loss: 8.6420 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.80941\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.5621 - acc: 0.8055 - val_loss: 8.5518 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.80941 to 0.80941, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.4754 - acc: 0.8193 - val_loss: 8.4533 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.80941\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.4401 - acc: 0.8085 - val_loss: 8.4160 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.80941\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.3649 - acc: 0.8186 - val_loss: 8.3485 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.80941\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.3145 - acc: 0.8199 - val_loss: 8.3184 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.80941\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.2781 - acc: 0.8001 - val_loss: 8.2276 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.80941 to 0.81188, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.2248 - acc: 0.8080 - val_loss: 8.1783 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.81188 to 0.81931, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.1564 - acc: 0.8160 - val_loss: 8.1271 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.81931\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.1237 - acc: 0.8125 - val_loss: 8.0842 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.81931 to 0.82178, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.0779 - acc: 0.8175 - val_loss: 8.0551 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.82178 to 0.82673, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.0250 - acc: 0.8337 - val_loss: 8.0668 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.82673\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.9764 - acc: 0.8307 - val_loss: 7.9624 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.82673 to 0.83168, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.9260 - acc: 0.8271 - val_loss: 7.9106 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83168\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 7.8727 - acc: 0.8454 - val_loss: 7.9022 - val_acc: 0.7970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83168\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.8662 - acc: 0.8271 - val_loss: 7.8458 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.83168 to 0.83416, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.8347 - acc: 0.8190 - val_loss: 7.7975 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.83416 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 7.7720 - acc: 0.8410 - val_loss: 7.7541 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83663\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.7420 - acc: 0.8319 - val_loss: 7.7569 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83663\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.7026 - acc: 0.8361 - val_loss: 7.7453 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83663\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.6605 - acc: 0.8422 - val_loss: 7.6543 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.83663 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.6429 - acc: 0.8359 - val_loss: 7.7327 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.84406\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.6346 - acc: 0.8196 - val_loss: 7.5970 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.84406\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.5846 - acc: 0.8364 - val_loss: 7.5651 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.84406\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.5454 - acc: 0.8385 - val_loss: 7.5355 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.84406\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.5068 - acc: 0.8377 - val_loss: 7.5020 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.84406\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.4759 - acc: 0.8498 - val_loss: 7.4873 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.84406\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.4462 - acc: 0.8445 - val_loss: 7.6044 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.84406\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.4473 - acc: 0.8274 - val_loss: 7.4153 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.84406\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.3967 - acc: 0.8517 - val_loss: 7.3928 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.84406\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.3467 - acc: 0.8658 - val_loss: 7.3717 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84406\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.3364 - acc: 0.8478 - val_loss: 7.3416 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.84406\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.3182 - acc: 0.8425 - val_loss: 7.3107 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.84406 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.2851 - acc: 0.8511 - val_loss: 7.3070 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.85149\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.2490 - acc: 0.8557 - val_loss: 7.2688 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.85149\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.2211 - acc: 0.8560 - val_loss: 7.2414 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.85149\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.2165 - acc: 0.8434 - val_loss: 7.2210 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.85149\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.1795 - acc: 0.8557 - val_loss: 7.1854 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.85149\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.1432 - acc: 0.8692 - val_loss: 7.2069 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.85149\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.1460 - acc: 0.8581 - val_loss: 7.2609 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.85149\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.1030 - acc: 0.8647 - val_loss: 7.1435 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.85149\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.0687 - acc: 0.8674 - val_loss: 7.1029 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.85149\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.0599 - acc: 0.8485 - val_loss: 7.0848 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.85149\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.0438 - acc: 0.8626 - val_loss: 7.0667 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.85149\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.9898 - acc: 0.8773 - val_loss: 7.0632 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.85149\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.9910 - acc: 0.8626 - val_loss: 7.0440 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.85149\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.9621 - acc: 0.8737 - val_loss: 7.0189 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.85149\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.9649 - acc: 0.8636 - val_loss: 6.9733 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.85149\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 6.9088 - acc: 0.8755 - val_loss: 6.9630 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.85149\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.9080 - acc: 0.8680 - val_loss: 7.0334 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.85149\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.8923 - acc: 0.8524 - val_loss: 6.9213 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.85149 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.8551 - acc: 0.8777 - val_loss: 6.9016 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.85644\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.8440 - acc: 0.8725 - val_loss: 6.9763 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.85644\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.8539 - acc: 0.8577 - val_loss: 6.9250 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.85644\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.8361 - acc: 0.8626 - val_loss: 6.8452 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.85644\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.7710 - acc: 0.8830 - val_loss: 6.8316 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.85644\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.7580 - acc: 0.8797 - val_loss: 6.8052 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.85644\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.7428 - acc: 0.8708 - val_loss: 6.7904 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.85644\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.7114 - acc: 0.8840 - val_loss: 6.7854 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.85644\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.7255 - acc: 0.8746 - val_loss: 6.7840 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.85644\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.7078 - acc: 0.8632 - val_loss: 6.7665 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.85644\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.6427 - acc: 0.8947 - val_loss: 6.7496 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.85644\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.6319 - acc: 0.8921 - val_loss: 6.7443 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.85644\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.6356 - acc: 0.8779 - val_loss: 6.7126 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.85644\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.5939 - acc: 0.8990 - val_loss: 6.7082 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.85644\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.6116 - acc: 0.8779 - val_loss: 6.6922 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.85644\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.5770 - acc: 0.8879 - val_loss: 6.6682 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.85644\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.5513 - acc: 0.8951 - val_loss: 6.6442 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.85644\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.5220 - acc: 0.9017 - val_loss: 6.6321 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.85644\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.5753 - acc: 0.8641 - val_loss: 6.6014 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.85644\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.5080 - acc: 0.9014 - val_loss: 6.6096 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.85644\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 6.4569 - acc: 0.9134 - val_loss: 6.6090 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.85644\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.4468 - acc: 0.9143 - val_loss: 6.6131 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.85644\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.4257 - acc: 0.9095 - val_loss: 6.5773 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.85644\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.4282 - acc: 0.9155 - val_loss: 6.5460 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.85644\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.4033 - acc: 0.9083 - val_loss: 6.6113 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.85644\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.4144 - acc: 0.9017 - val_loss: 6.5977 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.85644\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.3970 - acc: 0.9059 - val_loss: 6.5023 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.85644\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.3549 - acc: 0.9137 - val_loss: 6.4933 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.85644\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.3473 - acc: 0.9111 - val_loss: 6.4920 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.85644\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.3384 - acc: 0.9177 - val_loss: 6.4697 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.85644\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.3052 - acc: 0.9248 - val_loss: 6.5692 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.85644\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.3432 - acc: 0.9018 - val_loss: 6.4889 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.85644\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2948 - acc: 0.9155 - val_loss: 6.4403 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.85644\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.2823 - acc: 0.9200 - val_loss: 6.4743 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.85644\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2379 - acc: 0.9255 - val_loss: 6.4317 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.85644\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.2275 - acc: 0.9267 - val_loss: 6.4127 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.85644\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.2227 - acc: 0.9164 - val_loss: 6.3922 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.85644\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2015 - acc: 0.9285 - val_loss: 6.4282 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.85644\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.1846 - acc: 0.9269 - val_loss: 6.4288 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.85644\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.1612 - acc: 0.9299 - val_loss: 6.4223 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.85644\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.1895 - acc: 0.9149 - val_loss: 6.3565 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.85644\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.1639 - acc: 0.9167 - val_loss: 6.3550 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.85644\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.1200 - acc: 0.9372 - val_loss: 6.3483 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.85644\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.1522 - acc: 0.9194 - val_loss: 6.3298 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.85644\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.1122 - acc: 0.9306 - val_loss: 6.3137 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00111: val_acc improved from 0.85644 to 0.86139, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.1023 - acc: 0.9372 - val_loss: 6.3799 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.86139\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 6.0849 - acc: 0.9308 - val_loss: 6.3032 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.86139\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0651 - acc: 0.9402 - val_loss: 6.2883 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.86139\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.0815 - acc: 0.9197 - val_loss: 6.3445 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.86139\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.0512 - acc: 0.9297 - val_loss: 6.2869 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.86139\n",
      "Epoch 117/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 5.9998 - acc: 0.9468 - val_loss: 6.3067 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.86139\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0023 - acc: 0.9492 - val_loss: 6.2650 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.86139\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.9674 - acc: 0.9639 - val_loss: 6.2610 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.86139\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.9874 - acc: 0.9411 - val_loss: 6.2719 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.86139\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0238 - acc: 0.9233 - val_loss: 6.1993 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.86139\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.9684 - acc: 0.9465 - val_loss: 6.1768 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00122: val_acc improved from 0.86139 to 0.86386, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.9666 - acc: 0.9399 - val_loss: 6.1922 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.86386\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.9145 - acc: 0.9597 - val_loss: 6.2092 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.86386\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.8982 - acc: 0.9627 - val_loss: 6.2665 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.86386\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.9260 - acc: 0.9483 - val_loss: 6.1599 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00126: val_acc improved from 0.86386 to 0.86881, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 5.8797 - acc: 0.9681 - val_loss: 6.1819 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.86881\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.8670 - acc: 0.9621 - val_loss: 6.1530 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.86881\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.8630 - acc: 0.9609 - val_loss: 6.1547 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.86881\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.8347 - acc: 0.9648 - val_loss: 6.2069 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.86881\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.8733 - acc: 0.9428 - val_loss: 6.1557 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.86881\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.8280 - acc: 0.9564 - val_loss: 6.1344 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.86881\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.8210 - acc: 0.9636 - val_loss: 6.1338 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.86881\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.8021 - acc: 0.9699 - val_loss: 6.1011 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.86881\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.8110 - acc: 0.9462 - val_loss: 6.1388 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.86881\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 5.7916 - acc: 0.9624 - val_loss: 6.0748 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.86881\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.7718 - acc: 0.9597 - val_loss: 6.1638 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.86881\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.7640 - acc: 0.9594 - val_loss: 6.1198 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.86881\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.7699 - acc: 0.9516 - val_loss: 6.0574 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.86881\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.7317 - acc: 0.9675 - val_loss: 6.0864 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.86881\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.7506 - acc: 0.9567 - val_loss: 6.0630 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.86881\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.7113 - acc: 0.9699 - val_loss: 6.1236 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.86881\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.6945 - acc: 0.9751 - val_loss: 6.0376 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.86881\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.6955 - acc: 0.9712 - val_loss: 6.0412 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00144: val_acc improved from 0.86881 to 0.87376, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.6834 - acc: 0.9735 - val_loss: 6.0204 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.87376\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.6508 - acc: 0.9826 - val_loss: 6.0184 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00146: val_acc improved from 0.87376 to 0.87871, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.05.h5\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.6590 - acc: 0.9735 - val_loss: 6.0427 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.87871\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.7117 - acc: 0.9405 - val_loss: 6.0587 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.87871\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.6699 - acc: 0.9609 - val_loss: 5.9925 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.87871\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.6374 - acc: 0.9714 - val_loss: 6.0561 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.87871\n",
      "experiment: dropout=0.2 regularization0.01\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.6495 - acc: 0.7556 - val_loss: 4.5394 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.9326 - acc: 0.7414 - val_loss: 3.5307 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.2861 - acc: 0.7517 - val_loss: 3.0766 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.9874 - acc: 0.7508 - val_loss: 2.9046 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.7984 - acc: 0.7619 - val_loss: 2.7131 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.6817 - acc: 0.7625 - val_loss: 2.6342 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76238\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.5660 - acc: 0.7775 - val_loss: 2.5717 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.76238 to 0.77970, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 2.5015 - acc: 0.7925 - val_loss: 2.4407 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.77970 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.4328 - acc: 0.8040 - val_loss: 2.4017 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.79950 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 2.3664 - acc: 0.8139 - val_loss: 2.3469 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81683\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 2.3187 - acc: 0.8136 - val_loss: 2.3063 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81683 to 0.82178, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.2952 - acc: 0.8155 - val_loss: 2.3502 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82178\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.2959 - acc: 0.7977 - val_loss: 2.2666 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82178\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.2545 - acc: 0.7971 - val_loss: 2.2578 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82178\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.2016 - acc: 0.8190 - val_loss: 2.2215 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.82178\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.1867 - acc: 0.8217 - val_loss: 2.1613 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.82178 to 0.83168, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.1450 - acc: 0.8295 - val_loss: 2.1298 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83168\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.1114 - acc: 0.8409 - val_loss: 2.1172 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83168\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.1138 - acc: 0.8262 - val_loss: 2.0996 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.83168 to 0.83416, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0750 - acc: 0.8394 - val_loss: 2.1343 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83416\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0807 - acc: 0.8257 - val_loss: 2.1160 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83416\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0547 - acc: 0.8443 - val_loss: 2.0425 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83416\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0247 - acc: 0.8533 - val_loss: 2.0815 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83416\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0275 - acc: 0.8374 - val_loss: 2.0535 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.83416 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.9792 - acc: 0.8632 - val_loss: 2.0504 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83911\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.9807 - acc: 0.8575 - val_loss: 2.0371 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83911\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.9502 - acc: 0.8686 - val_loss: 2.0088 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83911\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.9284 - acc: 0.8671 - val_loss: 1.9808 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83911\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.9465 - acc: 0.8533 - val_loss: 2.0105 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83911\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.9145 - acc: 0.8783 - val_loss: 1.9812 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83911\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.8902 - acc: 0.8867 - val_loss: 1.9408 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.83911 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8824 - acc: 0.8776 - val_loss: 1.9636 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.84158 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8463 - acc: 0.8902 - val_loss: 1.9460 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.85149 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8226 - acc: 0.9008 - val_loss: 1.9437 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.85396\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8077 - acc: 0.9047 - val_loss: 1.9323 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.85396 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8509 - acc: 0.8779 - val_loss: 1.9658 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.85644\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8040 - acc: 0.9059 - val_loss: 1.8805 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.85644 to 0.86386, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8102 - acc: 0.8981 - val_loss: 1.8882 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.86386\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.7721 - acc: 0.9083 - val_loss: 1.9272 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.86386\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7447 - acc: 0.9149 - val_loss: 1.8914 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.86386 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7339 - acc: 0.9191 - val_loss: 1.8804 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.86634\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7386 - acc: 0.9180 - val_loss: 1.8940 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.86634 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7149 - acc: 0.9260 - val_loss: 1.8592 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.87129\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6994 - acc: 0.9317 - val_loss: 1.9041 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.87129\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6999 - acc: 0.9230 - val_loss: 1.9103 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.87129\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.6842 - acc: 0.9314 - val_loss: 1.8739 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.87129\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6569 - acc: 0.9398 - val_loss: 1.8693 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87129\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.6575 - acc: 0.9372 - val_loss: 1.9030 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.87129\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6916 - acc: 0.9182 - val_loss: 1.8250 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.87129\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.6776 - acc: 0.9180 - val_loss: 1.8446 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87129\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6416 - acc: 0.9360 - val_loss: 1.9328 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.87129\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.6548 - acc: 0.9321 - val_loss: 1.8573 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.87129\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.6039 - acc: 0.9507 - val_loss: 1.8662 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.87129\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5846 - acc: 0.9609 - val_loss: 1.8620 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.87129\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5738 - acc: 0.9651 - val_loss: 1.8756 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87129\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6216 - acc: 0.9396 - val_loss: 1.9124 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87129\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5830 - acc: 0.9519 - val_loss: 1.8148 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.87129 to 0.87376, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6136 - acc: 0.9362 - val_loss: 1.9144 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87376\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6012 - acc: 0.9444 - val_loss: 1.8808 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.87376\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5639 - acc: 0.9495 - val_loss: 1.8827 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.87376\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5708 - acc: 0.9528 - val_loss: 1.8391 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.87376\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5615 - acc: 0.9477 - val_loss: 1.8291 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00062: val_acc improved from 0.87376 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5186 - acc: 0.9729 - val_loss: 1.8252 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.87624 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5067 - acc: 0.9723 - val_loss: 1.8622 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89356\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5197 - acc: 0.9666 - val_loss: 1.8653 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89356\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5124 - acc: 0.9699 - val_loss: 1.8466 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89356\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.4842 - acc: 0.9763 - val_loss: 1.8539 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89356\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5090 - acc: 0.9597 - val_loss: 1.8307 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89356\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4769 - acc: 0.9741 - val_loss: 1.8568 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89356\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4951 - acc: 0.9664 - val_loss: 1.9942 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89356\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.5214 - acc: 0.9492 - val_loss: 1.8454 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89356\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4876 - acc: 0.9660 - val_loss: 1.8982 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89356\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.4680 - acc: 0.9765 - val_loss: 1.8940 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89356\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4543 - acc: 0.9777 - val_loss: 1.8664 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89356\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4929 - acc: 0.9591 - val_loss: 2.0051 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89356\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4767 - acc: 0.9636 - val_loss: 1.9892 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89356\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4762 - acc: 0.9645 - val_loss: 1.8076 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89356\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4461 - acc: 0.9747 - val_loss: 1.8226 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89356\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.4401 - acc: 0.9771 - val_loss: 1.8425 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89356\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4156 - acc: 0.9850 - val_loss: 1.8401 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89356\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4326 - acc: 0.9741 - val_loss: 1.7934 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89356\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4174 - acc: 0.9820 - val_loss: 1.8944 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89356\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4312 - acc: 0.9723 - val_loss: 1.8429 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89356\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4192 - acc: 0.9783 - val_loss: 1.8299 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89356\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4078 - acc: 0.9820 - val_loss: 1.8605 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89356\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4025 - acc: 0.9807 - val_loss: 1.8391 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89356\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4074 - acc: 0.9826 - val_loss: 1.8198 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89356\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3950 - acc: 0.9787 - val_loss: 1.8334 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89356\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3988 - acc: 0.9820 - val_loss: 1.7712 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.89356\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3811 - acc: 0.9850 - val_loss: 1.9352 - val_acc: 0.8688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00090: val_acc did not improve from 0.89356\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3879 - acc: 0.9789 - val_loss: 1.8027 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.89356\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3899 - acc: 0.9759 - val_loss: 1.7796 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.89356\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3936 - acc: 0.9783 - val_loss: 1.9255 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89356\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4028 - acc: 0.9753 - val_loss: 1.8860 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89356\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3919 - acc: 0.9775 - val_loss: 1.9793 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89356\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4080 - acc: 0.9705 - val_loss: 1.8336 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89356\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3832 - acc: 0.9805 - val_loss: 1.8586 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.89356\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3583 - acc: 0.9880 - val_loss: 1.8141 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.89356\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3508 - acc: 0.9874 - val_loss: 1.8468 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.89356\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3634 - acc: 0.9760 - val_loss: 1.7855 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89356\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3496 - acc: 0.9856 - val_loss: 1.8762 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89356\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3471 - acc: 0.9844 - val_loss: 1.9073 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89356\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3434 - acc: 0.9862 - val_loss: 1.8788 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89356\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3353 - acc: 0.9892 - val_loss: 1.8993 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89356\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3327 - acc: 0.9889 - val_loss: 1.8552 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89356\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3432 - acc: 0.9832 - val_loss: 1.9371 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89356\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3544 - acc: 0.9757 - val_loss: 1.9808 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.89356\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.3427 - acc: 0.9801 - val_loss: 1.7606 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.89356\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3209 - acc: 0.9898 - val_loss: 1.8689 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.89356\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3118 - acc: 0.9922 - val_loss: 1.9124 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.89356\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3135 - acc: 0.9904 - val_loss: 1.7860 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.89356\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3121 - acc: 0.9916 - val_loss: 1.8400 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.89356\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3238 - acc: 0.9807 - val_loss: 1.7941 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.89356\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3099 - acc: 0.9901 - val_loss: 1.7336 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.89356\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3135 - acc: 0.9868 - val_loss: 1.8106 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.89356\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3247 - acc: 0.9751 - val_loss: 1.7489 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.89356\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3215 - acc: 0.9820 - val_loss: 1.8383 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.89356\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2984 - acc: 0.9868 - val_loss: 1.8159 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.89356\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2939 - acc: 0.9910 - val_loss: 1.8786 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.89356\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2826 - acc: 0.9940 - val_loss: 1.8302 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.89356\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2798 - acc: 0.9940 - val_loss: 1.8863 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.89356\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2779 - acc: 0.9916 - val_loss: 1.7968 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.89356\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2727 - acc: 0.9940 - val_loss: 1.7776 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00123: val_acc improved from 0.89356 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2721 - acc: 0.9922 - val_loss: 1.8238 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.89356\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2682 - acc: 0.9934 - val_loss: 1.7656 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00125: val_acc improved from 0.89356 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.01.h5\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2758 - acc: 0.9910 - val_loss: 1.9320 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.89851\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4326 - acc: 0.9293 - val_loss: 1.7572 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.89851\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4045 - acc: 0.9380 - val_loss: 1.7802 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.89851\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3167 - acc: 0.9751 - val_loss: 1.7988 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.89851\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2979 - acc: 0.9814 - val_loss: 1.8344 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.89851\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2861 - acc: 0.9817 - val_loss: 1.8189 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.89851\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2722 - acc: 0.9886 - val_loss: 1.8176 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.89851\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.2621 - acc: 0.9916 - val_loss: 1.8130 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.89851\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2814 - acc: 0.9811 - val_loss: 1.8633 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.89851\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2675 - acc: 0.9874 - val_loss: 1.7287 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.89851\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2565 - acc: 0.9898 - val_loss: 1.7967 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.89851\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2510 - acc: 0.9922 - val_loss: 1.7514 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.89851\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2468 - acc: 0.9946 - val_loss: 1.8035 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.89851\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2498 - acc: 0.9916 - val_loss: 1.8186 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.89851\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2414 - acc: 0.9946 - val_loss: 1.7971 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.89851\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2330 - acc: 0.9952 - val_loss: 1.8301 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.89851\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.2304 - acc: 0.9952 - val_loss: 1.8042 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.89851\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2457 - acc: 0.9904 - val_loss: 1.9372 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.89851\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3098 - acc: 0.9633 - val_loss: 1.7036 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.89851\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2510 - acc: 0.9916 - val_loss: 1.7234 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.89851\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2368 - acc: 0.9910 - val_loss: 1.7099 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.89851\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2373 - acc: 0.9892 - val_loss: 1.8234 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.89851\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2340 - acc: 0.9916 - val_loss: 1.7942 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.89851\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2427 - acc: 0.9862 - val_loss: 1.9022 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.89851\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2612 - acc: 0.9795 - val_loss: 1.7377 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.89851\n",
      "experiment: dropout=0.2 regularization0.5\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 247.3223 - acc: 0.7342 - val_loss: 140.0616 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.5.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 119.8925 - acc: 0.7601 - val_loss: 107.4796 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 102.9223 - acc: 0.7619 - val_loss: 99.3974 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 97.3987 - acc: 0.7598 - val_loss: 95.6263 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.5.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 94.3402 - acc: 0.7646 - val_loss: 93.1314 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 92.2130 - acc: 0.7625 - val_loss: 91.3376 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76238\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 90.5759 - acc: 0.7654 - val_loss: 89.8143 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76238\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 89.2249 - acc: 0.7551 - val_loss: 88.5828 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76238\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 88.0347 - acc: 0.7639 - val_loss: 87.4681 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76238\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 86.9948 - acc: 0.7639 - val_loss: 86.4896 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76238\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 86.0706 - acc: 0.7610 - val_loss: 85.6080 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76238\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 85.2259 - acc: 0.7595 - val_loss: 84.7994 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76238\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 84.4402 - acc: 0.7625 - val_loss: 84.0508 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76238\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 83.7213 - acc: 0.7566 - val_loss: 83.3459 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76238\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 83.0294 - acc: 0.7595 - val_loss: 82.6910 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76238\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 82.3928 - acc: 0.7580 - val_loss: 82.0663 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76238\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 81.7849 - acc: 0.7639 - val_loss: 81.4760 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76238\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 81.2104 - acc: 0.7639 - val_loss: 80.9168 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76238\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 80.6619 - acc: 0.7654 - val_loss: 80.3794 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76238\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 80.1365 - acc: 0.7639 - val_loss: 79.8669 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76238\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 79.6335 - acc: 0.7639 - val_loss: 79.3762 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76238\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 79.1566 - acc: 0.7610 - val_loss: 78.9063 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76238\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 78.6841 - acc: 0.7639 - val_loss: 78.4482 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76238\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 78.2349 - acc: 0.7639 - val_loss: 78.0065 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76238\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 77.8059 - acc: 0.7610 - val_loss: 77.5816 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76238\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 77.3899 - acc: 0.7610 - val_loss: 77.1785 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76238\n",
      "Epoch 27/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 76.9847 - acc: 0.7610 - val_loss: 76.7743 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76238\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 76.5875 - acc: 0.7625 - val_loss: 76.3831 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76238\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 76.2110 - acc: 0.7610 - val_loss: 76.0085 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76238\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 75.8489 - acc: 0.7566 - val_loss: 75.6442 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76238\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 75.4749 - acc: 0.7610 - val_loss: 75.2853 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76238\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 75.1315 - acc: 0.7566 - val_loss: 74.9451 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76238\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 74.7804 - acc: 0.7669 - val_loss: 74.6146 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76238\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 74.4502 - acc: 0.7580 - val_loss: 74.2690 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.76238\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 74.1232 - acc: 0.7595 - val_loss: 73.9478 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.76238\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 73.8043 - acc: 0.7580 - val_loss: 73.6311 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.76238\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 73.4921 - acc: 0.7580 - val_loss: 73.3227 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.76238\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 73.1917 - acc: 0.7551 - val_loss: 73.0214 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.76238\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 72.8916 - acc: 0.7610 - val_loss: 72.7290 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.76238\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 72.5865 - acc: 0.7625 - val_loss: 72.4371 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.76238\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 72.3073 - acc: 0.7625 - val_loss: 72.1556 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.76238\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 72.0301 - acc: 0.7610 - val_loss: 71.8761 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.76238\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 71.7507 - acc: 0.7639 - val_loss: 71.6037 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.76238\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 71.4802 - acc: 0.7639 - val_loss: 71.3366 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.76238\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 71.2217 - acc: 0.7580 - val_loss: 71.0744 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.76238\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.9615 - acc: 0.7580 - val_loss: 70.8168 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.76238\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.7014 - acc: 0.7595 - val_loss: 70.5639 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.76238\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.4567 - acc: 0.7551 - val_loss: 70.3149 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.76238\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.1935 - acc: 0.7654 - val_loss: 70.0716 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.76238\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 69.9587 - acc: 0.7639 - val_loss: 69.8300 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.76238\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 69.7159 - acc: 0.7669 - val_loss: 69.5932 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.76238\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 69.4861 - acc: 0.7625 - val_loss: 69.3605 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.76238\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 69.2522 - acc: 0.7639 - val_loss: 69.1316 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.76238\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 69.0385 - acc: 0.7566 - val_loss: 68.9110 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.76238\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 68.8052 - acc: 0.7654 - val_loss: 68.6852 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.76238\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 68.5870 - acc: 0.7580 - val_loss: 68.4659 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.76238\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 68.3585 - acc: 0.7684 - val_loss: 68.2501 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.76238\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 68.1560 - acc: 0.7595 - val_loss: 68.0382 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.76238\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 67.9432 - acc: 0.7610 - val_loss: 67.8292 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.76238\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 67.7388 - acc: 0.7580 - val_loss: 67.6222 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.76238\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 67.5281 - acc: 0.7625 - val_loss: 67.4187 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.76238\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 67.3282 - acc: 0.7625 - val_loss: 67.2182 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.76238\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 67.1341 - acc: 0.7595 - val_loss: 67.0202 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.76238\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 66.9333 - acc: 0.7566 - val_loss: 66.8254 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.76238\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 66.7353 - acc: 0.7625 - val_loss: 66.6324 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.76238\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 66.5425 - acc: 0.7610 - val_loss: 66.4425 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.76238\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 66.3617 - acc: 0.7551 - val_loss: 66.2555 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.76238\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 66.1713 - acc: 0.7625 - val_loss: 66.0690 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.76238\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 65.9879 - acc: 0.7625 - val_loss: 65.8872 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.76238\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 65.8067 - acc: 0.7610 - val_loss: 65.7063 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.76238\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 65.6238 - acc: 0.7625 - val_loss: 65.5269 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.76238\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 65.4393 - acc: 0.7654 - val_loss: 65.3501 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.76238\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 65.2713 - acc: 0.7610 - val_loss: 65.1757 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.76238\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 65.0955 - acc: 0.7610 - val_loss: 65.0034 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.76238\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 64.9245 - acc: 0.7639 - val_loss: 64.8331 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.76238\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 64.7587 - acc: 0.7595 - val_loss: 64.6651 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.76238\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 64.5898 - acc: 0.7595 - val_loss: 64.4987 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.76238\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 64.4261 - acc: 0.7625 - val_loss: 64.3343 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.76238\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 64.2610 - acc: 0.7610 - val_loss: 64.1713 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.76238\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 64.1018 - acc: 0.7580 - val_loss: 64.0104 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.76238\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 63.9391 - acc: 0.7610 - val_loss: 63.8513 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.76238\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 63.7810 - acc: 0.7610 - val_loss: 63.6957 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.76238\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 63.6193 - acc: 0.7654 - val_loss: 63.5398 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.76238\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 63.4664 - acc: 0.7639 - val_loss: 63.3850 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.76238\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 63.3180 - acc: 0.7566 - val_loss: 63.2332 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.76238\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 63.1655 - acc: 0.7625 - val_loss: 63.0811 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.76238\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 63.0137 - acc: 0.7625 - val_loss: 62.9324 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.76238\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 62.8685 - acc: 0.7625 - val_loss: 62.7840 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.76238\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 62.7149 - acc: 0.7639 - val_loss: 62.6471 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.76238\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 62.5710 - acc: 0.7625 - val_loss: 62.4946 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.76238\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 62.4293 - acc: 0.7595 - val_loss: 62.3497 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.76238\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 62.2881 - acc: 0.7625 - val_loss: 62.2077 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.76238\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 62.1429 - acc: 0.7639 - val_loss: 62.0667 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.76238\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 62.0011 - acc: 0.7654 - val_loss: 61.9277 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.76238\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 61.8651 - acc: 0.7639 - val_loss: 61.7895 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.76238\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.7245 - acc: 0.7639 - val_loss: 61.6530 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.76238\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.5887 - acc: 0.7639 - val_loss: 61.5179 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.76238\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 61.4554 - acc: 0.7639 - val_loss: 61.3840 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.76238\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 61.3271 - acc: 0.7625 - val_loss: 61.2514 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.76238\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.1955 - acc: 0.7595 - val_loss: 61.1198 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.76238\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.0628 - acc: 0.7595 - val_loss: 60.9893 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.76238\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 60.9275 - acc: 0.7639 - val_loss: 60.8601 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.76238\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 60.8032 - acc: 0.7625 - val_loss: 60.7322 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.76238\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.6689 - acc: 0.7669 - val_loss: 60.6054 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.76238\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.5432 - acc: 0.7639 - val_loss: 60.4793 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.76238\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 60.4275 - acc: 0.7625 - val_loss: 60.3549 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.76238\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.3004 - acc: 0.7610 - val_loss: 60.2312 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.76238\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.1764 - acc: 0.7654 - val_loss: 60.1088 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.76238\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.0503 - acc: 0.7654 - val_loss: 59.9874 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.76238\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.9335 - acc: 0.7610 - val_loss: 59.8671 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.76238\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.8098 - acc: 0.7625 - val_loss: 59.7476 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.76238\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 59.6894 - acc: 0.7669 - val_loss: 59.6293 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.76238\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.5754 - acc: 0.7639 - val_loss: 59.5118 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.76238\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.4588 - acc: 0.7625 - val_loss: 59.3955 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.76238\n",
      "Epoch 115/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 59.3435 - acc: 0.7625 - val_loss: 59.2799 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.76238\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.2278 - acc: 0.7625 - val_loss: 59.1654 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.76238\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.1151 - acc: 0.7654 - val_loss: 59.0524 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.76238\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.0075 - acc: 0.7610 - val_loss: 58.9400 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.76238\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 58.8973 - acc: 0.7566 - val_loss: 58.8281 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.76238\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 58.7854 - acc: 0.7551 - val_loss: 58.7169 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.76238\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 58.6752 - acc: 0.7595 - val_loss: 58.6072 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.76238\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 58.5577 - acc: 0.7625 - val_loss: 58.4979 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.76238\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 58.4529 - acc: 0.7625 - val_loss: 58.3898 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.76238\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 58.3378 - acc: 0.7654 - val_loss: 58.2825 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.76238\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 58.2336 - acc: 0.7639 - val_loss: 58.1766 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.76238\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 58.1243 - acc: 0.7669 - val_loss: 58.0703 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.76238\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 58.0203 - acc: 0.7669 - val_loss: 57.9651 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.76238\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 57.9171 - acc: 0.7654 - val_loss: 57.8611 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.76238\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 57.8163 - acc: 0.7625 - val_loss: 57.7588 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.76238\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 57.7170 - acc: 0.7580 - val_loss: 57.6550 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.76238\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 57.6095 - acc: 0.7639 - val_loss: 57.5530 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.76238\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 57.5078 - acc: 0.7639 - val_loss: 57.4518 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.76238\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 57.4055 - acc: 0.7639 - val_loss: 57.3515 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.76238\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 57.3052 - acc: 0.7625 - val_loss: 57.2520 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.76238\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 57.2049 - acc: 0.7639 - val_loss: 57.1533 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.76238\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 57.1015 - acc: 0.7684 - val_loss: 57.0553 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.76238\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 57.0154 - acc: 0.7610 - val_loss: 56.9579 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.76238\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 56.9179 - acc: 0.7625 - val_loss: 56.8613 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.76238\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 56.8186 - acc: 0.7654 - val_loss: 56.7660 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.76238\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 56.7304 - acc: 0.7595 - val_loss: 56.6704 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.76238\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 56.6264 - acc: 0.7625 - val_loss: 56.5755 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.76238\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 56.5335 - acc: 0.7625 - val_loss: 56.4815 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.76238\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 56.4484 - acc: 0.7595 - val_loss: 56.3885 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.76238\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 56.3494 - acc: 0.7610 - val_loss: 56.2956 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.76238\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 56.2490 - acc: 0.7654 - val_loss: 56.2037 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.76238\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 56.1591 - acc: 0.7625 - val_loss: 56.1124 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.76238\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 56.0776 - acc: 0.7566 - val_loss: 56.0214 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.76238\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 55.9859 - acc: 0.7625 - val_loss: 55.9313 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.76238\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 55.8909 - acc: 0.7639 - val_loss: 55.8418 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.76238\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 55.8048 - acc: 0.7639 - val_loss: 55.7529 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.76238\n",
      "experiment: dropout=0.2 regularization0.1\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 51.9930 - acc: 0.7463 - val_loss: 30.2081 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 25.5999 - acc: 0.7625 - val_loss: 22.6905 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 21.6170 - acc: 0.7654 - val_loss: 20.7372 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 20.2768 - acc: 0.7652 - val_loss: 19.8470 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 19.5592 - acc: 0.7598 - val_loss: 19.3248 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 19.0736 - acc: 0.7672 - val_loss: 18.8522 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76238 to 0.76733, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 18.6975 - acc: 0.7666 - val_loss: 18.5229 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.76733 to 0.77228, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 18.3861 - acc: 0.7787 - val_loss: 18.2386 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.77228 to 0.78713, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 18.1489 - acc: 0.7724 - val_loss: 18.0200 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.78713 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 17.9446 - acc: 0.7787 - val_loss: 17.8686 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.79950 to 0.81436, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 17.7539 - acc: 0.7799 - val_loss: 17.6410 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.81436\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 17.5889 - acc: 0.7856 - val_loss: 17.4811 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.81436\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 17.4096 - acc: 0.7887 - val_loss: 17.3024 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.81436\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 17.2496 - acc: 0.7860 - val_loss: 17.1647 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.81436\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 17.0960 - acc: 0.7946 - val_loss: 17.0347 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.81436\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.9807 - acc: 0.7968 - val_loss: 16.9711 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.81436\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 16.8599 - acc: 0.8025 - val_loss: 16.7832 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.81436\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.7432 - acc: 0.7838 - val_loss: 16.7268 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.81436\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.6286 - acc: 0.7917 - val_loss: 16.5838 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.81436\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.5329 - acc: 0.7904 - val_loss: 16.4743 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.81436\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.4297 - acc: 0.7944 - val_loss: 16.3524 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.81436 to 0.82178, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 16.3083 - acc: 0.8069 - val_loss: 16.2583 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.82178\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.2346 - acc: 0.7980 - val_loss: 16.1751 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.82178 to 0.83168, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.1183 - acc: 0.8051 - val_loss: 16.0708 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83168\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 16.0267 - acc: 0.8190 - val_loss: 16.0013 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83168\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.9779 - acc: 0.7956 - val_loss: 15.9123 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83168\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.8662 - acc: 0.8163 - val_loss: 15.8227 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83168\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.7893 - acc: 0.8049 - val_loss: 15.8037 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83168\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.7244 - acc: 0.8121 - val_loss: 15.6770 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83168\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.6551 - acc: 0.8016 - val_loss: 15.6133 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83168\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.5597 - acc: 0.8205 - val_loss: 15.5478 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83168\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.4965 - acc: 0.8046 - val_loss: 15.4481 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83168\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.4127 - acc: 0.8277 - val_loss: 15.4243 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83168\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 15.3504 - acc: 0.8217 - val_loss: 15.3805 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83168\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.3081 - acc: 0.8067 - val_loss: 15.2751 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83168\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.2368 - acc: 0.8112 - val_loss: 15.1922 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.83168\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.1630 - acc: 0.8205 - val_loss: 15.1275 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.83168\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.0923 - acc: 0.8190 - val_loss: 15.0661 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.83168 to 0.83416, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.0324 - acc: 0.8223 - val_loss: 15.0078 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83416\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.9787 - acc: 0.8187 - val_loss: 14.9640 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.83416 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.9255 - acc: 0.8199 - val_loss: 14.9957 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83663\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 14.8733 - acc: 0.8158 - val_loss: 14.8346 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.83663\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 14.8036 - acc: 0.8244 - val_loss: 14.7849 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.83663\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 14.7745 - acc: 0.8073 - val_loss: 14.7297 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.83663\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.7045 - acc: 0.8221 - val_loss: 14.6662 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.83663\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.6394 - acc: 0.8278 - val_loss: 14.6203 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.83663 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 14.5901 - acc: 0.8353 - val_loss: 14.5643 - val_acc: 0.8292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00047: val_acc did not improve from 0.83911\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.5605 - acc: 0.8067 - val_loss: 14.6064 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.83911\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.5005 - acc: 0.8187 - val_loss: 14.5080 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.83911\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.4394 - acc: 0.8268 - val_loss: 14.4295 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.83911\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.3873 - acc: 0.8344 - val_loss: 14.3727 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.83911\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 14.3426 - acc: 0.8319 - val_loss: 14.3353 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.83911\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 14.3027 - acc: 0.8280 - val_loss: 14.2816 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.83911\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.2465 - acc: 0.8388 - val_loss: 14.2375 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.83911\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.1985 - acc: 0.8401 - val_loss: 14.2144 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.83911\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.1781 - acc: 0.8266 - val_loss: 14.1432 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.83911\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.0992 - acc: 0.8458 - val_loss: 14.1315 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.83911\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 14.0803 - acc: 0.8361 - val_loss: 14.0750 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.83911\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 14.0171 - acc: 0.8506 - val_loss: 14.0270 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.83911 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.9815 - acc: 0.8404 - val_loss: 13.9918 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84158\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.9444 - acc: 0.8349 - val_loss: 13.9479 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84158\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.9190 - acc: 0.8268 - val_loss: 13.9044 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84158\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.8639 - acc: 0.8391 - val_loss: 13.8630 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84158\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.8123 - acc: 0.8454 - val_loss: 13.8270 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84158\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.7709 - acc: 0.8488 - val_loss: 13.8447 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84158\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 13.7427 - acc: 0.8454 - val_loss: 13.7476 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.84158\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.7138 - acc: 0.8430 - val_loss: 13.7243 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.84158\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.6692 - acc: 0.8415 - val_loss: 13.6834 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.84158\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.6450 - acc: 0.8329 - val_loss: 13.6408 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.84158\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.5905 - acc: 0.8440 - val_loss: 13.6444 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.84158\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.5919 - acc: 0.8169 - val_loss: 13.5959 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.84158\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.5309 - acc: 0.8413 - val_loss: 13.5346 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.84158\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.4916 - acc: 0.8413 - val_loss: 13.4981 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.84158\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.4431 - acc: 0.8419 - val_loss: 13.4624 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.84158\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.4153 - acc: 0.8578 - val_loss: 13.4353 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.84158\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.3708 - acc: 0.8628 - val_loss: 13.4007 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.84158\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.3526 - acc: 0.8455 - val_loss: 13.4048 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.84158\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.3202 - acc: 0.8430 - val_loss: 13.3712 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.84158\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.3029 - acc: 0.8302 - val_loss: 13.2957 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.84158\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.2389 - acc: 0.8512 - val_loss: 13.3012 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.84158\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.2011 - acc: 0.8587 - val_loss: 13.2802 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.84158\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.1650 - acc: 0.8596 - val_loss: 13.2098 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.84158 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.1269 - acc: 0.8734 - val_loss: 13.1533 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.84406\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.1111 - acc: 0.8577 - val_loss: 13.1301 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.84406\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.0677 - acc: 0.8565 - val_loss: 13.1053 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.84406\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.0646 - acc: 0.8398 - val_loss: 13.0986 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.84406\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 13.0190 - acc: 0.8602 - val_loss: 13.0327 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.84406\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.9969 - acc: 0.8533 - val_loss: 13.0516 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.84406\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.9538 - acc: 0.8455 - val_loss: 12.9842 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.84406\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.9309 - acc: 0.8587 - val_loss: 12.9472 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00090: val_acc improved from 0.84406 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.9084 - acc: 0.8548 - val_loss: 12.9306 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.84901\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.8740 - acc: 0.8596 - val_loss: 12.9353 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.84901\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.8395 - acc: 0.8633 - val_loss: 12.8935 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.84901\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.8281 - acc: 0.8440 - val_loss: 12.8618 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.84901\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 12.7720 - acc: 0.8728 - val_loss: 12.8066 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.84901\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.7596 - acc: 0.8635 - val_loss: 12.8226 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.84901\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.7324 - acc: 0.8641 - val_loss: 12.7484 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.84901\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.6985 - acc: 0.8566 - val_loss: 12.7353 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.84901\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.6523 - acc: 0.8752 - val_loss: 12.6959 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.84901\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.6199 - acc: 0.8773 - val_loss: 12.7409 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.84901\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.6283 - acc: 0.8593 - val_loss: 12.6507 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.84901\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.5908 - acc: 0.8734 - val_loss: 12.6177 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00102: val_acc improved from 0.84901 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.5664 - acc: 0.8716 - val_loss: 12.6407 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.85149\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.5369 - acc: 0.8632 - val_loss: 12.5700 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.85149\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.4952 - acc: 0.8707 - val_loss: 12.5395 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.85149\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.4931 - acc: 0.8512 - val_loss: 12.5620 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.85149\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.4563 - acc: 0.8698 - val_loss: 12.5014 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.85149\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.4193 - acc: 0.8794 - val_loss: 12.4861 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.85149\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.3916 - acc: 0.8798 - val_loss: 12.4467 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.85149\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.3808 - acc: 0.8729 - val_loss: 12.4544 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.85149\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.3537 - acc: 0.8719 - val_loss: 12.4340 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.85149\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.3493 - acc: 0.8689 - val_loss: 12.3832 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.85149\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 12.3051 - acc: 0.8752 - val_loss: 12.3430 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00113: val_acc improved from 0.85149 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.2767 - acc: 0.8788 - val_loss: 12.3315 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.85644\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.2507 - acc: 0.8812 - val_loss: 12.3105 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.85644\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.2218 - acc: 0.8872 - val_loss: 12.2900 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.85644\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.1955 - acc: 0.8758 - val_loss: 12.2628 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.85644\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.1776 - acc: 0.8804 - val_loss: 12.2421 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.85644\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.1339 - acc: 0.8960 - val_loss: 12.2585 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.85644\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.1493 - acc: 0.8746 - val_loss: 12.1947 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.85644\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.0861 - acc: 0.8995 - val_loss: 12.1804 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.85644\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.1048 - acc: 0.8731 - val_loss: 12.2393 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.85644\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 12.0811 - acc: 0.8749 - val_loss: 12.1334 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.85644\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.0390 - acc: 0.8822 - val_loss: 12.1279 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.85644\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.0058 - acc: 0.8935 - val_loss: 12.0985 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.85644\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.9988 - acc: 0.8942 - val_loss: 12.0910 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.85644\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.9724 - acc: 0.8855 - val_loss: 12.0505 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.85644\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.9937 - acc: 0.8740 - val_loss: 12.0510 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.85644\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.9484 - acc: 0.8855 - val_loss: 12.0225 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.85644\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.9064 - acc: 0.8920 - val_loss: 12.0091 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.85644\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.8788 - acc: 0.8927 - val_loss: 11.9711 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.85644\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.8570 - acc: 0.9007 - val_loss: 11.9574 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.85644\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.8305 - acc: 0.8990 - val_loss: 11.9309 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.85644\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.8251 - acc: 0.8941 - val_loss: 11.9119 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.85644\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.8254 - acc: 0.8963 - val_loss: 11.8939 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.85644\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.7992 - acc: 0.8839 - val_loss: 11.8952 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.85644\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.7695 - acc: 0.8855 - val_loss: 11.8427 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.85644\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.7244 - acc: 0.9062 - val_loss: 11.8433 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.85644\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.7107 - acc: 0.9013 - val_loss: 11.8293 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.85644\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.6869 - acc: 0.9047 - val_loss: 11.7913 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.85644\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.6742 - acc: 0.9062 - val_loss: 11.7739 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.85644\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 11.6420 - acc: 0.9122 - val_loss: 11.8028 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.85644\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.6118 - acc: 0.9218 - val_loss: 11.7415 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.85644\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.5930 - acc: 0.9185 - val_loss: 11.7028 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.85644\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.5741 - acc: 0.9203 - val_loss: 11.7343 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.85644\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.5593 - acc: 0.9125 - val_loss: 11.6850 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.85644\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.5386 - acc: 0.9212 - val_loss: 11.6687 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00147: val_acc improved from 0.85644 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.1.h5\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.5710 - acc: 0.8936 - val_loss: 11.7658 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.85891\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.5316 - acc: 0.8984 - val_loss: 11.6445 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.85891\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 11.4931 - acc: 0.9188 - val_loss: 11.6218 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.85891\n",
      "experiment: dropout=0.3 regularization0.001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.3373 - acc: 0.7313 - val_loss: 1.1623 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.0546 - acc: 0.7604 - val_loss: 0.9733 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.9426 - acc: 0.7655 - val_loss: 0.9067 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.76485, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.8796 - acc: 0.7529 - val_loss: 0.8369 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76485 to 0.77228, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.8245 - acc: 0.7835 - val_loss: 0.7930 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77228 to 0.79703, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7948 - acc: 0.7871 - val_loss: 0.7680 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.79703 to 0.80693, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7597 - acc: 0.7935 - val_loss: 0.7739 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.80693\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7421 - acc: 0.7994 - val_loss: 0.7087 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.80693 to 0.82178, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7106 - acc: 0.8243 - val_loss: 0.6923 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.82178 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7263 - acc: 0.7929 - val_loss: 0.6957 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.83663\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6708 - acc: 0.8292 - val_loss: 0.6697 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.83663\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6636 - acc: 0.8283 - val_loss: 0.6782 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.83663 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6592 - acc: 0.8250 - val_loss: 0.6585 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.84158\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6464 - acc: 0.8295 - val_loss: 0.6689 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.84158\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6248 - acc: 0.8443 - val_loss: 0.6347 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.84158\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6130 - acc: 0.8467 - val_loss: 0.6267 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.84158 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5849 - acc: 0.8620 - val_loss: 0.6470 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84406\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5737 - acc: 0.8614 - val_loss: 0.6719 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.84406\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5654 - acc: 0.8614 - val_loss: 0.6699 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84406\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5473 - acc: 0.8714 - val_loss: 0.6280 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.84406 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5238 - acc: 0.8825 - val_loss: 0.6168 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85396\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5648 - acc: 0.8755 - val_loss: 0.6417 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.85396\n",
      "Epoch 23/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 0.5440 - acc: 0.8740 - val_loss: 0.6102 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.85396 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4900 - acc: 0.9071 - val_loss: 0.6468 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.85396\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4918 - acc: 0.8906 - val_loss: 0.6287 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.85396 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4944 - acc: 0.8941 - val_loss: 0.6143 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.85891\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4581 - acc: 0.9113 - val_loss: 0.6206 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.85891 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4579 - acc: 0.9044 - val_loss: 0.5812 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.87129 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4624 - acc: 0.9065 - val_loss: 0.6248 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87624\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4755 - acc: 0.9047 - val_loss: 0.6045 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87624\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4276 - acc: 0.9158 - val_loss: 0.7003 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87624\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4131 - acc: 0.9249 - val_loss: 0.8728 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87624\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4388 - acc: 0.9122 - val_loss: 0.6549 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87624\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3971 - acc: 0.9290 - val_loss: 0.6268 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87624\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3977 - acc: 0.9315 - val_loss: 0.6908 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87624\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4291 - acc: 0.9209 - val_loss: 0.6680 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87624\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3810 - acc: 0.9378 - val_loss: 0.6149 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87624\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3823 - acc: 0.9378 - val_loss: 0.6905 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.87624 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3737 - acc: 0.9384 - val_loss: 0.8556 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87624\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3687 - acc: 0.9396 - val_loss: 0.7565 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87624\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3541 - acc: 0.9453 - val_loss: 0.7439 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87624\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3548 - acc: 0.9444 - val_loss: 0.6593 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.87624 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3409 - acc: 0.9522 - val_loss: 0.6949 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88119\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3566 - acc: 0.9411 - val_loss: 0.7722 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88119\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3374 - acc: 0.9561 - val_loss: 0.6704 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.88119 to 0.90347, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3416 - acc: 0.9522 - val_loss: 0.6764 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.90347\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3100 - acc: 0.9630 - val_loss: 0.7583 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.90347\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2991 - acc: 0.9717 - val_loss: 0.8208 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.90347\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2826 - acc: 0.9747 - val_loss: 0.9016 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.90347\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3119 - acc: 0.9672 - val_loss: 0.6645 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.90347\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2956 - acc: 0.9684 - val_loss: 0.7997 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.90347\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2939 - acc: 0.9693 - val_loss: 0.7509 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.90347\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3127 - acc: 0.9588 - val_loss: 0.6794 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.90347\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2977 - acc: 0.9681 - val_loss: 0.6967 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.90347\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2872 - acc: 0.9741 - val_loss: 0.7704 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.90347\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2798 - acc: 0.9775 - val_loss: 0.7981 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.90347\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2780 - acc: 0.9771 - val_loss: 0.8503 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.90347\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2727 - acc: 0.9783 - val_loss: 0.8169 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.90347\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3104 - acc: 0.9600 - val_loss: 0.8305 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.90347\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2878 - acc: 0.9684 - val_loss: 0.7292 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90347\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2794 - acc: 0.9721 - val_loss: 0.7319 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90347\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2913 - acc: 0.9714 - val_loss: 1.1175 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90347\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2960 - acc: 0.9687 - val_loss: 0.7470 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90347\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2531 - acc: 0.9814 - val_loss: 0.7979 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90347\n",
      "Epoch 65/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 0.2528 - acc: 0.9795 - val_loss: 0.7842 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90347\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2475 - acc: 0.9844 - val_loss: 0.8804 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90347\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2466 - acc: 0.9850 - val_loss: 0.9707 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90347\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2822 - acc: 0.9739 - val_loss: 0.7400 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.90347\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2611 - acc: 0.9777 - val_loss: 0.7255 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90347\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2436 - acc: 0.9850 - val_loss: 0.8723 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90347\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2386 - acc: 0.9844 - val_loss: 0.9610 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.90347\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2817 - acc: 0.9729 - val_loss: 0.7915 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.90347\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2987 - acc: 0.9672 - val_loss: 0.7855 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90347\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2708 - acc: 0.9715 - val_loss: 0.7954 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.90347\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2427 - acc: 0.9874 - val_loss: 0.7873 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.90347\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2345 - acc: 0.9886 - val_loss: 0.8370 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.90347\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2313 - acc: 0.9886 - val_loss: 0.7970 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.90347\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2295 - acc: 0.9880 - val_loss: 0.8244 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.90347\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2341 - acc: 0.9892 - val_loss: 0.7887 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00079: val_acc improved from 0.90347 to 0.91089, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2778 - acc: 0.9733 - val_loss: 0.8264 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.91089\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2452 - acc: 0.9807 - val_loss: 0.7077 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.91089 to 0.91584, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001.h5\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2270 - acc: 0.9892 - val_loss: 0.8647 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.91584\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2318 - acc: 0.9874 - val_loss: 0.8321 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.91584\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2454 - acc: 0.9775 - val_loss: 0.8385 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.91584\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2389 - acc: 0.9847 - val_loss: 0.7550 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.91584\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2260 - acc: 0.9922 - val_loss: 0.7946 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.91584\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2183 - acc: 0.9922 - val_loss: 0.8642 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.91584\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2178 - acc: 0.9928 - val_loss: 0.9391 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.91584\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2218 - acc: 0.9874 - val_loss: 0.8825 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.91584\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2206 - acc: 0.9916 - val_loss: 0.8659 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.91584\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2100 - acc: 0.9958 - val_loss: 0.9650 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91584\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2194 - acc: 0.9934 - val_loss: 0.9858 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91584\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2469 - acc: 0.9820 - val_loss: 0.8468 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.91584\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2326 - acc: 0.9916 - val_loss: 0.8684 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.91584\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2272 - acc: 0.9874 - val_loss: 0.8445 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.91584\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2307 - acc: 0.9850 - val_loss: 0.8669 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91584\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2218 - acc: 0.9886 - val_loss: 0.9417 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.91584\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2324 - acc: 0.9823 - val_loss: 0.8661 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.91584\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2267 - acc: 0.9892 - val_loss: 0.7624 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91584\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2101 - acc: 0.9934 - val_loss: 0.7935 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.91584\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2120 - acc: 0.9952 - val_loss: 0.8173 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.91584\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2029 - acc: 0.9964 - val_loss: 0.9182 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.91584\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2358 - acc: 0.9832 - val_loss: 0.7911 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.91584\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2503 - acc: 0.9807 - val_loss: 0.7306 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.91584\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2147 - acc: 0.9910 - val_loss: 0.7855 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.91584\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2099 - acc: 0.9940 - val_loss: 0.7939 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.91584\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2074 - acc: 0.9922 - val_loss: 0.8585 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.91584\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1992 - acc: 0.9958 - val_loss: 0.8122 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.91584\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 0.2095 - acc: 0.9910 - val_loss: 0.8086 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.91584\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2156 - acc: 0.9898 - val_loss: 1.1178 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.91584\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2031 - acc: 0.9952 - val_loss: 0.8514 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.91584\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2102 - acc: 0.9934 - val_loss: 0.8573 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.91584\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2169 - acc: 0.9850 - val_loss: 0.8687 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.91584\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2111 - acc: 0.9910 - val_loss: 0.8892 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.91584\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2041 - acc: 0.9946 - val_loss: 0.7942 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.91584\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1964 - acc: 0.9964 - val_loss: 0.8588 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.91584\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2019 - acc: 0.9958 - val_loss: 0.9103 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.91584\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2069 - acc: 0.9940 - val_loss: 0.8066 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.91584\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1987 - acc: 0.9952 - val_loss: 0.8361 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.91584\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1992 - acc: 0.9952 - val_loss: 0.7901 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.91584\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1963 - acc: 0.9964 - val_loss: 0.8291 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.91584\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2194 - acc: 0.9883 - val_loss: 0.7608 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.91584\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2317 - acc: 0.9807 - val_loss: 0.8391 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.91584\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2067 - acc: 0.9940 - val_loss: 0.8567 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.91584\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2050 - acc: 0.9928 - val_loss: 0.8708 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.91584\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2056 - acc: 0.9910 - val_loss: 0.8742 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.91584\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2025 - acc: 0.9946 - val_loss: 0.8968 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.91584\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1997 - acc: 0.9934 - val_loss: 0.8909 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.91584\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2105 - acc: 0.9898 - val_loss: 0.8788 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.91584\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1904 - acc: 0.9982 - val_loss: 0.9703 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.91584\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1909 - acc: 0.9982 - val_loss: 0.9302 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.91584\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2244 - acc: 0.9847 - val_loss: 0.8451 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.91584\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2095 - acc: 0.9880 - val_loss: 0.8775 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.91584\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2033 - acc: 0.9913 - val_loss: 0.8624 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.91584\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1965 - acc: 0.9940 - val_loss: 0.8299 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.91584\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2044 - acc: 0.9904 - val_loss: 0.7263 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.91584\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1896 - acc: 0.9982 - val_loss: 0.8752 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.91584\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2158 - acc: 0.9886 - val_loss: 0.8506 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.91584\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2334 - acc: 0.9801 - val_loss: 0.7541 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.91584\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2359 - acc: 0.9807 - val_loss: 0.6859 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.91584\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2026 - acc: 0.9934 - val_loss: 0.7320 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.91584\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1886 - acc: 0.9970 - val_loss: 0.7515 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.91584\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1866 - acc: 0.9982 - val_loss: 0.7962 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.91584\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1954 - acc: 0.9946 - val_loss: 0.7707 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.91584\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1890 - acc: 0.9970 - val_loss: 0.8156 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.91584\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1888 - acc: 0.9964 - val_loss: 0.8160 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.91584\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1864 - acc: 0.9958 - val_loss: 0.7969 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.91584\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1834 - acc: 0.9988 - val_loss: 0.8348 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.91584\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1824 - acc: 0.9982 - val_loss: 0.8738 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.91584\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1799 - acc: 0.9994 - val_loss: 0.9019 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.91584\n",
      "experiment: dropout=0.3 regularization0.05\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 27.4680 - acc: 0.7231 - val_loss: 16.4566 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 13.8650 - acc: 0.7598 - val_loss: 12.1602 - val_acc: 0.7624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 11.5154 - acc: 0.7613 - val_loss: 10.9989 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 10.7222 - acc: 0.7663 - val_loss: 10.4594 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 10.2862 - acc: 0.7657 - val_loss: 10.0993 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 9.9857 - acc: 0.7660 - val_loss: 9.8570 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76238 to 0.77723, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 9.7874 - acc: 0.7676 - val_loss: 9.6896 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77723\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 9.6024 - acc: 0.7637 - val_loss: 9.5481 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.77723\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 9.4649 - acc: 0.7859 - val_loss: 9.4275 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.77723\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 9.3448 - acc: 0.7928 - val_loss: 9.3443 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.77723\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 9.2628 - acc: 0.7664 - val_loss: 9.1774 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.77723 to 0.78960, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 9.1504 - acc: 0.7842 - val_loss: 9.1049 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.78960 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 9.0698 - acc: 0.7940 - val_loss: 9.0288 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79950\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.9869 - acc: 0.7992 - val_loss: 8.9259 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79950\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.9040 - acc: 0.8039 - val_loss: 8.8704 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79950\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.8434 - acc: 0.8033 - val_loss: 8.7929 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79950\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.7615 - acc: 0.8099 - val_loss: 8.7232 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.79950\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.7128 - acc: 0.8049 - val_loss: 8.6699 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.79950\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.6482 - acc: 0.8073 - val_loss: 8.6231 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.79950 to 0.80941, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.6248 - acc: 0.7910 - val_loss: 8.5630 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.80941\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.5600 - acc: 0.8049 - val_loss: 8.5249 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.80941\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.5458 - acc: 0.7827 - val_loss: 8.4970 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.80941\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 8.4757 - acc: 0.8018 - val_loss: 8.4345 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.80941\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.4308 - acc: 0.7965 - val_loss: 8.3891 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.80941\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.3835 - acc: 0.7999 - val_loss: 8.3281 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.80941 to 0.81436, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.3155 - acc: 0.8082 - val_loss: 8.3020 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.81436\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.2688 - acc: 0.8178 - val_loss: 8.2809 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.81436\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.2661 - acc: 0.8018 - val_loss: 8.2499 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.81436\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.2227 - acc: 0.7965 - val_loss: 8.1885 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.81436\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.1618 - acc: 0.8198 - val_loss: 8.1527 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81436\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.1358 - acc: 0.8126 - val_loss: 8.1056 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.81436\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.0868 - acc: 0.8236 - val_loss: 8.0585 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.81436 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 8.0458 - acc: 0.8178 - val_loss: 8.0413 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.81683\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 8.0104 - acc: 0.8202 - val_loss: 7.9984 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.81683 to 0.82178, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.9733 - acc: 0.8181 - val_loss: 8.0175 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.82178\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.9681 - acc: 0.8128 - val_loss: 7.9422 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.82178\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.8945 - acc: 0.8367 - val_loss: 7.9023 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.82178 to 0.82426, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.8694 - acc: 0.8331 - val_loss: 7.8673 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.82426\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.8391 - acc: 0.8313 - val_loss: 7.8297 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.82426\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.8166 - acc: 0.8388 - val_loss: 7.8014 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.82426\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.7810 - acc: 0.8305 - val_loss: 7.8237 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.82426\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.7857 - acc: 0.8205 - val_loss: 7.7461 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.82426\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.7386 - acc: 0.8274 - val_loss: 7.7585 - val_acc: 0.8094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00043: val_acc did not improve from 0.82426\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.7180 - acc: 0.8267 - val_loss: 7.6877 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.82426\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.6861 - acc: 0.8296 - val_loss: 7.6710 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.82426 to 0.83168, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.6611 - acc: 0.8271 - val_loss: 7.6401 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.83168 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.6309 - acc: 0.8302 - val_loss: 7.6174 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.83911\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.5898 - acc: 0.8502 - val_loss: 7.6240 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.83911\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.5702 - acc: 0.8395 - val_loss: 7.5633 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.83911 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 7.5369 - acc: 0.8355 - val_loss: 7.5645 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.84158\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.5159 - acc: 0.8401 - val_loss: 7.5054 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.84158\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.4844 - acc: 0.8400 - val_loss: 7.4900 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.84158\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.4693 - acc: 0.8446 - val_loss: 7.4536 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.84158\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.4459 - acc: 0.8377 - val_loss: 7.4959 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.84158\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.4240 - acc: 0.8335 - val_loss: 7.4276 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.84158\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.4489 - acc: 0.8112 - val_loss: 7.3970 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.84158\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.3988 - acc: 0.8290 - val_loss: 7.3842 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.84158\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.3653 - acc: 0.8406 - val_loss: 7.3634 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00058: val_acc improved from 0.84158 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.3344 - acc: 0.8406 - val_loss: 7.3798 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84901\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.3092 - acc: 0.8395 - val_loss: 7.3163 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84901\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.2972 - acc: 0.8500 - val_loss: 7.3502 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84901\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.2701 - acc: 0.8433 - val_loss: 7.2697 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84901\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.2350 - acc: 0.8509 - val_loss: 7.2482 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84901\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.2163 - acc: 0.8482 - val_loss: 7.2254 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84901\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.1921 - acc: 0.8545 - val_loss: 7.2075 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84901\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.1900 - acc: 0.8449 - val_loss: 7.1882 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.84901 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.1780 - acc: 0.8377 - val_loss: 7.2179 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.85149\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.1510 - acc: 0.8416 - val_loss: 7.1523 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.85149\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 7.1209 - acc: 0.8482 - val_loss: 7.1429 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.85149\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.1366 - acc: 0.8307 - val_loss: 7.3358 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.85149\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.1345 - acc: 0.8349 - val_loss: 7.1331 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.85149\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.0668 - acc: 0.8578 - val_loss: 7.0816 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.85149\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.0365 - acc: 0.8653 - val_loss: 7.0804 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.85149\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 7.0086 - acc: 0.8608 - val_loss: 7.0546 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00074: val_acc improved from 0.85149 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.9856 - acc: 0.8719 - val_loss: 7.0380 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.85644\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.9783 - acc: 0.8551 - val_loss: 7.0670 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.85644\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.9777 - acc: 0.8629 - val_loss: 7.0034 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.85644\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.9428 - acc: 0.8659 - val_loss: 7.0064 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.85644\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.9183 - acc: 0.8686 - val_loss: 6.9708 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.85644\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.8915 - acc: 0.8734 - val_loss: 6.9431 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.85644\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.9136 - acc: 0.8581 - val_loss: 6.9429 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.85644\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.8697 - acc: 0.8714 - val_loss: 6.9343 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.85644\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.8549 - acc: 0.8716 - val_loss: 6.9322 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.85644\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.8338 - acc: 0.8761 - val_loss: 6.8990 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.85644\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.8108 - acc: 0.8692 - val_loss: 6.9005 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.85644\n",
      "Epoch 86/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 6.8018 - acc: 0.8791 - val_loss: 6.8919 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.85644\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.8051 - acc: 0.8671 - val_loss: 6.9119 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.85644\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.7596 - acc: 0.8789 - val_loss: 6.8468 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.85644\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.7570 - acc: 0.8845 - val_loss: 6.8188 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.85644\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.7416 - acc: 0.8843 - val_loss: 6.8027 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.85644\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.7614 - acc: 0.8569 - val_loss: 6.8343 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.85644\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.7245 - acc: 0.8711 - val_loss: 6.7977 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.85644\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.6802 - acc: 0.8884 - val_loss: 6.7912 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.85644\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.6622 - acc: 0.8869 - val_loss: 6.7625 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.85644\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.6914 - acc: 0.8593 - val_loss: 6.7300 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.85644\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.6640 - acc: 0.8788 - val_loss: 6.8132 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.85644\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.6548 - acc: 0.8725 - val_loss: 6.7163 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.85644\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.6259 - acc: 0.8831 - val_loss: 6.7218 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.85644\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.6039 - acc: 0.8918 - val_loss: 6.6972 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.85644\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.5695 - acc: 0.8900 - val_loss: 6.7008 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.85644\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.5632 - acc: 0.8885 - val_loss: 6.6630 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.85644\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.5572 - acc: 0.8914 - val_loss: 6.6537 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.85644\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.5286 - acc: 0.9005 - val_loss: 6.6695 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.85644\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.5172 - acc: 0.8893 - val_loss: 6.6354 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.85644\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.5149 - acc: 0.8863 - val_loss: 6.6210 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.85644\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.4886 - acc: 0.8993 - val_loss: 6.6029 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00106: val_acc improved from 0.85644 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 6.5003 - acc: 0.8821 - val_loss: 6.5841 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.85891\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.4534 - acc: 0.9131 - val_loss: 6.6002 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.85891\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.4267 - acc: 0.9065 - val_loss: 6.5488 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.85891\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.4391 - acc: 0.9017 - val_loss: 6.5772 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.85891\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.4213 - acc: 0.9020 - val_loss: 6.5211 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00111: val_acc improved from 0.85891 to 0.86386, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.4073 - acc: 0.9071 - val_loss: 6.5118 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.86386\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.3830 - acc: 0.9053 - val_loss: 6.4979 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00113: val_acc improved from 0.86386 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 6.3587 - acc: 0.9230 - val_loss: 6.4924 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.86634\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.3394 - acc: 0.9194 - val_loss: 6.5095 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.86634\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.3212 - acc: 0.9206 - val_loss: 6.4834 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.86634\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.3445 - acc: 0.9119 - val_loss: 6.5841 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.86634\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.3569 - acc: 0.8963 - val_loss: 6.4298 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.86634\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2865 - acc: 0.9228 - val_loss: 6.4273 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.86634\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.2814 - acc: 0.9206 - val_loss: 6.4344 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.86634\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2672 - acc: 0.9179 - val_loss: 6.4322 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.86634\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.2446 - acc: 0.9233 - val_loss: 6.4353 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.86634\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2623 - acc: 0.9101 - val_loss: 6.5229 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.86634\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.2820 - acc: 0.8957 - val_loss: 6.4082 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.86634\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2307 - acc: 0.9236 - val_loss: 6.4256 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.86634\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2190 - acc: 0.9176 - val_loss: 6.4368 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.86634\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2033 - acc: 0.9218 - val_loss: 6.3570 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.86634\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.1738 - acc: 0.9263 - val_loss: 6.3733 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00128: val_acc improved from 0.86634 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.1844 - acc: 0.9239 - val_loss: 6.3570 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.86634\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.2189 - acc: 0.8900 - val_loss: 6.3504 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.86634\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.1405 - acc: 0.9317 - val_loss: 6.3808 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.86634\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.1260 - acc: 0.9294 - val_loss: 6.4334 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.86634\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.1149 - acc: 0.9354 - val_loss: 6.3336 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.86634\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.0854 - acc: 0.9495 - val_loss: 6.3383 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.86634\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0878 - acc: 0.9362 - val_loss: 6.3524 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.86634\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0945 - acc: 0.9311 - val_loss: 6.4018 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.86634\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0778 - acc: 0.9381 - val_loss: 6.3124 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.86634\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.0529 - acc: 0.9416 - val_loss: 6.2939 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.86634\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.0313 - acc: 0.9483 - val_loss: 6.2739 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.86634\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0291 - acc: 0.9393 - val_loss: 6.3853 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.86634\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0252 - acc: 0.9369 - val_loss: 6.2469 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.86634\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 6.0086 - acc: 0.9384 - val_loss: 6.3422 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.86634\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.9979 - acc: 0.9441 - val_loss: 6.2476 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00143: val_acc improved from 0.86634 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.9705 - acc: 0.9561 - val_loss: 6.2294 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00144: val_acc improved from 0.87129 to 0.87376, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.05.h5\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.9713 - acc: 0.9432 - val_loss: 6.2494 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.87376\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.9660 - acc: 0.9495 - val_loss: 6.2370 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.87376\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 5.9273 - acc: 0.9591 - val_loss: 6.2510 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.87376\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 6.0387 - acc: 0.9020 - val_loss: 6.1608 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.87376\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.9757 - acc: 0.9323 - val_loss: 6.2177 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.87376\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 5.9600 - acc: 0.9362 - val_loss: 6.1695 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.87376\n",
      "experiment: dropout=0.3 regularization0.01\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.7908 - acc: 0.7390 - val_loss: 4.6375 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 3.9956 - acc: 0.7528 - val_loss: 3.4939 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.78218, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 3.3206 - acc: 0.7310 - val_loss: 3.0963 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.78218\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.9878 - acc: 0.7805 - val_loss: 2.8648 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.78218\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.7986 - acc: 0.7823 - val_loss: 2.7158 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78218 to 0.79455, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 2.6766 - acc: 0.7997 - val_loss: 2.6214 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.79455 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.5882 - acc: 0.7973 - val_loss: 2.5427 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.79950 to 0.81436, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.5069 - acc: 0.8148 - val_loss: 2.4683 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81436\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.4800 - acc: 0.7974 - val_loss: 2.4192 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81436\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 2.4165 - acc: 0.8157 - val_loss: 2.3811 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.81436 to 0.83168, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.3897 - acc: 0.8045 - val_loss: 2.3395 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.83168\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.3430 - acc: 0.8082 - val_loss: 2.3111 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.83168\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.3156 - acc: 0.8034 - val_loss: 2.2911 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.83168\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.2669 - acc: 0.8274 - val_loss: 2.2782 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.83168\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.2491 - acc: 0.8187 - val_loss: 2.2261 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.83168\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.2137 - acc: 0.8310 - val_loss: 2.2111 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.83168\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 2.2044 - acc: 0.8202 - val_loss: 2.2469 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83168\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.1784 - acc: 0.8295 - val_loss: 2.1788 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83168\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.1522 - acc: 0.8373 - val_loss: 2.1516 - val_acc: 0.8490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00019: val_acc improved from 0.83168 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.1394 - acc: 0.8256 - val_loss: 2.1540 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.84901\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.1184 - acc: 0.8415 - val_loss: 2.1142 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.84901 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0989 - acc: 0.8422 - val_loss: 2.0964 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.84901 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0797 - acc: 0.8457 - val_loss: 2.0844 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.85149 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0674 - acc: 0.8400 - val_loss: 2.0993 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.85149\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0556 - acc: 0.8485 - val_loss: 2.0574 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.85149 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0368 - acc: 0.8476 - val_loss: 2.0730 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.85396\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 2.0237 - acc: 0.8524 - val_loss: 2.0415 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.85396 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 2.0132 - acc: 0.8614 - val_loss: 2.0283 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.85644 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.9891 - acc: 0.8629 - val_loss: 2.0410 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.85891\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 2.0042 - acc: 0.8515 - val_loss: 2.0178 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.85891\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.9677 - acc: 0.8653 - val_loss: 2.0082 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.85891\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.9399 - acc: 0.8740 - val_loss: 2.0441 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.85891\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.9290 - acc: 0.8806 - val_loss: 2.0204 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.85891\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.9253 - acc: 0.8782 - val_loss: 1.9874 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.85891\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8867 - acc: 0.8960 - val_loss: 2.0290 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.85891\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.8941 - acc: 0.8863 - val_loss: 1.9765 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.85891\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.8729 - acc: 0.8963 - val_loss: 1.9600 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.85891\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8582 - acc: 0.9011 - val_loss: 1.9968 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.85891\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.8571 - acc: 0.8987 - val_loss: 1.9835 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.85891\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.8519 - acc: 0.8936 - val_loss: 1.9615 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.85891\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.8360 - acc: 0.8981 - val_loss: 1.9684 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.85891\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.8504 - acc: 0.8858 - val_loss: 1.9228 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.85891 to 0.86139, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7870 - acc: 0.9209 - val_loss: 2.0471 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.86139\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.8288 - acc: 0.8846 - val_loss: 1.9262 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.86139\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.7884 - acc: 0.9035 - val_loss: 1.9366 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.86139\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.7802 - acc: 0.9116 - val_loss: 1.9058 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.86139 to 0.87871, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7324 - acc: 0.9311 - val_loss: 1.9076 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87871\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7446 - acc: 0.9311 - val_loss: 1.9157 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.87871\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.7341 - acc: 0.9257 - val_loss: 1.9544 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.87871\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7075 - acc: 0.9318 - val_loss: 1.9266 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87871\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.7189 - acc: 0.9206 - val_loss: 1.9144 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.87871\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.7168 - acc: 0.9269 - val_loss: 1.9457 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.87871\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.6858 - acc: 0.9362 - val_loss: 1.9402 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.87871\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6859 - acc: 0.9402 - val_loss: 1.9294 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.87871\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6878 - acc: 0.9305 - val_loss: 1.9994 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87871\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.6902 - acc: 0.9246 - val_loss: 1.9851 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87871\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6868 - acc: 0.9236 - val_loss: 1.9270 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.87871\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.6327 - acc: 0.9528 - val_loss: 1.9067 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87871\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.6671 - acc: 0.9323 - val_loss: 1.9104 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.87871\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6410 - acc: 0.9480 - val_loss: 1.9426 - val_acc: 0.8441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00060: val_acc did not improve from 0.87871\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.6323 - acc: 0.9405 - val_loss: 1.9442 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.87871\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.6356 - acc: 0.9414 - val_loss: 1.8820 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.87871\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.6135 - acc: 0.9543 - val_loss: 1.8579 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.87871\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5900 - acc: 0.9543 - val_loss: 1.8731 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.87871\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.5872 - acc: 0.9498 - val_loss: 1.9578 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.87871\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.6049 - acc: 0.9459 - val_loss: 1.9014 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.87871\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5931 - acc: 0.9465 - val_loss: 1.8435 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.87871\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5885 - acc: 0.9549 - val_loss: 1.8682 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.87871 to 0.89109, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5446 - acc: 0.9642 - val_loss: 1.9022 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89109\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5615 - acc: 0.9576 - val_loss: 1.8943 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89109\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5426 - acc: 0.9609 - val_loss: 1.9215 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89109\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5428 - acc: 0.9645 - val_loss: 1.8493 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89109\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5759 - acc: 0.9453 - val_loss: 1.8299 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89109\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5143 - acc: 0.9729 - val_loss: 1.9165 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89109\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5294 - acc: 0.9657 - val_loss: 1.8666 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89109\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5288 - acc: 0.9618 - val_loss: 2.0541 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89109\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5091 - acc: 0.9672 - val_loss: 1.8631 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89109\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5176 - acc: 0.9633 - val_loss: 1.8753 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89109\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.5024 - acc: 0.9663 - val_loss: 1.9595 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89109\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.5108 - acc: 0.9657 - val_loss: 1.9585 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89109\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4960 - acc: 0.9721 - val_loss: 1.8868 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89109\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5386 - acc: 0.9471 - val_loss: 1.8677 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89109\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5014 - acc: 0.9669 - val_loss: 1.8653 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89109\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4696 - acc: 0.9741 - val_loss: 1.9254 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89109\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4772 - acc: 0.9733 - val_loss: 1.9523 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89109\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.5139 - acc: 0.9564 - val_loss: 1.8526 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89109\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4936 - acc: 0.9657 - val_loss: 2.0057 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89109\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4543 - acc: 0.9777 - val_loss: 1.9057 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89109\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4352 - acc: 0.9832 - val_loss: 1.9492 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.89109\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4438 - acc: 0.9795 - val_loss: 1.8707 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.89109\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4527 - acc: 0.9735 - val_loss: 1.8208 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.89109\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4461 - acc: 0.9759 - val_loss: 1.8642 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.89109\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4237 - acc: 0.9844 - val_loss: 1.8891 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89109\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4270 - acc: 0.9835 - val_loss: 1.9259 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89109\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4310 - acc: 0.9783 - val_loss: 1.9686 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89109\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4191 - acc: 0.9838 - val_loss: 1.9020 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89109\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4325 - acc: 0.9729 - val_loss: 1.9880 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00097: val_acc improved from 0.89109 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4227 - acc: 0.9777 - val_loss: 1.8964 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.89356\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4028 - acc: 0.9850 - val_loss: 1.8512 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.89356\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3953 - acc: 0.9862 - val_loss: 1.9461 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89356\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3985 - acc: 0.9832 - val_loss: 1.9427 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89356\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4493 - acc: 0.9634 - val_loss: 1.8450 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89356\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4131 - acc: 0.9795 - val_loss: 1.8440 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89356\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4078 - acc: 0.9807 - val_loss: 1.8385 - val_acc: 0.8787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89356\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3921 - acc: 0.9783 - val_loss: 1.8424 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89356\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3887 - acc: 0.9862 - val_loss: 1.7986 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89356\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4403 - acc: 0.9615 - val_loss: 1.8395 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.89356\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3815 - acc: 0.9838 - val_loss: 1.8182 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.89356\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3788 - acc: 0.9862 - val_loss: 1.8356 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00109: val_acc improved from 0.89356 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3755 - acc: 0.9874 - val_loss: 1.9006 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.89851\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3804 - acc: 0.9826 - val_loss: 1.8225 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.89851\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4008 - acc: 0.9751 - val_loss: 1.8553 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.89851\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3666 - acc: 0.9844 - val_loss: 1.9801 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.89851\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3617 - acc: 0.9844 - val_loss: 1.8918 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.89851\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3797 - acc: 0.9765 - val_loss: 1.9191 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.89851\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3637 - acc: 0.9789 - val_loss: 1.8334 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.89851\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3579 - acc: 0.9853 - val_loss: 1.8237 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.89851\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3512 - acc: 0.9850 - val_loss: 1.8360 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.89851\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3397 - acc: 0.9898 - val_loss: 1.8548 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.89851\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3596 - acc: 0.9829 - val_loss: 1.8450 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.89851\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3347 - acc: 0.9886 - val_loss: 1.8457 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.89851\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3298 - acc: 0.9886 - val_loss: 1.9155 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.89851\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3337 - acc: 0.9874 - val_loss: 1.8086 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.89851\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3245 - acc: 0.9904 - val_loss: 1.9245 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.89851\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3159 - acc: 0.9940 - val_loss: 1.9010 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.89851\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.4376 - acc: 0.9519 - val_loss: 2.2780 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.89851\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.4983 - acc: 0.9158 - val_loss: 1.6179 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.89851\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3523 - acc: 0.9820 - val_loss: 1.7525 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.89851\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3274 - acc: 0.9865 - val_loss: 1.7457 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00129: val_acc improved from 0.89851 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.01.h5\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3255 - acc: 0.9880 - val_loss: 1.8051 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.89851\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3164 - acc: 0.9868 - val_loss: 1.8241 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.89851\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3060 - acc: 0.9916 - val_loss: 1.7690 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.89851\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3035 - acc: 0.9922 - val_loss: 1.8411 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.89851\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3017 - acc: 0.9946 - val_loss: 1.7674 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.89851\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2951 - acc: 0.9928 - val_loss: 1.8228 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.89851\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3053 - acc: 0.9895 - val_loss: 1.9088 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.89851\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3026 - acc: 0.9898 - val_loss: 1.8492 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.89851\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2857 - acc: 0.9940 - val_loss: 1.8626 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.89851\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2988 - acc: 0.9886 - val_loss: 1.9992 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.89851\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3147 - acc: 0.9847 - val_loss: 1.7689 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.89851\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2861 - acc: 0.9922 - val_loss: 1.7863 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.89851\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.2822 - acc: 0.9946 - val_loss: 1.7757 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.89851\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2695 - acc: 0.9982 - val_loss: 1.8219 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.89851\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2662 - acc: 0.9970 - val_loss: 1.9600 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.89851\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2867 - acc: 0.9868 - val_loss: 1.8984 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.89851\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2826 - acc: 0.9892 - val_loss: 1.9288 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.89851\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2781 - acc: 0.9892 - val_loss: 1.8740 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.89851\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.2694 - acc: 0.9946 - val_loss: 1.7969 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.89851\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2566 - acc: 0.9958 - val_loss: 1.7771 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.89851\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.2550 - acc: 0.9946 - val_loss: 1.9068 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.89851\n",
      "experiment: dropout=0.3 regularization0.5\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 259.2735 - acc: 0.7489 - val_loss: 151.1052 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.5.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 127.3510 - acc: 0.7568 - val_loss: 112.5029 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.5.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 106.9995 - acc: 0.7577 - val_loss: 102.6616 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 100.3062 - acc: 0.7654 - val_loss: 98.1808 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 96.7696 - acc: 0.7654 - val_loss: 95.4329 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 94.4313 - acc: 0.7595 - val_loss: 93.4436 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76238\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 92.6746 - acc: 0.7610 - val_loss: 91.8842 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76238\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 91.2654 - acc: 0.7595 - val_loss: 90.6010 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76238\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 90.0698 - acc: 0.7654 - val_loss: 89.5010 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76238\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 89.0409 - acc: 0.7566 - val_loss: 88.5316 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76238\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 88.1106 - acc: 0.7639 - val_loss: 87.6592 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76238\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 87.2836 - acc: 0.7625 - val_loss: 86.8634 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76238\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 86.5230 - acc: 0.7595 - val_loss: 86.1308 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76238\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 85.8124 - acc: 0.7580 - val_loss: 85.4492 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76238\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 85.1525 - acc: 0.7669 - val_loss: 84.8106 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76238\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 84.5259 - acc: 0.7610 - val_loss: 84.2111 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76238\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 83.9444 - acc: 0.7610 - val_loss: 83.6397 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76238\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 83.3869 - acc: 0.7639 - val_loss: 83.0982 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76238\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 82.8679 - acc: 0.7580 - val_loss: 82.5818 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76238\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 82.3535 - acc: 0.7595 - val_loss: 82.0883 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76238\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 81.8775 - acc: 0.7625 - val_loss: 81.6167 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76238\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 81.4027 - acc: 0.7639 - val_loss: 81.1571 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76238\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 80.9612 - acc: 0.7580 - val_loss: 80.7174 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76238\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 80.5236 - acc: 0.7639 - val_loss: 80.2935 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76238\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 80.0979 - acc: 0.7639 - val_loss: 79.8834 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76238\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 79.6998 - acc: 0.7610 - val_loss: 79.4848 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76238\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 79.3040 - acc: 0.7639 - val_loss: 79.0997 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76238\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 78.9348 - acc: 0.7580 - val_loss: 78.7269 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76238\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 78.5507 - acc: 0.7625 - val_loss: 78.3626 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76238\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 78.2118 - acc: 0.7536 - val_loss: 78.0072 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76238\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 77.8522 - acc: 0.7610 - val_loss: 77.6644 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76238\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 77.5084 - acc: 0.7654 - val_loss: 77.3262 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76238\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 77.1812 - acc: 0.7610 - val_loss: 76.9966 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76238\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 76.8527 - acc: 0.7580 - val_loss: 76.6757 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.76238\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 76.5292 - acc: 0.7610 - val_loss: 76.3663 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.76238\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 76.2205 - acc: 0.7610 - val_loss: 76.0557 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.76238\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 75.9301 - acc: 0.7566 - val_loss: 75.7559 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.76238\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 75.6272 - acc: 0.7625 - val_loss: 75.4619 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.76238\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 75.3329 - acc: 0.7625 - val_loss: 75.1752 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.76238\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 75.0488 - acc: 0.7639 - val_loss: 74.8933 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.76238\n",
      "Epoch 41/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 74.7690 - acc: 0.7580 - val_loss: 74.6151 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.76238\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 74.4938 - acc: 0.7654 - val_loss: 74.3437 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.76238\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 74.2289 - acc: 0.7551 - val_loss: 74.0782 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.76238\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 73.9646 - acc: 0.7595 - val_loss: 73.8189 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.76238\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 73.7062 - acc: 0.7610 - val_loss: 73.5581 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.76238\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 73.4523 - acc: 0.7610 - val_loss: 73.3100 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.76238\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 73.1949 - acc: 0.7625 - val_loss: 73.0570 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.76238\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 72.9475 - acc: 0.7639 - val_loss: 72.8120 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.76238\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 72.7092 - acc: 0.7595 - val_loss: 72.5717 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.76238\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 72.4638 - acc: 0.7639 - val_loss: 72.3350 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.76238\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 72.2292 - acc: 0.7625 - val_loss: 72.1020 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.76238\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 72.0035 - acc: 0.7595 - val_loss: 71.8722 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.76238\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 71.7770 - acc: 0.7610 - val_loss: 71.6469 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.76238\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 71.5384 - acc: 0.7669 - val_loss: 71.4234 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.76238\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 71.3270 - acc: 0.7625 - val_loss: 71.2039 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.76238\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 71.1088 - acc: 0.7625 - val_loss: 70.9873 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.76238\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.8921 - acc: 0.7625 - val_loss: 70.7760 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.76238\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.6818 - acc: 0.7580 - val_loss: 70.5635 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.76238\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.4646 - acc: 0.7654 - val_loss: 70.3559 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.76238\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.2615 - acc: 0.7639 - val_loss: 70.1513 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.76238\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.0517 - acc: 0.7669 - val_loss: 69.9490 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.76238\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 69.8633 - acc: 0.7610 - val_loss: 69.7494 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.76238\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 69.6574 - acc: 0.7639 - val_loss: 69.5525 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.76238\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 69.4628 - acc: 0.7639 - val_loss: 69.3581 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.76238\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 69.2757 - acc: 0.7625 - val_loss: 69.1660 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.76238\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 69.0813 - acc: 0.7610 - val_loss: 68.9761 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.76238\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 68.8946 - acc: 0.7639 - val_loss: 68.7890 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.76238\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 68.7100 - acc: 0.7639 - val_loss: 68.6049 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.76238\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 68.5196 - acc: 0.7639 - val_loss: 68.4206 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.76238\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 68.3353 - acc: 0.7639 - val_loss: 68.2396 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.76238\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 68.1597 - acc: 0.7654 - val_loss: 68.0608 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.76238\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 67.9816 - acc: 0.7654 - val_loss: 67.8845 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.76238\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 67.8065 - acc: 0.7639 - val_loss: 67.7096 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.76238\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 67.6253 - acc: 0.7669 - val_loss: 67.5388 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.76238\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 67.4576 - acc: 0.7654 - val_loss: 67.3668 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.76238\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 67.2953 - acc: 0.7625 - val_loss: 67.1968 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.76238\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 67.1277 - acc: 0.7566 - val_loss: 67.0295 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.76238\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 66.9562 - acc: 0.7639 - val_loss: 66.8645 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.76238\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 66.7938 - acc: 0.7610 - val_loss: 66.7002 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.76238\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 66.6319 - acc: 0.7625 - val_loss: 66.5383 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.76238\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 66.4623 - acc: 0.7625 - val_loss: 66.3782 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.76238\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 66.3067 - acc: 0.7639 - val_loss: 66.2191 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.76238\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 66.1536 - acc: 0.7625 - val_loss: 66.0629 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.76238\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 66.0035 - acc: 0.7580 - val_loss: 65.9068 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.76238\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 65.8374 - acc: 0.7639 - val_loss: 65.7525 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.76238\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 65.6889 - acc: 0.7625 - val_loss: 65.6003 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.76238\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 65.5304 - acc: 0.7654 - val_loss: 65.4488 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.76238\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 65.3833 - acc: 0.7625 - val_loss: 65.3003 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.76238\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 65.2286 - acc: 0.7639 - val_loss: 65.1511 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.76238\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 65.0895 - acc: 0.7625 - val_loss: 65.0049 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.76238\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 64.9429 - acc: 0.7595 - val_loss: 64.8592 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.76238\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 64.7910 - acc: 0.7654 - val_loss: 64.7151 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.76238\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 64.6499 - acc: 0.7610 - val_loss: 64.5724 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.76238\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 64.5161 - acc: 0.7625 - val_loss: 64.4315 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.76238\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 64.3775 - acc: 0.7610 - val_loss: 64.2914 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.76238\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 64.2290 - acc: 0.7654 - val_loss: 64.1524 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.76238\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 64.0918 - acc: 0.7610 - val_loss: 64.0149 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.76238\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 63.9593 - acc: 0.7639 - val_loss: 63.8790 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.76238\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 63.8198 - acc: 0.7551 - val_loss: 63.7434 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.76238\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 63.6846 - acc: 0.7639 - val_loss: 63.6097 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.76238\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 63.5468 - acc: 0.7669 - val_loss: 63.4765 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.76238\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 63.4285 - acc: 0.7580 - val_loss: 63.3475 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.76238\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 63.2942 - acc: 0.7610 - val_loss: 63.2145 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.76238\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 63.1639 - acc: 0.7580 - val_loss: 63.0851 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.76238\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 63.0299 - acc: 0.7639 - val_loss: 62.9568 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.76238\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 62.9060 - acc: 0.7595 - val_loss: 62.8302 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.76238\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 62.7790 - acc: 0.7625 - val_loss: 62.7049 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.76238\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 62.6553 - acc: 0.7595 - val_loss: 62.5786 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.76238\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 62.5279 - acc: 0.7625 - val_loss: 62.4543 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.76238\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 62.4004 - acc: 0.7625 - val_loss: 62.3312 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.76238\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 62.2858 - acc: 0.7566 - val_loss: 62.2092 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.76238\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 62.1594 - acc: 0.7610 - val_loss: 62.0879 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.76238\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 62.0398 - acc: 0.7580 - val_loss: 61.9680 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.76238\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 61.9160 - acc: 0.7625 - val_loss: 61.8494 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.76238\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.7977 - acc: 0.7625 - val_loss: 61.7302 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.76238\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.6840 - acc: 0.7610 - val_loss: 61.6127 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.76238\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.5612 - acc: 0.7639 - val_loss: 61.4962 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.76238\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.4515 - acc: 0.7595 - val_loss: 61.3832 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.76238\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.3404 - acc: 0.7580 - val_loss: 61.2665 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.76238\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.2137 - acc: 0.7654 - val_loss: 61.1537 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.76238\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 61.1017 - acc: 0.7654 - val_loss: 61.0399 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.76238\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 60.9891 - acc: 0.7610 - val_loss: 60.9301 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.76238\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.8767 - acc: 0.7639 - val_loss: 60.8165 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.76238\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.7639 - acc: 0.7654 - val_loss: 60.7062 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.76238\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 60.6637 - acc: 0.7580 - val_loss: 60.5966 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.76238\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.5573 - acc: 0.7610 - val_loss: 60.4884 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.76238\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.4405 - acc: 0.7654 - val_loss: 60.3804 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.76238\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 60.3389 - acc: 0.7595 - val_loss: 60.2731 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.76238\n",
      "Epoch 129/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 60.2255 - acc: 0.7639 - val_loss: 60.1664 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.76238\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.1191 - acc: 0.7625 - val_loss: 60.0612 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.76238\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 60.0145 - acc: 0.7639 - val_loss: 59.9572 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.76238\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.9184 - acc: 0.7595 - val_loss: 59.8530 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.76238\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.8140 - acc: 0.7566 - val_loss: 59.7489 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.76238\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.7072 - acc: 0.7625 - val_loss: 59.6466 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.76238\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.6047 - acc: 0.7625 - val_loss: 59.5441 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.76238\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.4982 - acc: 0.7654 - val_loss: 59.4422 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.76238\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 59.3983 - acc: 0.7625 - val_loss: 59.3421 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.76238\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 59.2972 - acc: 0.7654 - val_loss: 59.2418 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.76238\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.2032 - acc: 0.7625 - val_loss: 59.1436 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.76238\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 59.0988 - acc: 0.7639 - val_loss: 59.0446 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.76238\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 59.0090 - acc: 0.7595 - val_loss: 58.9471 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.76238\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 58.9101 - acc: 0.7639 - val_loss: 58.8498 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.76238\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 58.8137 - acc: 0.7625 - val_loss: 58.7533 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.76238\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 58.7121 - acc: 0.7610 - val_loss: 58.6575 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.76238\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 58.6107 - acc: 0.7669 - val_loss: 58.5631 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.76238\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 58.5223 - acc: 0.7639 - val_loss: 58.4689 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.76238\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 58.4340 - acc: 0.7566 - val_loss: 58.3803 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.76238\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 58.3296 - acc: 0.7639 - val_loss: 58.2822 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.76238\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 58.2397 - acc: 0.7610 - val_loss: 58.1887 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.76238\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 58.1569 - acc: 0.7580 - val_loss: 58.1007 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.76238\n",
      "experiment: dropout=0.3 regularization0.1\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 53.6439 - acc: 0.7478 - val_loss: 31.7888 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 26.6940 - acc: 0.7642 - val_loss: 23.5006 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 22.2539 - acc: 0.7585 - val_loss: 21.3018 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.76733, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 20.7645 - acc: 0.7637 - val_loss: 20.2825 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76733 to 0.78218, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 20.0008 - acc: 0.7511 - val_loss: 19.6842 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.78218\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 19.4769 - acc: 0.7666 - val_loss: 19.2587 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.78218\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 19.1078 - acc: 0.7621 - val_loss: 18.9565 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.78218\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 18.8120 - acc: 0.7651 - val_loss: 18.6522 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.78218\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 18.5648 - acc: 0.7598 - val_loss: 18.4237 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.78218\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 18.3325 - acc: 0.7673 - val_loss: 18.2156 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.78218\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 18.1436 - acc: 0.7580 - val_loss: 18.0429 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.78218\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 17.9675 - acc: 0.7691 - val_loss: 17.8621 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.78218\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 17.7977 - acc: 0.7784 - val_loss: 17.7127 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.78218 to 0.78465, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 17.6666 - acc: 0.7730 - val_loss: 17.5648 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.78465 to 0.79208, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 17.5119 - acc: 0.7793 - val_loss: 17.4306 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79208\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 17.3987 - acc: 0.7922 - val_loss: 17.3084 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79208\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 17.2699 - acc: 0.7862 - val_loss: 17.2482 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.79208\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 17.1600 - acc: 0.7949 - val_loss: 17.0833 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.79208\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 17.0661 - acc: 0.7851 - val_loss: 17.0531 - val_acc: 0.7426\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.79208\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 16.9688 - acc: 0.7796 - val_loss: 16.8943 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.79208 to 0.80198, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 16.8589 - acc: 0.7898 - val_loss: 16.7846 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.80198 to 0.80941, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 16.7529 - acc: 0.8003 - val_loss: 16.6871 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.80941\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.6713 - acc: 0.7947 - val_loss: 16.6086 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.80941 to 0.81188, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.5872 - acc: 0.7989 - val_loss: 16.5515 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.81188\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.5063 - acc: 0.7922 - val_loss: 16.4515 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.81188\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.4023 - acc: 0.8136 - val_loss: 16.3552 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.81188\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.3270 - acc: 0.8043 - val_loss: 16.2726 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.81188\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 16.2538 - acc: 0.8067 - val_loss: 16.2125 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.81188\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.1686 - acc: 0.8088 - val_loss: 16.1204 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.81188 to 0.81436, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 16.0986 - acc: 0.8051 - val_loss: 16.0647 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81436\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 16.0193 - acc: 0.8178 - val_loss: 15.9781 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.81436 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.9544 - acc: 0.8192 - val_loss: 15.9074 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.81683 to 0.82178, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.8839 - acc: 0.8205 - val_loss: 15.8590 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.82178\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.8145 - acc: 0.8148 - val_loss: 15.7998 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.82178 to 0.82426, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.7771 - acc: 0.8022 - val_loss: 15.7172 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.82426\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.6888 - acc: 0.8199 - val_loss: 15.6516 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.82426\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.6273 - acc: 0.8232 - val_loss: 15.6099 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.82426\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 15.5598 - acc: 0.8277 - val_loss: 15.5600 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.82426\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.5379 - acc: 0.8109 - val_loss: 15.5339 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.82426\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.4536 - acc: 0.8232 - val_loss: 15.4291 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.82426\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 15.4266 - acc: 0.7995 - val_loss: 15.3911 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.82426\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.3628 - acc: 0.8106 - val_loss: 15.3193 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.82426\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 15.2899 - acc: 0.8163 - val_loss: 15.2682 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.82426\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 15.2334 - acc: 0.8227 - val_loss: 15.2013 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.82426 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.1763 - acc: 0.8298 - val_loss: 15.1536 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.83663\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 15.1216 - acc: 0.8325 - val_loss: 15.1359 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.83663\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 15.0835 - acc: 0.8364 - val_loss: 15.1036 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.83663\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 15.0272 - acc: 0.8259 - val_loss: 15.0003 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.83663\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.9804 - acc: 0.8250 - val_loss: 14.9826 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.83663\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.9357 - acc: 0.8367 - val_loss: 14.9165 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.83663\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.8853 - acc: 0.8217 - val_loss: 14.9135 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.83663\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.8329 - acc: 0.8364 - val_loss: 14.8297 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.83663\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.7847 - acc: 0.8289 - val_loss: 14.7731 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.83663 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 14.7397 - acc: 0.8298 - val_loss: 14.7345 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.83663\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.6964 - acc: 0.8361 - val_loss: 14.6762 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00055: val_acc improved from 0.83663 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.6489 - acc: 0.8290 - val_loss: 14.6344 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.84158\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.6274 - acc: 0.8236 - val_loss: 14.5909 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.84158\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.5518 - acc: 0.8346 - val_loss: 14.5473 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.84158\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.5172 - acc: 0.8361 - val_loss: 14.5373 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84158\n",
      "Epoch 60/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 34s 1s/step - loss: 14.4893 - acc: 0.8325 - val_loss: 14.4774 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84158\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.4382 - acc: 0.8476 - val_loss: 14.4270 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84158\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.4122 - acc: 0.8286 - val_loss: 14.4350 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84158\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.3776 - acc: 0.8349 - val_loss: 14.3797 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84158\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.3154 - acc: 0.8520 - val_loss: 14.3227 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.84158\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.2642 - acc: 0.8539 - val_loss: 14.2971 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84158\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.2745 - acc: 0.8265 - val_loss: 14.2572 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.84158\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.2341 - acc: 0.8332 - val_loss: 14.2420 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.84158\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.1674 - acc: 0.8427 - val_loss: 14.1569 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.84158\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 14.1307 - acc: 0.8428 - val_loss: 14.1411 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.84158\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.1225 - acc: 0.8262 - val_loss: 14.1526 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.84158\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.0727 - acc: 0.8410 - val_loss: 14.0518 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.84158\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 14.0236 - acc: 0.8499 - val_loss: 14.0346 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.84158\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.9857 - acc: 0.8509 - val_loss: 13.9786 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.84158\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.9587 - acc: 0.8421 - val_loss: 14.0021 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.84158\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.9144 - acc: 0.8452 - val_loss: 13.9615 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.84158\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.8759 - acc: 0.8487 - val_loss: 13.8739 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.84158\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.8380 - acc: 0.8541 - val_loss: 13.8703 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.84158\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.7955 - acc: 0.8554 - val_loss: 13.8120 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.84158\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.7665 - acc: 0.8578 - val_loss: 13.7799 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.84158\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.7288 - acc: 0.8596 - val_loss: 13.7529 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.84158\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.7117 - acc: 0.8422 - val_loss: 13.7639 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.84158\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.6779 - acc: 0.8343 - val_loss: 13.6849 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.84158 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.6456 - acc: 0.8455 - val_loss: 13.6436 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.84901 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.5971 - acc: 0.8668 - val_loss: 13.6804 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.85149\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.5645 - acc: 0.8580 - val_loss: 13.5873 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.85149\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.5337 - acc: 0.8614 - val_loss: 13.5612 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.85149\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.5056 - acc: 0.8539 - val_loss: 13.5240 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.85149\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.4721 - acc: 0.8755 - val_loss: 13.4948 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.85149\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.4446 - acc: 0.8641 - val_loss: 13.4658 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.85149\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.4062 - acc: 0.8650 - val_loss: 13.4319 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.85149\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.3889 - acc: 0.8563 - val_loss: 13.4530 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.85149\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.3788 - acc: 0.8497 - val_loss: 13.3811 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.85149\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 13.3301 - acc: 0.8665 - val_loss: 13.3537 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.85149\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.3023 - acc: 0.8539 - val_loss: 13.3227 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.85149\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.2820 - acc: 0.8482 - val_loss: 13.2979 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.85149\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.2629 - acc: 0.8564 - val_loss: 13.2971 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.85149\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.2443 - acc: 0.8412 - val_loss: 13.2531 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.85149\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.1772 - acc: 0.8710 - val_loss: 13.2250 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00098: val_acc improved from 0.85149 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.1507 - acc: 0.8692 - val_loss: 13.2287 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.85149\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.1712 - acc: 0.8367 - val_loss: 13.1877 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.85149\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.1021 - acc: 0.8701 - val_loss: 13.1321 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00101: val_acc improved from 0.85149 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.0683 - acc: 0.8698 - val_loss: 13.0980 - val_acc: 0.8515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00102: val_acc did not improve from 0.85396\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 13.0273 - acc: 0.8800 - val_loss: 13.0875 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.85396\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 13.0068 - acc: 0.8773 - val_loss: 13.0581 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.85396\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.9722 - acc: 0.8779 - val_loss: 13.0232 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.85396\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.9587 - acc: 0.8671 - val_loss: 13.0379 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.85396\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.9355 - acc: 0.8746 - val_loss: 12.9946 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.85396\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.8961 - acc: 0.8870 - val_loss: 12.9749 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.85396\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.8852 - acc: 0.8725 - val_loss: 12.9195 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.85396\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.8325 - acc: 0.8863 - val_loss: 12.9552 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.85396\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.8254 - acc: 0.8855 - val_loss: 12.8920 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.85396\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.8158 - acc: 0.8764 - val_loss: 12.8531 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.85396\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.7741 - acc: 0.8806 - val_loss: 12.8257 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00113: val_acc improved from 0.85396 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.7567 - acc: 0.8753 - val_loss: 12.8156 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.85396\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.7508 - acc: 0.8689 - val_loss: 12.7895 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.85396\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.7045 - acc: 0.8848 - val_loss: 12.8166 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.85396\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.6844 - acc: 0.8833 - val_loss: 12.7688 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.85396\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.6841 - acc: 0.8636 - val_loss: 12.7541 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.85396\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.6516 - acc: 0.8719 - val_loss: 12.6969 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.85396\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.6147 - acc: 0.8791 - val_loss: 12.6815 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.85396\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.5863 - acc: 0.8870 - val_loss: 12.6731 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.85396\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.5478 - acc: 0.9017 - val_loss: 12.6332 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00122: val_acc improved from 0.85396 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.5304 - acc: 0.8848 - val_loss: 12.6625 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.85396\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.5376 - acc: 0.8674 - val_loss: 12.6037 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.85396\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.4743 - acc: 0.9089 - val_loss: 12.5621 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.85396\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.4755 - acc: 0.8876 - val_loss: 12.5435 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.85396\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.4340 - acc: 0.8920 - val_loss: 12.5258 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.85396\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.4158 - acc: 0.8993 - val_loss: 12.5247 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.85396\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.3834 - acc: 0.9062 - val_loss: 12.4952 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00129: val_acc improved from 0.85396 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.3683 - acc: 0.8993 - val_loss: 12.4812 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.85644\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.3624 - acc: 0.8903 - val_loss: 12.4453 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.85644\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.3169 - acc: 0.8983 - val_loss: 12.4072 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.85644\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.2966 - acc: 0.9068 - val_loss: 12.4266 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.85644\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.2821 - acc: 0.8897 - val_loss: 12.3814 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.85644\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.2650 - acc: 0.8972 - val_loss: 12.3385 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00135: val_acc improved from 0.85644 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.1.h5\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.2289 - acc: 0.9113 - val_loss: 12.3340 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.85891\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.2164 - acc: 0.8995 - val_loss: 12.3155 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.85891\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.2033 - acc: 0.8948 - val_loss: 12.3303 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.85891\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.1764 - acc: 0.9049 - val_loss: 12.2753 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.85891\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.1596 - acc: 0.8996 - val_loss: 12.2436 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.85891\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.1245 - acc: 0.9161 - val_loss: 12.2361 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.85891\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.1435 - acc: 0.8860 - val_loss: 12.3064 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.85891\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.1251 - acc: 0.8918 - val_loss: 12.2008 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.85891\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.0747 - acc: 0.9083 - val_loss: 12.1664 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.85891\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.0436 - acc: 0.9119 - val_loss: 12.1629 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.85891\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.0399 - acc: 0.9056 - val_loss: 12.1399 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.85891\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 12.0126 - acc: 0.9113 - val_loss: 12.1458 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.85891\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 11.9925 - acc: 0.9137 - val_loss: 12.1133 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.85891\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 12.0205 - acc: 0.8869 - val_loss: 12.1742 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.85891\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 11.9531 - acc: 0.9077 - val_loss: 12.0695 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.85891\n",
      "experiment: dropout=0.4 regularization0.001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 1.3646 - acc: 0.7555 - val_loss: 1.1497 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.0958 - acc: 0.7543 - val_loss: 1.0319 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.9789 - acc: 0.7639 - val_loss: 0.9171 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.78713, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.9158 - acc: 0.7610 - val_loss: 0.8850 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.78713\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.8668 - acc: 0.7655 - val_loss: 0.8286 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78713 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.8119 - acc: 0.7937 - val_loss: 0.7773 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.79950 to 0.80446, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7884 - acc: 0.7967 - val_loss: 0.7591 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.80446 to 0.82178, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7709 - acc: 0.8043 - val_loss: 0.7542 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.82178\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7265 - acc: 0.8313 - val_loss: 0.7106 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.82178\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.7232 - acc: 0.8179 - val_loss: 0.7343 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82178\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7037 - acc: 0.8347 - val_loss: 0.6951 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.82178\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6943 - acc: 0.8190 - val_loss: 0.7366 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82178\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6890 - acc: 0.8290 - val_loss: 0.6657 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.82178 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6395 - acc: 0.8592 - val_loss: 0.6401 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.83663 to 0.84653, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.6351 - acc: 0.8533 - val_loss: 0.6601 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.84653 to 0.84653, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.6244 - acc: 0.8503 - val_loss: 0.6531 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.84653\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.6096 - acc: 0.8590 - val_loss: 0.6643 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84653\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6044 - acc: 0.8602 - val_loss: 0.6456 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.84653\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5639 - acc: 0.8752 - val_loss: 0.6088 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.84653 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5568 - acc: 0.8849 - val_loss: 0.6219 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.84901 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5246 - acc: 0.8981 - val_loss: 0.6352 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.85644 to 0.86881, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4993 - acc: 0.9059 - val_loss: 0.7364 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86881\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5157 - acc: 0.8911 - val_loss: 0.6246 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86881\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5058 - acc: 0.8975 - val_loss: 0.6393 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.86881 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4914 - acc: 0.8993 - val_loss: 0.5868 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87129\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4804 - acc: 0.9134 - val_loss: 0.6399 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87129\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4701 - acc: 0.9105 - val_loss: 0.6128 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87129\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4706 - acc: 0.9098 - val_loss: 0.6342 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.87129 to 0.87871, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4294 - acc: 0.9362 - val_loss: 0.6374 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.87871 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3940 - acc: 0.9410 - val_loss: 0.6331 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88119\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4248 - acc: 0.9230 - val_loss: 0.6489 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88119\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4226 - acc: 0.9302 - val_loss: 0.6117 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.88119 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3752 - acc: 0.9447 - val_loss: 0.7445 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.89356\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4000 - acc: 0.9384 - val_loss: 0.6033 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.89356\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3744 - acc: 0.9495 - val_loss: 0.6888 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.89356\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3711 - acc: 0.9450 - val_loss: 0.6903 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.89356\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3651 - acc: 0.9525 - val_loss: 0.6328 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.89356\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3401 - acc: 0.9627 - val_loss: 0.7374 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.89356\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3468 - acc: 0.9582 - val_loss: 0.7349 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.89356\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3453 - acc: 0.9546 - val_loss: 0.8644 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89356\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3607 - acc: 0.9534 - val_loss: 0.7716 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89356\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3488 - acc: 0.9510 - val_loss: 0.8182 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89356\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3313 - acc: 0.9657 - val_loss: 0.7469 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89356\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3462 - acc: 0.9585 - val_loss: 0.6894 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89356\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3253 - acc: 0.9645 - val_loss: 0.7035 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89356\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2996 - acc: 0.9706 - val_loss: 0.7929 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89356\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3743 - acc: 0.9480 - val_loss: 0.7350 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89356\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3176 - acc: 0.9645 - val_loss: 0.6526 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89356\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3083 - acc: 0.9705 - val_loss: 0.7966 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89356\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3172 - acc: 0.9651 - val_loss: 0.7997 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89356\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2926 - acc: 0.9775 - val_loss: 0.8272 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89356\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3337 - acc: 0.9640 - val_loss: 0.6757 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89356\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2924 - acc: 0.9783 - val_loss: 0.7189 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89356\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2698 - acc: 0.9862 - val_loss: 0.8625 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89356\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2809 - acc: 0.9805 - val_loss: 0.7424 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89356\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2722 - acc: 0.9847 - val_loss: 0.9182 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89356\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3390 - acc: 0.9588 - val_loss: 0.6700 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89356\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3125 - acc: 0.9678 - val_loss: 0.8263 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.89356\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2980 - acc: 0.9711 - val_loss: 0.7767 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89356\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3005 - acc: 0.9681 - val_loss: 1.0129 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.89356\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3288 - acc: 0.9606 - val_loss: 0.6911 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89356\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2841 - acc: 0.9771 - val_loss: 0.7127 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89356\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2621 - acc: 0.9850 - val_loss: 0.7410 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.89356 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2514 - acc: 0.9874 - val_loss: 0.7904 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.89851 to 0.90099, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2496 - acc: 0.9892 - val_loss: 0.8181 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00065: val_acc improved from 0.90099 to 0.90347, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2470 - acc: 0.9892 - val_loss: 0.8320 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90347\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2696 - acc: 0.9832 - val_loss: 0.8183 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90347\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2536 - acc: 0.9880 - val_loss: 0.8945 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.90347\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2433 - acc: 0.9934 - val_loss: 0.7970 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90347\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2368 - acc: 0.9928 - val_loss: 0.8684 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90347\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2444 - acc: 0.9892 - val_loss: 1.0198 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.90347\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2496 - acc: 0.9916 - val_loss: 0.9139 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.90347\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2470 - acc: 0.9850 - val_loss: 0.8819 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90347\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2473 - acc: 0.9877 - val_loss: 0.7722 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.90347\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2340 - acc: 0.9910 - val_loss: 0.8748 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.90347\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2287 - acc: 0.9952 - val_loss: 0.8325 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.90347\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2326 - acc: 0.9934 - val_loss: 0.8639 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.90347\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2331 - acc: 0.9928 - val_loss: 0.8408 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.90347\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2263 - acc: 0.9940 - val_loss: 0.9475 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.90347\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2490 - acc: 0.9844 - val_loss: 1.1210 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90347\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2388 - acc: 0.9886 - val_loss: 0.8293 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90347\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2400 - acc: 0.9910 - val_loss: 0.9018 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90347\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2374 - acc: 0.9910 - val_loss: 0.9330 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90347\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2474 - acc: 0.9862 - val_loss: 0.8002 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90347\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2334 - acc: 0.9928 - val_loss: 0.8087 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00085: val_acc improved from 0.90347 to 0.90594, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2274 - acc: 0.9934 - val_loss: 0.8155 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90594\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2229 - acc: 0.9952 - val_loss: 0.8030 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00087: val_acc improved from 0.90594 to 0.91089, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2200 - acc: 0.9946 - val_loss: 0.8199 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.91089\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2253 - acc: 0.9940 - val_loss: 0.8722 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.91089\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2169 - acc: 0.9958 - val_loss: 0.9171 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.91089\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2191 - acc: 0.9940 - val_loss: 0.8745 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91089\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2203 - acc: 0.9928 - val_loss: 0.9048 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91089\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2157 - acc: 0.9952 - val_loss: 0.8872 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.91089\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2129 - acc: 0.9970 - val_loss: 0.9346 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.91089\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2086 - acc: 0.9976 - val_loss: 0.9302 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.91089\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2239 - acc: 0.9928 - val_loss: 0.8939 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91089\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2275 - acc: 0.9892 - val_loss: 0.8522 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.91089\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2305 - acc: 0.9904 - val_loss: 1.1070 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.91089\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2457 - acc: 0.9823 - val_loss: 1.1963 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91089\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2613 - acc: 0.9823 - val_loss: 0.8338 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.91089\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2461 - acc: 0.9856 - val_loss: 0.7742 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.91089\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2433 - acc: 0.9844 - val_loss: 0.8247 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.91089\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2316 - acc: 0.9880 - val_loss: 0.8153 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.91089\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2154 - acc: 0.9934 - val_loss: 0.8140 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.91089\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2151 - acc: 0.9934 - val_loss: 0.8489 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.91089\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2140 - acc: 0.9940 - val_loss: 0.9202 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.91089\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2170 - acc: 0.9895 - val_loss: 0.9500 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.91089\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2116 - acc: 0.9958 - val_loss: 1.0533 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.91089\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2772 - acc: 0.9787 - val_loss: 0.8644 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.91089\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2276 - acc: 0.9898 - val_loss: 0.8465 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.91089\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2076 - acc: 0.9976 - val_loss: 0.8528 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.91089\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2129 - acc: 0.9946 - val_loss: 0.9585 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.91089\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2122 - acc: 0.9940 - val_loss: 0.8778 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.91089\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2050 - acc: 0.9976 - val_loss: 0.9689 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.91089\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2026 - acc: 0.9970 - val_loss: 1.0504 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.91089\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1988 - acc: 0.9994 - val_loss: 0.9704 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.91089\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2013 - acc: 0.9970 - val_loss: 0.9942 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.91089\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2298 - acc: 0.9910 - val_loss: 0.9861 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.91089\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2200 - acc: 0.9922 - val_loss: 0.9491 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.91089\n",
      "Epoch 120/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 34s 1s/step - loss: 0.2164 - acc: 0.9952 - val_loss: 0.7476 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.91089\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2059 - acc: 0.9958 - val_loss: 0.7604 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.91089\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2008 - acc: 0.9976 - val_loss: 0.8053 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.91089\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2005 - acc: 0.9958 - val_loss: 0.8855 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.91089\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2058 - acc: 0.9940 - val_loss: 0.9081 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.91089\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2101 - acc: 0.9946 - val_loss: 0.8998 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.91089\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2020 - acc: 0.9964 - val_loss: 0.8084 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.91089\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1950 - acc: 0.9988 - val_loss: 0.8287 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.91089\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1936 - acc: 0.9988 - val_loss: 0.8437 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.91089\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1954 - acc: 0.9976 - val_loss: 0.7996 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.91089\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2055 - acc: 0.9931 - val_loss: 0.8213 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00130: val_acc improved from 0.91089 to 0.91337, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001.h5\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2014 - acc: 0.9964 - val_loss: 0.9500 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.91337\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2122 - acc: 0.9916 - val_loss: 1.1077 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.91337\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2312 - acc: 0.9886 - val_loss: 0.8660 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.91337\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2088 - acc: 0.9934 - val_loss: 0.8581 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.91337\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2270 - acc: 0.9856 - val_loss: 0.8247 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.91337\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2082 - acc: 0.9946 - val_loss: 0.8449 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.91337\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1976 - acc: 0.9970 - val_loss: 0.8770 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.91337\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1921 - acc: 0.9988 - val_loss: 0.9440 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.91337\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1902 - acc: 0.9994 - val_loss: 0.9415 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.91337\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1909 - acc: 0.9982 - val_loss: 0.9434 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.91337\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1879 - acc: 0.9994 - val_loss: 0.9981 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.91337\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1870 - acc: 0.9994 - val_loss: 1.0086 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.91337\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1870 - acc: 0.9994 - val_loss: 1.0091 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.91337\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1867 - acc: 1.0000 - val_loss: 0.9300 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.91337\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1881 - acc: 0.9994 - val_loss: 0.9506 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.91337\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1894 - acc: 0.9988 - val_loss: 0.9611 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.91337\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1863 - acc: 0.9994 - val_loss: 0.9954 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.91337\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1853 - acc: 0.9994 - val_loss: 1.0089 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.91337\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1882 - acc: 0.9976 - val_loss: 0.9940 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.91337\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2014 - acc: 0.9922 - val_loss: 1.0517 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.91337\n",
      "experiment: dropout=0.4 regularization0.05\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 28.6947 - acc: 0.7318 - val_loss: 17.8108 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 14.8516 - acc: 0.7565 - val_loss: 12.8853 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 12.1158 - acc: 0.7586 - val_loss: 11.5098 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 11.1367 - acc: 0.7579 - val_loss: 10.8229 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76238 to 0.77475, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 10.6265 - acc: 0.7613 - val_loss: 10.4356 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77475 to 0.78713, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 10.2785 - acc: 0.7678 - val_loss: 10.1989 - val_acc: 0.6906\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.78713\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 10.0653 - acc: 0.7643 - val_loss: 9.9431 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.78713\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 9.8590 - acc: 0.7670 - val_loss: 9.7697 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.78713\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 9.6931 - acc: 0.7718 - val_loss: 9.6194 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.78713\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 9.5836 - acc: 0.7746 - val_loss: 9.5033 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.78713\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 9.4601 - acc: 0.7819 - val_loss: 9.3932 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.78713 to 0.79455, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 9.3842 - acc: 0.7853 - val_loss: 9.3197 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79455\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 9.2812 - acc: 0.7787 - val_loss: 9.2173 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79455\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 9.1983 - acc: 0.7916 - val_loss: 9.1365 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79455\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 9.1239 - acc: 0.7913 - val_loss: 9.0893 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79455\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 9.0503 - acc: 0.7905 - val_loss: 9.0254 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79455\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.9851 - acc: 0.8084 - val_loss: 8.9428 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.79455 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.9284 - acc: 0.7988 - val_loss: 8.8826 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.81683\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.8682 - acc: 0.7931 - val_loss: 8.8169 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.81683 to 0.82426, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.8194 - acc: 0.7907 - val_loss: 8.8040 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.82426\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 8.7486 - acc: 0.8040 - val_loss: 8.7206 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.82426\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 8.7207 - acc: 0.8067 - val_loss: 8.6664 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.82426\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.6585 - acc: 0.8175 - val_loss: 8.6349 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.82426\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.6262 - acc: 0.7983 - val_loss: 8.5825 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.82426 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 8.5662 - acc: 0.8229 - val_loss: 8.6243 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83663\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.5436 - acc: 0.7977 - val_loss: 8.5176 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83663\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 8.4946 - acc: 0.8103 - val_loss: 8.4498 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83663\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.4390 - acc: 0.8241 - val_loss: 8.4155 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83663\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 8.4173 - acc: 0.8063 - val_loss: 8.4101 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83663\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.3845 - acc: 0.8022 - val_loss: 8.3439 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83663\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 8.3469 - acc: 0.8061 - val_loss: 8.3105 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83663\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 8.3017 - acc: 0.8269 - val_loss: 8.2764 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83663\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.2637 - acc: 0.8196 - val_loss: 8.2343 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83663\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.2210 - acc: 0.8235 - val_loss: 8.2349 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83663\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.1857 - acc: 0.8328 - val_loss: 8.1768 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83663\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 8.1754 - acc: 0.8286 - val_loss: 8.1438 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.83663\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 8.1289 - acc: 0.8313 - val_loss: 8.1156 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.83663\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 8.1219 - acc: 0.8106 - val_loss: 8.0891 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.83663\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.0894 - acc: 0.8214 - val_loss: 8.0733 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.83663\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.0359 - acc: 0.8329 - val_loss: 8.0545 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.83663\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 8.0120 - acc: 0.8197 - val_loss: 8.0072 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.83663\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.9713 - acc: 0.8325 - val_loss: 8.0935 - val_acc: 0.7475\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.83663\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.9883 - acc: 0.8118 - val_loss: 7.9854 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.83663\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.9314 - acc: 0.8319 - val_loss: 7.9504 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.83663\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.9152 - acc: 0.8341 - val_loss: 7.9213 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.83663 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.8698 - acc: 0.8347 - val_loss: 7.8806 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.83911\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.8460 - acc: 0.8358 - val_loss: 7.8416 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.83911\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 7.8024 - acc: 0.8583 - val_loss: 7.8071 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.83911 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.8113 - acc: 0.8416 - val_loss: 7.8514 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.84158\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.7802 - acc: 0.8410 - val_loss: 7.7706 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.84158\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.7490 - acc: 0.8317 - val_loss: 7.7388 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.84158\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.7036 - acc: 0.8551 - val_loss: 7.7049 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.84158\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.6962 - acc: 0.8434 - val_loss: 7.6959 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.84158 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.6767 - acc: 0.8440 - val_loss: 7.6949 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.84158\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.6337 - acc: 0.8620 - val_loss: 7.6952 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.84158\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.6247 - acc: 0.8452 - val_loss: 7.6304 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.84158 to 0.84653, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.6473 - acc: 0.8268 - val_loss: 7.6610 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.84653\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.6264 - acc: 0.8245 - val_loss: 7.6089 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.84653\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.5674 - acc: 0.8455 - val_loss: 7.6033 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.84653\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.5648 - acc: 0.8394 - val_loss: 7.5469 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.84653\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 7.5159 - acc: 0.8482 - val_loss: 7.5251 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.84653\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.4763 - acc: 0.8575 - val_loss: 7.5271 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.84653\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 7.4603 - acc: 0.8647 - val_loss: 7.4811 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.84653\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.4364 - acc: 0.8611 - val_loss: 7.4644 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.84653 to 0.84653, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.4220 - acc: 0.8593 - val_loss: 7.4597 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.84653\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.3996 - acc: 0.8634 - val_loss: 7.4265 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.84653\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.3878 - acc: 0.8578 - val_loss: 7.4216 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.84653\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.3671 - acc: 0.8542 - val_loss: 7.3889 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.84653\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.3367 - acc: 0.8644 - val_loss: 7.3756 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.84653\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.3426 - acc: 0.8389 - val_loss: 7.3563 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.84653\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.3242 - acc: 0.8506 - val_loss: 7.3320 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.84653\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.2798 - acc: 0.8677 - val_loss: 7.3271 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.84653\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.2602 - acc: 0.8677 - val_loss: 7.3043 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.84653\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.2289 - acc: 0.8698 - val_loss: 7.3044 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00074: val_acc improved from 0.84653 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.2036 - acc: 0.8833 - val_loss: 7.2787 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.85396\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.1952 - acc: 0.8899 - val_loss: 7.2466 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.85396\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.1577 - acc: 0.8866 - val_loss: 7.2323 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00077: val_acc improved from 0.85396 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.1457 - acc: 0.8819 - val_loss: 7.2567 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.85644\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.1701 - acc: 0.8677 - val_loss: 7.2376 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.85644\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.1198 - acc: 0.8743 - val_loss: 7.2036 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.85644 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.0856 - acc: 0.8941 - val_loss: 7.1742 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.85644\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.0674 - acc: 0.8914 - val_loss: 7.1815 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.85644\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.0756 - acc: 0.8869 - val_loss: 7.1446 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.85644\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 7.0489 - acc: 0.8837 - val_loss: 7.1386 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.85644\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 7.1002 - acc: 0.8404 - val_loss: 7.1338 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.85644\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.0094 - acc: 0.8972 - val_loss: 7.1496 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.85644\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 7.0021 - acc: 0.8860 - val_loss: 7.0936 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.85644\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.9760 - acc: 0.8848 - val_loss: 7.0707 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.85644\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.9438 - acc: 0.9044 - val_loss: 7.0405 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.85644\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.9386 - acc: 0.8894 - val_loss: 7.0555 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.85644\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.9349 - acc: 0.8918 - val_loss: 7.0597 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.85644\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.9333 - acc: 0.8876 - val_loss: 7.0358 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.85644\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.8782 - acc: 0.8995 - val_loss: 7.0219 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00093: val_acc improved from 0.85644 to 0.86386, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.8707 - acc: 0.9044 - val_loss: 6.9796 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.86386\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.8554 - acc: 0.9011 - val_loss: 7.0103 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.86386\n",
      "Epoch 96/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 37s 1s/step - loss: 6.8597 - acc: 0.8884 - val_loss: 7.0385 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.86386\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.8382 - acc: 0.9035 - val_loss: 6.9461 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.86386\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.8114 - acc: 0.9092 - val_loss: 6.9159 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.86386\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.7837 - acc: 0.9170 - val_loss: 6.9068 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.86386\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.7962 - acc: 0.9029 - val_loss: 6.9135 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.86386\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.7772 - acc: 0.9043 - val_loss: 6.9030 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.86386\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.7601 - acc: 0.9113 - val_loss: 6.8781 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.86386\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.7113 - acc: 0.9204 - val_loss: 6.8773 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.86386\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.7074 - acc: 0.9164 - val_loss: 6.9230 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.86386\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.7199 - acc: 0.9053 - val_loss: 6.8372 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.86386\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.6776 - acc: 0.9236 - val_loss: 6.8445 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.86386\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.6606 - acc: 0.9260 - val_loss: 6.8727 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00107: val_acc improved from 0.86386 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.6698 - acc: 0.9074 - val_loss: 6.8612 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.87129\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.6600 - acc: 0.9059 - val_loss: 6.8399 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.87129\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.6536 - acc: 0.8969 - val_loss: 6.7655 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.87129\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.6322 - acc: 0.9140 - val_loss: 6.7930 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.87129\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.6040 - acc: 0.9257 - val_loss: 6.7600 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00112: val_acc improved from 0.87129 to 0.87376, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.5944 - acc: 0.9213 - val_loss: 6.7532 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.87376\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.5623 - acc: 0.9269 - val_loss: 6.7765 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.87376\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.5441 - acc: 0.9348 - val_loss: 6.7364 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.87376\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.5609 - acc: 0.9164 - val_loss: 6.7337 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.87376\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.5400 - acc: 0.9254 - val_loss: 6.6999 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.87376\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.5199 - acc: 0.9284 - val_loss: 6.7118 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.87376\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.4972 - acc: 0.9348 - val_loss: 6.7083 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.87376\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.4755 - acc: 0.9386 - val_loss: 6.7382 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.87376\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.4828 - acc: 0.9317 - val_loss: 6.7191 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.87376\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 39s 2s/step - loss: 6.4591 - acc: 0.9369 - val_loss: 6.6546 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.87376\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.4393 - acc: 0.9420 - val_loss: 6.6894 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.87376\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.3997 - acc: 0.9564 - val_loss: 6.6450 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.87376\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.4103 - acc: 0.9468 - val_loss: 6.6695 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.87376\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.4040 - acc: 0.9368 - val_loss: 6.7261 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.87376\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.3725 - acc: 0.9535 - val_loss: 6.6838 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.87376\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.3768 - acc: 0.9384 - val_loss: 6.6836 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.87376\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.3780 - acc: 0.9366 - val_loss: 6.5905 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.87376\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.3518 - acc: 0.9456 - val_loss: 6.6070 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.87376\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.3303 - acc: 0.9543 - val_loss: 6.6133 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.87376\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.3001 - acc: 0.9582 - val_loss: 6.5751 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.87376\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.4070 - acc: 0.9087 - val_loss: 6.5651 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.87376\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.3309 - acc: 0.9416 - val_loss: 6.5681 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.87376\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.2920 - acc: 0.9480 - val_loss: 6.5575 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.87376\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 6.2813 - acc: 0.9543 - val_loss: 6.5236 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00136: val_acc improved from 0.87376 to 0.87871, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.05.h5\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.2861 - acc: 0.9402 - val_loss: 6.5613 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.87871\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.2548 - acc: 0.9573 - val_loss: 6.6081 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.87871\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.2581 - acc: 0.9507 - val_loss: 6.7312 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.87871\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.2551 - acc: 0.9465 - val_loss: 6.5212 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.87871\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.2337 - acc: 0.9489 - val_loss: 6.4640 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.87871\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.1931 - acc: 0.9600 - val_loss: 6.5316 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.87871\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.1994 - acc: 0.9561 - val_loss: 6.5279 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.87871\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 6.1727 - acc: 0.9705 - val_loss: 6.4705 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.87871\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.1901 - acc: 0.9534 - val_loss: 6.5637 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.87871\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.1672 - acc: 0.9594 - val_loss: 6.4295 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.87871\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.1475 - acc: 0.9645 - val_loss: 6.5137 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.87871\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 6.1380 - acc: 0.9648 - val_loss: 6.4661 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.87871\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 6.1352 - acc: 0.9534 - val_loss: 6.4708 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.87871\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 6.1293 - acc: 0.9597 - val_loss: 6.4270 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.87871\n",
      "experiment: dropout=0.4 regularization0.01\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 40s 2s/step - loss: 6.9704 - acc: 0.7330 - val_loss: 4.9186 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 4.2013 - acc: 0.7526 - val_loss: 3.6562 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 3.4119 - acc: 0.7595 - val_loss: 3.2324 - val_acc: 0.7153\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 3.0699 - acc: 0.7613 - val_loss: 3.0010 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 2.8965 - acc: 0.7619 - val_loss: 2.8528 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.76238 to 0.76485, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.7760 - acc: 0.7630 - val_loss: 2.6899 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76485\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.6293 - acc: 0.7895 - val_loss: 2.5823 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.76485 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.5912 - acc: 0.7724 - val_loss: 2.5346 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79950\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 2.5380 - acc: 0.7754 - val_loss: 2.4986 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79950\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 2.4715 - acc: 0.7919 - val_loss: 2.4736 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79950\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.4271 - acc: 0.8051 - val_loss: 2.4027 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.79950 to 0.80198, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.3777 - acc: 0.8040 - val_loss: 2.3705 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.80198\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.3519 - acc: 0.8081 - val_loss: 2.3176 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80198\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.2981 - acc: 0.8325 - val_loss: 2.2943 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.80198 to 0.81188, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.2942 - acc: 0.8160 - val_loss: 2.2922 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.81188\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.2648 - acc: 0.8253 - val_loss: 2.2764 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.81188\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.2429 - acc: 0.8223 - val_loss: 2.2209 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.81188 to 0.82921, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.2582 - acc: 0.7880 - val_loss: 2.2330 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.82921\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.2195 - acc: 0.8220 - val_loss: 2.1922 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.82921\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.1769 - acc: 0.8314 - val_loss: 2.2104 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.82921\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.1910 - acc: 0.8139 - val_loss: 2.1813 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.82921\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.1560 - acc: 0.8289 - val_loss: 2.1736 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.82921\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 2.1397 - acc: 0.8262 - val_loss: 2.1348 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.82921 to 0.83416, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.1198 - acc: 0.8358 - val_loss: 2.1525 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83416\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 2.1103 - acc: 0.8365 - val_loss: 2.1416 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83416\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.0939 - acc: 0.8380 - val_loss: 2.1080 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.83416 to 0.83416, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 2.0694 - acc: 0.8364 - val_loss: 2.0886 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.83416 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 28/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 35s 1s/step - loss: 2.0576 - acc: 0.8499 - val_loss: 2.0773 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.83911 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.0503 - acc: 0.8566 - val_loss: 2.0732 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.84406\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.0220 - acc: 0.8554 - val_loss: 2.1196 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.84406\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 2.0482 - acc: 0.8430 - val_loss: 2.0647 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.84406\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 2.0019 - acc: 0.8556 - val_loss: 2.0581 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84406\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 2.0130 - acc: 0.8497 - val_loss: 2.0231 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.84406 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.9933 - acc: 0.8638 - val_loss: 2.0238 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.85149\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.9875 - acc: 0.8627 - val_loss: 2.0325 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.85149\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.9419 - acc: 0.8767 - val_loss: 2.0317 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.85149\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.9769 - acc: 0.8527 - val_loss: 1.9986 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.85149 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.9403 - acc: 0.8710 - val_loss: 2.0099 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.85891 to 0.86139, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.9804 - acc: 0.8542 - val_loss: 2.0060 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.86139\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.9116 - acc: 0.8941 - val_loss: 1.9907 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.86139\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.9034 - acc: 0.8821 - val_loss: 2.0087 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.86139\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.9078 - acc: 0.8795 - val_loss: 1.9782 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.86139\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.8621 - acc: 0.8987 - val_loss: 1.9986 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.86139\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.8596 - acc: 0.8956 - val_loss: 2.0577 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.86139\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.8465 - acc: 0.9029 - val_loss: 2.0047 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.86139\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.8769 - acc: 0.8819 - val_loss: 1.9384 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.86139\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.8270 - acc: 0.9095 - val_loss: 1.9688 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00047: val_acc improved from 0.86139 to 0.86386, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.8119 - acc: 0.9074 - val_loss: 1.9697 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.86386 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.8594 - acc: 0.8891 - val_loss: 1.9436 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.87129\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.8473 - acc: 0.8915 - val_loss: 1.9456 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.87129\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.8193 - acc: 0.8999 - val_loss: 1.9378 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00051: val_acc improved from 0.87129 to 0.88366, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 1.8128 - acc: 0.9005 - val_loss: 1.9574 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.88366\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.7725 - acc: 0.9248 - val_loss: 1.9384 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88366\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.7350 - acc: 0.9329 - val_loss: 1.9764 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.88366\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.7552 - acc: 0.9176 - val_loss: 1.9559 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.88366\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.7708 - acc: 0.9056 - val_loss: 1.9165 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.88366\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.7394 - acc: 0.9242 - val_loss: 1.9139 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.88366\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.7381 - acc: 0.9227 - val_loss: 1.9973 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.88366\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.7276 - acc: 0.9245 - val_loss: 1.9338 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.88366\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.7058 - acc: 0.9362 - val_loss: 1.9853 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.88366\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.6870 - acc: 0.9378 - val_loss: 1.9753 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.88366\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.7339 - acc: 0.9242 - val_loss: 1.8907 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.88366\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.6903 - acc: 0.9366 - val_loss: 1.9007 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.88366\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.6923 - acc: 0.9315 - val_loss: 2.0244 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.88366\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.7094 - acc: 0.9197 - val_loss: 1.9498 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.88366\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.6704 - acc: 0.9368 - val_loss: 1.9021 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.88366\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.6443 - acc: 0.9426 - val_loss: 1.9221 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.88366\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.6417 - acc: 0.9456 - val_loss: 2.0039 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.88366\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.6546 - acc: 0.9384 - val_loss: 1.9832 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.88366\n",
      "Epoch 70/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 36s 1s/step - loss: 1.6416 - acc: 0.9441 - val_loss: 1.9440 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.88366\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.6223 - acc: 0.9525 - val_loss: 1.8869 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.88366\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.6251 - acc: 0.9420 - val_loss: 1.9009 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.88366\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.6101 - acc: 0.9507 - val_loss: 1.9446 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.88366\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.5906 - acc: 0.9573 - val_loss: 1.9372 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.88366\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5914 - acc: 0.9585 - val_loss: 1.9396 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.88366\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5869 - acc: 0.9609 - val_loss: 1.9165 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.88366\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5972 - acc: 0.9525 - val_loss: 1.8963 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.88366\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5727 - acc: 0.9525 - val_loss: 2.0063 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.88366\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5700 - acc: 0.9549 - val_loss: 1.9367 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.88366\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5962 - acc: 0.9447 - val_loss: 1.8641 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.88366\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.5823 - acc: 0.9534 - val_loss: 1.8601 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.88366 to 0.88861, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.5457 - acc: 0.9678 - val_loss: 1.9170 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.88861\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5486 - acc: 0.9618 - val_loss: 1.9682 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.88861\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5489 - acc: 0.9630 - val_loss: 1.8960 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.88861\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5640 - acc: 0.9540 - val_loss: 1.8321 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.88861\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5474 - acc: 0.9615 - val_loss: 1.8646 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.88861\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5204 - acc: 0.9693 - val_loss: 1.9869 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.88861\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5277 - acc: 0.9639 - val_loss: 2.0476 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.88861\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5258 - acc: 0.9657 - val_loss: 1.8876 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.88861\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5507 - acc: 0.9531 - val_loss: 1.8941 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.88861\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5316 - acc: 0.9645 - val_loss: 1.8649 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.88861\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5187 - acc: 0.9615 - val_loss: 1.8422 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.88861\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5014 - acc: 0.9723 - val_loss: 1.9576 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.88861\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5066 - acc: 0.9672 - val_loss: 1.8626 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.88861\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.5001 - acc: 0.9663 - val_loss: 1.9428 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.88861\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.5018 - acc: 0.9660 - val_loss: 1.8706 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.88861\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4806 - acc: 0.9714 - val_loss: 1.9502 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.88861\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4599 - acc: 0.9826 - val_loss: 2.0196 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.88861\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4960 - acc: 0.9648 - val_loss: 1.8660 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.88861\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4750 - acc: 0.9729 - val_loss: 1.9357 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00100: val_acc improved from 0.88861 to 0.89604, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4519 - acc: 0.9778 - val_loss: 2.0342 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89604\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4639 - acc: 0.9753 - val_loss: 1.9273 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89604\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4397 - acc: 0.9820 - val_loss: 2.1149 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89604\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4738 - acc: 0.9696 - val_loss: 1.8314 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89604\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.4490 - acc: 0.9771 - val_loss: 1.9045 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89604\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4434 - acc: 0.9801 - val_loss: 1.9372 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89604\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4193 - acc: 0.9904 - val_loss: 1.9491 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.89604\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4206 - acc: 0.9844 - val_loss: 1.9278 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.89604\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4203 - acc: 0.9850 - val_loss: 1.9646 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.89604\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4575 - acc: 0.9690 - val_loss: 2.0693 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.89604\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4430 - acc: 0.9775 - val_loss: 1.8880 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.89604\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4539 - acc: 0.9729 - val_loss: 2.0836 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.89604\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4203 - acc: 0.9838 - val_loss: 1.9441 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.89604\n",
      "Epoch 114/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 35s 1s/step - loss: 1.4268 - acc: 0.9759 - val_loss: 1.8658 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.89604\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4052 - acc: 0.9895 - val_loss: 1.9823 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.89604\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4197 - acc: 0.9790 - val_loss: 1.9740 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.89604\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4155 - acc: 0.9811 - val_loss: 1.9818 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.89604\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4258 - acc: 0.9705 - val_loss: 2.0954 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.89604\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4329 - acc: 0.9741 - val_loss: 1.8911 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.89604\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.3989 - acc: 0.9829 - val_loss: 2.0234 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.89604\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4123 - acc: 0.9753 - val_loss: 1.9022 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.89604\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4008 - acc: 0.9814 - val_loss: 1.9227 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00122: val_acc improved from 0.89604 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4249 - acc: 0.9721 - val_loss: 1.9590 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.89851\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.4207 - acc: 0.9696 - val_loss: 1.8152 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.89851\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3844 - acc: 0.9856 - val_loss: 1.9227 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.89851\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.4331 - acc: 0.9628 - val_loss: 1.8548 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.89851\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.4128 - acc: 0.9723 - val_loss: 1.7838 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.89851\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3920 - acc: 0.9835 - val_loss: 1.8398 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.89851\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3716 - acc: 0.9883 - val_loss: 1.8929 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.89851\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3729 - acc: 0.9844 - val_loss: 1.8678 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.89851\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3680 - acc: 0.9868 - val_loss: 1.8911 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.89851\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3584 - acc: 0.9886 - val_loss: 1.9128 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.89851\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3613 - acc: 0.9874 - val_loss: 1.8601 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.89851\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.3427 - acc: 0.9940 - val_loss: 1.9274 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.89851\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.3441 - acc: 0.9892 - val_loss: 1.9800 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.89851\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3563 - acc: 0.9844 - val_loss: 1.9599 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.89851\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3556 - acc: 0.9853 - val_loss: 1.8817 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.89851\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3471 - acc: 0.9874 - val_loss: 1.8423 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00138: val_acc improved from 0.89851 to 0.90594, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.01.h5\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3295 - acc: 0.9952 - val_loss: 2.0577 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90594\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3811 - acc: 0.9741 - val_loss: 1.8520 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90594\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3332 - acc: 0.9889 - val_loss: 1.9764 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90594\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3840 - acc: 0.9714 - val_loss: 1.8277 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90594\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.3471 - acc: 0.9841 - val_loss: 1.9489 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90594\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.3347 - acc: 0.9892 - val_loss: 1.9354 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90594\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.3244 - acc: 0.9904 - val_loss: 1.8971 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90594\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.3216 - acc: 0.9916 - val_loss: 1.9234 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90594\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3114 - acc: 0.9952 - val_loss: 1.8630 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90594\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3116 - acc: 0.9913 - val_loss: 1.9144 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90594\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3099 - acc: 0.9922 - val_loss: 1.9122 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90594\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3017 - acc: 0.9934 - val_loss: 1.9077 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90594\n",
      "experiment: dropout=0.4 regularization0.5\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 270.0686 - acc: 0.7252 - val_loss: 162.2298 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.5.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 135.3215 - acc: 0.7601 - val_loss: 118.0662 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.5.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 111.4281 - acc: 0.7643 - val_loss: 106.2254 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 103.4241 - acc: 0.7609 - val_loss: 100.9373 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 99.3017 - acc: 0.7604 - val_loss: 97.7550 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 96.6309 - acc: 0.7654 - val_loss: 95.5500 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76238\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 94.6942 - acc: 0.7595 - val_loss: 93.8534 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.5.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 93.1988 - acc: 0.7580 - val_loss: 92.4870 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76238\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 91.9317 - acc: 0.7639 - val_loss: 91.3376 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76238\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 90.8515 - acc: 0.7669 - val_loss: 90.3412 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76238\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 89.9235 - acc: 0.7625 - val_loss: 89.4706 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76238\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 89.0846 - acc: 0.7625 - val_loss: 88.6616 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76238\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 88.3218 - acc: 0.7589 - val_loss: 87.9331 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76238\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 87.6154 - acc: 0.7639 - val_loss: 87.2618 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76238\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 86.9637 - acc: 0.7669 - val_loss: 86.6349 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76238\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 86.3534 - acc: 0.7639 - val_loss: 86.0415 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76238\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 85.7816 - acc: 0.7610 - val_loss: 85.4869 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76238\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 85.2376 - acc: 0.7625 - val_loss: 84.9568 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76238\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 84.7320 - acc: 0.7580 - val_loss: 84.4620 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76238\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 84.2284 - acc: 0.7654 - val_loss: 83.9741 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76238\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 83.7613 - acc: 0.7639 - val_loss: 83.5166 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76238\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 83.3178 - acc: 0.7566 - val_loss: 83.0725 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76238\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 82.8757 - acc: 0.7625 - val_loss: 82.6540 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76238\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 82.4609 - acc: 0.7595 - val_loss: 82.2343 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76238\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 82.0509 - acc: 0.7610 - val_loss: 81.8392 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76238\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 81.6555 - acc: 0.7639 - val_loss: 81.4519 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76238\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 81.2790 - acc: 0.7610 - val_loss: 81.0784 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76238\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 80.9101 - acc: 0.7610 - val_loss: 80.7162 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76238\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 80.5461 - acc: 0.7654 - val_loss: 80.3631 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76238\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 80.2048 - acc: 0.7625 - val_loss: 80.0209 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76238\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 79.8642 - acc: 0.7610 - val_loss: 79.6856 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76238\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 79.5413 - acc: 0.7639 - val_loss: 79.3592 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76238\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 79.2195 - acc: 0.7625 - val_loss: 79.0408 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76238\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 78.8926 - acc: 0.7669 - val_loss: 78.7300 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.76238\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 78.5944 - acc: 0.7580 - val_loss: 78.4249 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.76238\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 78.2915 - acc: 0.7639 - val_loss: 78.1278 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.76238\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 77.9894 - acc: 0.7654 - val_loss: 77.8358 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.76238\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 77.7072 - acc: 0.7610 - val_loss: 77.5504 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.76238\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 77.4255 - acc: 0.7639 - val_loss: 77.2701 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.76238\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 77.1444 - acc: 0.7595 - val_loss: 76.9958 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.76238\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 76.8788 - acc: 0.7639 - val_loss: 76.7348 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.76238\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 76.6038 - acc: 0.7669 - val_loss: 76.4612 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.76238\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 76.3442 - acc: 0.7610 - val_loss: 76.2012 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.76238\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 76.0826 - acc: 0.7610 - val_loss: 75.9455 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.76238\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 75.8357 - acc: 0.7566 - val_loss: 75.6946 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.76238\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 75.5813 - acc: 0.7654 - val_loss: 75.4474 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.76238\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 75.3423 - acc: 0.7610 - val_loss: 75.2071 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.76238\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 75.0983 - acc: 0.7625 - val_loss: 74.9665 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.76238\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 74.8533 - acc: 0.7654 - val_loss: 74.7299 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.76238\n",
      "Epoch 50/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 74.6226 - acc: 0.7639 - val_loss: 74.4985 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.76238\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 74.3983 - acc: 0.7610 - val_loss: 74.2710 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.76238\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 74.1659 - acc: 0.7654 - val_loss: 74.0443 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.76238\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 73.9462 - acc: 0.7625 - val_loss: 73.8228 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.76238\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 73.7305 - acc: 0.7580 - val_loss: 73.6046 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.76238\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 73.5129 - acc: 0.7610 - val_loss: 73.3881 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.76238\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 73.2955 - acc: 0.7610 - val_loss: 73.1751 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.76238\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 73.0809 - acc: 0.7610 - val_loss: 72.9661 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.76238\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 72.8854 - acc: 0.7551 - val_loss: 72.7586 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.76238\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 72.6715 - acc: 0.7580 - val_loss: 72.5563 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.76238\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 72.4795 - acc: 0.7566 - val_loss: 72.3526 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.76238\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 72.2667 - acc: 0.7610 - val_loss: 72.1537 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.76238\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 72.0695 - acc: 0.7566 - val_loss: 71.9553 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.76238\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 71.8721 - acc: 0.7625 - val_loss: 71.7619 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.76238\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 71.6786 - acc: 0.7595 - val_loss: 71.5682 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.76238\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 71.4904 - acc: 0.7610 - val_loss: 71.3784 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.76238\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 71.2977 - acc: 0.7610 - val_loss: 71.1911 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.76238\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 71.1071 - acc: 0.7639 - val_loss: 71.0051 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.76238\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 70.9223 - acc: 0.7610 - val_loss: 70.8214 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.76238\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.7401 - acc: 0.7654 - val_loss: 70.6402 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.76238\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 70.5590 - acc: 0.7625 - val_loss: 70.4606 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.76238\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 70.3774 - acc: 0.7654 - val_loss: 70.2838 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.76238\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 70.2147 - acc: 0.7595 - val_loss: 70.1092 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.76238\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 70.0365 - acc: 0.7595 - val_loss: 69.9350 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.76238\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 69.8590 - acc: 0.7625 - val_loss: 69.7620 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.76238\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 69.6780 - acc: 0.7654 - val_loss: 69.5916 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.76238\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 69.5193 - acc: 0.7625 - val_loss: 69.4240 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.76238\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 69.3556 - acc: 0.7595 - val_loss: 69.2590 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.76238\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 69.1865 - acc: 0.7595 - val_loss: 69.0923 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.76238\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 69.0234 - acc: 0.7625 - val_loss: 68.9284 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.76238\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 68.8603 - acc: 0.7566 - val_loss: 68.7697 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.76238\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 68.6968 - acc: 0.7595 - val_loss: 68.6070 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.76238\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 68.5356 - acc: 0.7595 - val_loss: 68.4483 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.76238\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 68.3921 - acc: 0.7580 - val_loss: 68.2910 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.76238\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 68.2205 - acc: 0.7625 - val_loss: 68.1358 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.76238\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 68.0693 - acc: 0.7610 - val_loss: 67.9828 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.76238\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 67.9140 - acc: 0.7648 - val_loss: 67.8287 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.76238\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 67.7681 - acc: 0.7625 - val_loss: 67.6774 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.76238\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 67.6146 - acc: 0.7610 - val_loss: 67.5292 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.76238\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 67.4600 - acc: 0.7639 - val_loss: 67.3807 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.76238\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 67.3163 - acc: 0.7639 - val_loss: 67.2320 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.76238\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 67.1707 - acc: 0.7610 - val_loss: 67.0873 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.76238\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 67.0217 - acc: 0.7595 - val_loss: 66.9417 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.76238\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 66.8786 - acc: 0.7625 - val_loss: 66.7981 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.76238\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 66.7351 - acc: 0.7625 - val_loss: 66.6557 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.76238\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 66.5940 - acc: 0.7625 - val_loss: 66.5153 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.76238\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 66.4575 - acc: 0.7595 - val_loss: 66.3751 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.76238\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 66.3170 - acc: 0.7639 - val_loss: 66.2370 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.76238\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 66.1748 - acc: 0.7639 - val_loss: 66.0993 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.76238\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 66.0426 - acc: 0.7595 - val_loss: 65.9630 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.76238\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 65.9034 - acc: 0.7625 - val_loss: 65.8281 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.76238\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 65.7764 - acc: 0.7580 - val_loss: 65.6968 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.76238\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 65.6292 - acc: 0.7654 - val_loss: 65.5609 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.76238\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 65.5021 - acc: 0.7625 - val_loss: 65.4298 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.76238\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 65.3788 - acc: 0.7610 - val_loss: 65.2998 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.76238\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 65.2462 - acc: 0.7625 - val_loss: 65.1688 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.76238\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 65.1177 - acc: 0.7625 - val_loss: 65.0409 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.76238\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 64.9951 - acc: 0.7610 - val_loss: 64.9168 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.76238\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 64.8598 - acc: 0.7625 - val_loss: 64.7864 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.76238\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 64.7385 - acc: 0.7595 - val_loss: 64.6610 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.76238\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 64.6102 - acc: 0.7625 - val_loss: 64.5366 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.76238\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 64.4824 - acc: 0.7610 - val_loss: 64.4123 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.76238\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 64.3633 - acc: 0.7595 - val_loss: 64.2893 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.76238\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 64.2443 - acc: 0.7595 - val_loss: 64.1683 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.76238\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 64.1143 - acc: 0.7625 - val_loss: 64.0471 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.76238\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 64.0013 - acc: 0.7595 - val_loss: 63.9267 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.76238\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 63.8776 - acc: 0.7595 - val_loss: 63.8075 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.76238\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 63.7619 - acc: 0.7610 - val_loss: 63.6894 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.76238\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 63.6381 - acc: 0.7639 - val_loss: 63.5718 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.76238\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 63.5181 - acc: 0.7654 - val_loss: 63.4552 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.76238\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 63.4084 - acc: 0.7625 - val_loss: 63.3395 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.76238\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 63.2836 - acc: 0.7654 - val_loss: 63.2249 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.76238\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 63.1799 - acc: 0.7610 - val_loss: 63.1119 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.76238\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 63.0605 - acc: 0.7625 - val_loss: 62.9976 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.76238\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.9500 - acc: 0.7625 - val_loss: 62.8851 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.76238\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.8333 - acc: 0.7610 - val_loss: 62.7744 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.76238\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 62.7274 - acc: 0.7654 - val_loss: 62.6635 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.76238\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.6183 - acc: 0.7580 - val_loss: 62.5534 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.76238\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 62.5059 - acc: 0.7610 - val_loss: 62.4443 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.76238\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.4000 - acc: 0.7625 - val_loss: 62.3356 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.76238\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.2935 - acc: 0.7595 - val_loss: 62.2296 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.76238\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 62.1803 - acc: 0.7639 - val_loss: 62.1209 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.76238\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 62.0835 - acc: 0.7610 - val_loss: 62.0145 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.76238\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 61.9770 - acc: 0.7580 - val_loss: 61.9092 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.76238\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 61.8609 - acc: 0.7654 - val_loss: 61.8044 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.76238\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 61.7575 - acc: 0.7654 - val_loss: 61.7005 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.76238\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 61.6558 - acc: 0.7639 - val_loss: 61.5972 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.76238\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 61.5550 - acc: 0.7610 - val_loss: 61.4941 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.76238\n",
      "Epoch 138/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 36s 1s/step - loss: 61.4523 - acc: 0.7595 - val_loss: 61.3919 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.76238\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 61.3486 - acc: 0.7654 - val_loss: 61.2906 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.76238\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 61.2463 - acc: 0.7654 - val_loss: 61.1900 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.76238\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 61.1513 - acc: 0.7595 - val_loss: 61.0904 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.76238\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 61.0393 - acc: 0.7654 - val_loss: 60.9902 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.76238\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 60.9564 - acc: 0.7580 - val_loss: 60.8917 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.76238\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 60.8531 - acc: 0.7610 - val_loss: 60.7958 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.76238\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 60.7503 - acc: 0.7625 - val_loss: 60.6960 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.76238\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 60.6530 - acc: 0.7639 - val_loss: 60.5987 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.76238\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 60.5580 - acc: 0.7639 - val_loss: 60.5025 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.76238\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 60.4621 - acc: 0.7610 - val_loss: 60.4068 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.76238\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 60.3657 - acc: 0.7639 - val_loss: 60.3116 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.76238\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 60.2707 - acc: 0.7625 - val_loss: 60.2173 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.76238\n",
      "experiment: dropout=0.4 regularization0.1\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      " 6/26 [=====>........................] - ETA: 45s - loss: 80.0105 - acc: 0.6927"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: replica_0_25/sequential_35/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](replica_0_25/sequential_35/block1_conv1/Relu, block1_conv2_34/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training_32/Adam/gradients/dense_68_1/concat_grad/Slice_1/_21215 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_893_training_32/Adam/gradients/dense_68_1/concat_grad/Slice_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e58706e45698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         history = train_model(adam,model,'leishmaniasis',150,\n\u001b[0;32m---> 15\u001b[0;31m                               save_as='finetuning_reg_drop_'+str(_dropout)+'_'+str(_regularization))\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# store result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_dropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_regularization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-cdd8d7b7a337>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(optimizer, model, dataset, epochs, save_as, image_size, batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_flow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             verbose=1)\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: replica_0_25/sequential_35/block1_conv2/convolution = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](replica_0_25/sequential_35/block1_conv1/Relu, block1_conv2_34/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: training_32/Adam/gradients/dense_68_1/concat_grad/Slice_1/_21215 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:1\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_893_training_32/Adam/gradients/dense_68_1/concat_grad/Slice_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:1\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# create optimizer to train model\n",
    "adam = optimizers.Adam(lr=0.0001,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.00001,amsgrad=False)\n",
    "# create array to store experiment results\n",
    "data = []\n",
    "data.append(['dropout','regularization','val_acc'])\n",
    "# run experiment\n",
    "for _dropout in dropout:\n",
    "    for _regularization in regularization:\n",
    "        # Print level\n",
    "        print('experiment: dropout='+str(_dropout)+' regularization'+str(_regularization))\n",
    "        # build model\n",
    "        model = build_VGG19_fine_tuning_model(_dropout, _regularization)\n",
    "        # train model\n",
    "        history = train_model(adam,model,'leishmaniasis',150,\n",
    "                              save_as='finetuning_reg_drop_'+str(_dropout)+'_'+str(_regularization))\n",
    "        # store result\n",
    "        data.append([_dropout,_regularization,max(history.history['val_acc'])])\n",
    "        # Export history\n",
    "        export_history(history,'finetuning_reg_drop_'+str(_dropout)+'_'+str(_regularization),150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, imprimimos los resultados del experimento; para cada combinación de dropout y regularización, cual es el máximo valor de la exactitud de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dropout</td>\n",
       "      <td>regularization</td>\n",
       "      <td>val_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.918317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.881188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.90099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.762376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.856436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.915842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.878713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.898515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.762376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.858911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.915842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.873762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.898515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.762376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.858911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.913366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.878713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.905941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.762376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0               1         2\n",
       "0   dropout  regularization   val_acc\n",
       "1       0.1           0.001  0.918317\n",
       "2       0.1            0.05  0.881188\n",
       "3       0.1            0.01   0.90099\n",
       "4       0.1             0.5  0.762376\n",
       "5       0.1             0.1  0.856436\n",
       "6       0.2           0.001  0.915842\n",
       "7       0.2            0.05  0.878713\n",
       "8       0.2            0.01  0.898515\n",
       "9       0.2             0.5  0.762376\n",
       "10      0.2             0.1  0.858911\n",
       "11      0.3           0.001  0.915842\n",
       "12      0.3            0.05  0.873762\n",
       "13      0.3            0.01  0.898515\n",
       "14      0.3             0.5  0.762376\n",
       "15      0.3             0.1  0.858911\n",
       "16      0.4           0.001  0.913366\n",
       "17      0.4            0.05  0.878713\n",
       "18      0.4            0.01  0.905941\n",
       "19      0.4             0.5  0.762376"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print data from the experiment\n",
    "result = pd.DataFrame(data)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo intento\n",
    "\n",
    "Observamos que no se obtuvieron resultados mejores que en el modelado anterior. Se procede a eliminar una capa de dropout (como se tenia en el experimento con mayor exactitud de validación), y se corre de nuevo el experimento con valores mas pequeños en la regularización, ya que en la tabla anterior se observa que para valores más pequeños de regularizacón el modelo desempeña mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds model for transfer learning and fine tunning\n",
    "#  PARAMS\n",
    "#     dropout: dropout value for dropout layers\n",
    "#     regularization: regularization \n",
    "def build_VGG19_fine_tuning_model(dropout, regularization):\n",
    "    model = Sequential()\n",
    "    # block 1\n",
    "    model.add(Conv2D(64, (3, 3),activation='relu',padding='same',name='block1_conv1',input_shape=(224,224,3)))\n",
    "    model.add(Conv2D(64, (3, 3),activation='relu',padding='same',name='block1_conv2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3, 3),activation='relu',padding='same',name='block2_conv1'))\n",
    "    model.add(Conv2D(128, (3, 3),activation='relu',padding='same',name='block2_conv2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv1'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv2'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv3'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv4'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv1'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv2'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv3'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv4'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n",
    "    # Block 5\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv1',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv2',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv3',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv4',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))\n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        cache_subdir='models',\n",
    "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
    "    model.load_weights(weights_path)\n",
    "    # Freeze conv layers that are not going to be trained\n",
    "    for layer in model.layers[:-5]:\n",
    "        layer.trainable = False \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1,activation='sigmoid'))    \n",
    "    # Multigpu model to speed up\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    return model  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo, definimos los diferentes niveles de regularización y dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "regularization = [0.001, 0.0005, 0.0001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tercero, deifnimos la función de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model\n",
    "#     PARAMS:\n",
    "#        optimizer: optimizer to train model\n",
    "#        model: model to be trained\n",
    "#        dataset: dataset to train modelo \n",
    "#        epoochs: number of epochs to train model\n",
    "#        save_as: name to save best weigths\n",
    "#     RETURNS\n",
    "#        history: vaules for acc and val_acc for each epoch\n",
    "def train_model(optimizer,model,dataset,epochs,save_as,image_size=224,batch_size=64):\n",
    "    # Data generator to  rescale training images\n",
    "    # Data Augmentation: Horizontal and vertical flips\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255.0,\n",
    "                                      vertical_flip=True,\n",
    "                                      horizontal_flip=True)\n",
    "    # Data generator to rescale test images\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255.0)\n",
    "    # Data flow training images\n",
    "    train_flow = train_datagen.flow_from_directory(\n",
    "        directory='src/'+dataset+'/training',  \n",
    "        target_size=(image_size, image_size),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    # Data flow test images\n",
    "    test_flow = test_datagen.flow_from_directory(\n",
    "        directory='src/'+dataset+'/test',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['acc'])\n",
    "    # Create check point call back to store best validation weigths\n",
    "    bestWeigthsPath='src/trainingWeigths/best_' + save_as+'.h5'\n",
    "    checkpoint = ModelCheckpoint(bestWeigthsPath, monitor='val_acc',save_weights_only=False, verbose=1, save_best_only=True, mode='max')\n",
    "    # Run experiment\n",
    "    history = model.fit_generator(\n",
    "            generator=train_flow,\n",
    "            epochs=epochs,\n",
    "            validation_data=test_flow,\n",
    "            callbacks=[checkpoint],\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuarto, definimos función para exportar historia de cada entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_history(history, save_as,epochs):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    file = open('src/history/history' + save_as+'.txt','w')\n",
    "    file.write('acc,val_acc'+'\\n')\n",
    "    for i in range(epochs):\n",
    "        file.write(str(acc[i])+','+str(val_acc[i])+'\\n')\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quinto, ejecutamos el experimento para cada combinación de dropout y regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment: dropout=0.1 regularization0.001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 1.4731 - acc: 0.7363 - val_loss: 1.2926 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.2444 - acc: 0.7580 - val_loss: 1.1760 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.77228, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.1256 - acc: 0.7745 - val_loss: 1.0983 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77228 to 0.78960, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.0553 - acc: 0.7998 - val_loss: 0.9991 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78960 to 0.81931, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.9791 - acc: 0.8202 - val_loss: 0.9729 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.81931\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.9112 - acc: 0.8416 - val_loss: 0.8947 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.81931 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.9027 - acc: 0.8208 - val_loss: 0.8854 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.84901\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.8242 - acc: 0.8599 - val_loss: 0.8585 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.84901\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.7931 - acc: 0.8669 - val_loss: 0.8406 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.84901\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.7779 - acc: 0.8693 - val_loss: 0.8327 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.84901\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.7226 - acc: 0.8966 - val_loss: 0.7710 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.84901 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7284 - acc: 0.8795 - val_loss: 0.8251 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.88119\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.7050 - acc: 0.8891 - val_loss: 0.8106 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88119\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.6417 - acc: 0.9239 - val_loss: 0.7862 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.88119\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.6135 - acc: 0.9227 - val_loss: 0.7651 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88119\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.5776 - acc: 0.9366 - val_loss: 0.7903 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.88119 to 0.88366, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5692 - acc: 0.9390 - val_loss: 0.7916 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88366\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.5451 - acc: 0.9477 - val_loss: 0.7646 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.88366 to 0.88614, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5120 - acc: 0.9561 - val_loss: 0.7994 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88614\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.5272 - acc: 0.9525 - val_loss: 0.7560 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.88614 to 0.88861, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5230 - acc: 0.9528 - val_loss: 0.7256 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88861\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5228 - acc: 0.9422 - val_loss: 0.7536 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88861\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4932 - acc: 0.9564 - val_loss: 0.6899 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.88861 to 0.90594, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4678 - acc: 0.9669 - val_loss: 0.7209 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.90594\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4477 - acc: 0.9765 - val_loss: 0.7472 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.90594 to 0.91089, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4792 - acc: 0.9618 - val_loss: 0.8553 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.91089\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4616 - acc: 0.9693 - val_loss: 0.7730 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.91089\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4261 - acc: 0.9838 - val_loss: 0.8364 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.91089\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4419 - acc: 0.9721 - val_loss: 0.7797 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.91089\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.4605 - acc: 0.9640 - val_loss: 0.8428 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.91089\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.4220 - acc: 0.9807 - val_loss: 0.7749 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.91089\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4302 - acc: 0.9747 - val_loss: 0.7971 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.91089\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4201 - acc: 0.9765 - val_loss: 0.7649 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.91089\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.4010 - acc: 0.9844 - val_loss: 0.7987 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.91089\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.4167 - acc: 0.9778 - val_loss: 0.7358 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.91089\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 0.4173 - acc: 0.9741 - val_loss: 0.7525 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.91089\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4038 - acc: 0.9820 - val_loss: 0.9137 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.91089\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3995 - acc: 0.9795 - val_loss: 0.8593 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.91089\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3907 - acc: 0.9868 - val_loss: 0.7154 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.91089\n",
      "Epoch 40/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 35s 1s/step - loss: 0.3959 - acc: 0.9801 - val_loss: 0.8176 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.91089\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3814 - acc: 0.9886 - val_loss: 0.7694 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.91089\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3740 - acc: 0.9910 - val_loss: 0.8750 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.91089\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3665 - acc: 0.9892 - val_loss: 0.7758 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.91089\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 0.3778 - acc: 0.9820 - val_loss: 0.9208 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.91089\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3686 - acc: 0.9886 - val_loss: 0.9711 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.91089\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.3789 - acc: 0.9826 - val_loss: 0.9206 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.91089\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4025 - acc: 0.9741 - val_loss: 1.1271 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.91089\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.4179 - acc: 0.9699 - val_loss: 0.7386 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.91089\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3651 - acc: 0.9868 - val_loss: 0.7374 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.91089\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3641 - acc: 0.9847 - val_loss: 0.8205 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.91089\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3435 - acc: 0.9934 - val_loss: 0.7570 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.91089\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3369 - acc: 0.9958 - val_loss: 0.8365 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.91089\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3310 - acc: 0.9964 - val_loss: 0.7981 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.91089\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3274 - acc: 0.9964 - val_loss: 0.8693 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.91089\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3382 - acc: 0.9928 - val_loss: 0.9452 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.91089\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3311 - acc: 0.9970 - val_loss: 0.8924 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.91089\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3222 - acc: 0.9976 - val_loss: 0.9224 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.91089\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3352 - acc: 0.9898 - val_loss: 0.8511 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.91089\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3325 - acc: 0.9922 - val_loss: 0.7709 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.91089 to 0.91337, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3199 - acc: 0.9952 - val_loss: 0.7140 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.91337\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.3249 - acc: 0.9901 - val_loss: 0.7898 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.91337\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3236 - acc: 0.9928 - val_loss: 0.9097 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.91337\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.3422 - acc: 0.9874 - val_loss: 0.7647 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.91337\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3577 - acc: 0.9745 - val_loss: 0.7547 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.91337\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3535 - acc: 0.9783 - val_loss: 0.7934 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.91337\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3326 - acc: 0.9922 - val_loss: 0.7963 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.91337\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3197 - acc: 0.9952 - val_loss: 0.7619 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.91337\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3139 - acc: 0.9940 - val_loss: 0.7861 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.91337\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 39s 1s/step - loss: 0.3142 - acc: 0.9946 - val_loss: 0.8253 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.91337\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.3234 - acc: 0.9916 - val_loss: 0.9444 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.91337\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3249 - acc: 0.9874 - val_loss: 0.7576 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.91337\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3116 - acc: 0.9913 - val_loss: 0.7748 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.91337\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3333 - acc: 0.9823 - val_loss: 0.8105 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.91337\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3376 - acc: 0.9880 - val_loss: 0.7889 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.91337\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3184 - acc: 0.9892 - val_loss: 0.6123 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00075: val_acc improved from 0.91337 to 0.91832, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2966 - acc: 0.9976 - val_loss: 0.6955 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.91832\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2914 - acc: 0.9994 - val_loss: 0.6920 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00077: val_acc improved from 0.91832 to 0.92079, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2875 - acc: 0.9988 - val_loss: 0.6832 - val_acc: 0.9257\n",
      "\n",
      "Epoch 00078: val_acc improved from 0.92079 to 0.92574, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.001_second.h5\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2882 - acc: 0.9982 - val_loss: 0.7577 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.92574\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2884 - acc: 0.9976 - val_loss: 0.7107 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.92574\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2945 - acc: 0.9958 - val_loss: 0.7001 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.92574\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3001 - acc: 0.9913 - val_loss: 0.7295 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.92574\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 36s 1s/step - loss: 0.2971 - acc: 0.9934 - val_loss: 0.7963 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.92574\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2963 - acc: 0.9913 - val_loss: 0.7807 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.92574\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3009 - acc: 0.9934 - val_loss: 0.7947 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.92574\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.2943 - acc: 0.9934 - val_loss: 0.8489 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.92574\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3048 - acc: 0.9880 - val_loss: 0.8568 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.92574\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2985 - acc: 0.9919 - val_loss: 0.8193 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.92574\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3043 - acc: 0.9880 - val_loss: 0.8593 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.92574\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3017 - acc: 0.9904 - val_loss: 0.6785 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.92574\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2798 - acc: 0.9970 - val_loss: 0.7193 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.92574\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2861 - acc: 0.9940 - val_loss: 0.8407 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.92574\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3076 - acc: 0.9877 - val_loss: 0.8076 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.92574\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2876 - acc: 0.9940 - val_loss: 0.7316 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.92574\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.2772 - acc: 0.9970 - val_loss: 0.8804 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.92574\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2970 - acc: 0.9898 - val_loss: 0.7940 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.92574\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2861 - acc: 0.9976 - val_loss: 0.7830 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.92574\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2735 - acc: 0.9982 - val_loss: 0.7630 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.92574\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.2756 - acc: 0.9955 - val_loss: 1.0236 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.92574\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2936 - acc: 0.9892 - val_loss: 0.8372 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.92574\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2980 - acc: 0.9871 - val_loss: 0.7642 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.92574\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2729 - acc: 0.9964 - val_loss: 0.8055 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.92574\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2699 - acc: 0.9976 - val_loss: 0.7574 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.92574\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2658 - acc: 0.9976 - val_loss: 0.7507 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.92574\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2598 - acc: 0.9988 - val_loss: 0.7322 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.92574\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2578 - acc: 0.9994 - val_loss: 0.8102 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.92574\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2549 - acc: 1.0000 - val_loss: 0.8680 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.92574\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2522 - acc: 1.0000 - val_loss: 0.7730 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.92574\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2506 - acc: 1.0000 - val_loss: 0.7803 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.92574\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2487 - acc: 1.0000 - val_loss: 0.7830 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.92574\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2468 - acc: 1.0000 - val_loss: 0.8036 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.92574\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2455 - acc: 1.0000 - val_loss: 0.7890 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.92574\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2436 - acc: 1.0000 - val_loss: 0.7869 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.92574\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.2437 - acc: 0.9994 - val_loss: 0.7656 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.92574\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2446 - acc: 0.9988 - val_loss: 0.7200 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.92574\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2559 - acc: 0.9952 - val_loss: 0.7626 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.92574\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2749 - acc: 0.9868 - val_loss: 0.9701 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.92574\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3221 - acc: 0.9717 - val_loss: 0.8456 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.92574\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2785 - acc: 0.9886 - val_loss: 0.6857 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.92574\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2527 - acc: 0.9970 - val_loss: 0.7522 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.92574\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2444 - acc: 1.0000 - val_loss: 0.7082 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.92574\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2475 - acc: 0.9976 - val_loss: 0.8820 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.92574\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2461 - acc: 0.9970 - val_loss: 0.7754 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.92574\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2455 - acc: 0.9958 - val_loss: 0.7933 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.92574\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2376 - acc: 1.0000 - val_loss: 0.7834 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.92574\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2371 - acc: 0.9988 - val_loss: 0.8514 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.92574\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 0.2492 - acc: 0.9934 - val_loss: 0.8687 - val_acc: 0.8936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00127: val_acc did not improve from 0.92574\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 0.2599 - acc: 0.9898 - val_loss: 0.7230 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.92574\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2936 - acc: 0.9826 - val_loss: 0.7464 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.92574\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2686 - acc: 0.9868 - val_loss: 0.7088 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.92574\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2466 - acc: 0.9952 - val_loss: 0.6845 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.92574\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2394 - acc: 0.9967 - val_loss: 0.7370 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.92574\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2382 - acc: 0.9976 - val_loss: 0.7673 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.92574\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2436 - acc: 0.9964 - val_loss: 0.7751 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.92574\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3184 - acc: 0.9760 - val_loss: 0.8381 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.92574\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.3067 - acc: 0.9747 - val_loss: 0.6057 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.92574\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2576 - acc: 0.9883 - val_loss: 0.6528 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.92574\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2433 - acc: 0.9952 - val_loss: 0.7781 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.92574\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2386 - acc: 0.9976 - val_loss: 0.7263 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.92574\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2306 - acc: 0.9994 - val_loss: 0.7789 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.92574\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2283 - acc: 1.0000 - val_loss: 0.7093 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.92574\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2260 - acc: 1.0000 - val_loss: 0.7201 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.92574\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2246 - acc: 1.0000 - val_loss: 0.7558 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.92574\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2229 - acc: 1.0000 - val_loss: 0.7533 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.92574\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2217 - acc: 1.0000 - val_loss: 0.7377 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.92574\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2205 - acc: 1.0000 - val_loss: 0.8392 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.92574\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2196 - acc: 1.0000 - val_loss: 0.8181 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.92574\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2179 - acc: 1.0000 - val_loss: 0.7596 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.92574\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2166 - acc: 1.0000 - val_loss: 0.7651 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.92574\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2155 - acc: 1.0000 - val_loss: 0.7699 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.92574\n",
      "experiment: dropout=0.1 regularization0.0005\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 37s 1s/step - loss: 1.0626 - acc: 0.7279 - val_loss: 0.9414 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.8628 - acc: 0.7607 - val_loss: 0.8174 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.7786 - acc: 0.7580 - val_loss: 0.7370 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.7380 - acc: 0.7625 - val_loss: 0.7206 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.7032 - acc: 0.7625 - val_loss: 0.7056 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6773 - acc: 0.7660 - val_loss: 0.6622 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76238\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.6801 - acc: 0.7643 - val_loss: 0.6411 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76238\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6306 - acc: 0.7877 - val_loss: 0.6273 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.76238 to 0.78218, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6220 - acc: 0.7859 - val_loss: 0.5991 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.78218 to 0.80198, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.6092 - acc: 0.7941 - val_loss: 0.6148 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.80198\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5884 - acc: 0.8118 - val_loss: 0.5776 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.80198 to 0.80693, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5696 - acc: 0.8289 - val_loss: 0.5633 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.80693 to 0.81188, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5628 - acc: 0.8208 - val_loss: 0.5663 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.81188 to 0.82178, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5473 - acc: 0.8310 - val_loss: 0.6340 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82178\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5576 - acc: 0.8205 - val_loss: 0.5527 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.82178 to 0.82426, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5245 - acc: 0.8346 - val_loss: 0.5628 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.82426 to 0.82673, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5238 - acc: 0.8236 - val_loss: 0.5440 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.82673 to 0.82673, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5088 - acc: 0.8505 - val_loss: 0.5275 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.82673 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5104 - acc: 0.8419 - val_loss: 0.5181 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.83911 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4916 - acc: 0.8569 - val_loss: 0.5701 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.84158\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4738 - acc: 0.8713 - val_loss: 0.5017 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.84158 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5138 - acc: 0.8253 - val_loss: 0.5176 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.84901\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4711 - acc: 0.8617 - val_loss: 0.5153 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.84901\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4459 - acc: 0.8782 - val_loss: 0.5420 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.84901\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4638 - acc: 0.8653 - val_loss: 0.4923 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.84901\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4383 - acc: 0.8743 - val_loss: 0.5041 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.84901 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4320 - acc: 0.8725 - val_loss: 0.5069 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.85396\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4118 - acc: 0.8872 - val_loss: 0.5628 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.85396\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4231 - acc: 0.8840 - val_loss: 0.5051 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.85396 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3907 - acc: 0.8983 - val_loss: 0.5793 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.86634\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3928 - acc: 0.9008 - val_loss: 0.5563 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.86634\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3683 - acc: 0.9113 - val_loss: 0.6916 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.86634\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3847 - acc: 0.9008 - val_loss: 0.4819 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.86634\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3552 - acc: 0.9098 - val_loss: 0.5341 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.86634\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3711 - acc: 0.9017 - val_loss: 0.5224 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.86634\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3640 - acc: 0.9086 - val_loss: 0.5721 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.86634\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3615 - acc: 0.9054 - val_loss: 0.5365 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.86634\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3317 - acc: 0.9230 - val_loss: 0.5393 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.86634\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3338 - acc: 0.9168 - val_loss: 0.5275 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.86634\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3318 - acc: 0.9107 - val_loss: 0.7217 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.86634\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3287 - acc: 0.9248 - val_loss: 0.5179 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.86634 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3007 - acc: 0.9339 - val_loss: 0.5826 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88119\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2978 - acc: 0.9348 - val_loss: 0.6166 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88119\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2846 - acc: 0.9392 - val_loss: 0.5316 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88119\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2948 - acc: 0.9270 - val_loss: 0.7233 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88119\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3023 - acc: 0.9264 - val_loss: 0.5236 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.88119 to 0.88366, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2631 - acc: 0.9486 - val_loss: 0.5599 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88366\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2753 - acc: 0.9420 - val_loss: 0.6392 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88366\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2750 - acc: 0.9420 - val_loss: 0.5274 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88366\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2616 - acc: 0.9471 - val_loss: 0.5211 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00050: val_acc improved from 0.88366 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2370 - acc: 0.9561 - val_loss: 0.5678 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89356\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2351 - acc: 0.9579 - val_loss: 0.6623 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89356\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2319 - acc: 0.9615 - val_loss: 0.5422 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89356\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2283 - acc: 0.9588 - val_loss: 0.5667 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89356\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2086 - acc: 0.9660 - val_loss: 0.5750 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89356\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2242 - acc: 0.9585 - val_loss: 0.6555 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89356\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2314 - acc: 0.9585 - val_loss: 0.7229 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89356\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1996 - acc: 0.9711 - val_loss: 0.7031 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.89356\n",
      "Epoch 59/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 35s 1s/step - loss: 0.2058 - acc: 0.9666 - val_loss: 0.6544 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89356\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2029 - acc: 0.9621 - val_loss: 0.6161 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.89356\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2555 - acc: 0.9468 - val_loss: 0.6120 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89356\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2147 - acc: 0.9687 - val_loss: 0.5699 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89356\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1971 - acc: 0.9723 - val_loss: 0.5919 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89356\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1777 - acc: 0.9783 - val_loss: 0.6726 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89356\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1778 - acc: 0.9811 - val_loss: 0.7390 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89356\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1712 - acc: 0.9820 - val_loss: 0.6093 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.89356 to 0.89604, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1789 - acc: 0.9793 - val_loss: 0.6290 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89604\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1750 - acc: 0.9795 - val_loss: 0.6350 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89604\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2075 - acc: 0.9640 - val_loss: 0.8780 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89604\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2287 - acc: 0.9582 - val_loss: 0.6908 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89604\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1774 - acc: 0.9795 - val_loss: 0.6438 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89604\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1586 - acc: 0.9850 - val_loss: 0.6500 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00072: val_acc improved from 0.89604 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1557 - acc: 0.9862 - val_loss: 0.6947 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89851\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1646 - acc: 0.9814 - val_loss: 0.7593 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89851\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1958 - acc: 0.9705 - val_loss: 0.8276 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89851\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1695 - acc: 0.9789 - val_loss: 0.6631 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89851\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1509 - acc: 0.9880 - val_loss: 0.6074 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89851\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1532 - acc: 0.9874 - val_loss: 0.6333 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89851\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1512 - acc: 0.9850 - val_loss: 0.6401 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00079: val_acc improved from 0.89851 to 0.90842, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1466 - acc: 0.9877 - val_loss: 0.6922 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90842\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1639 - acc: 0.9801 - val_loss: 0.7475 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90842\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1557 - acc: 0.9844 - val_loss: 0.7361 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90842\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1430 - acc: 0.9922 - val_loss: 0.6957 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90842\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1372 - acc: 0.9910 - val_loss: 0.7193 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90842\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1671 - acc: 0.9801 - val_loss: 0.6406 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90842\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1492 - acc: 0.9874 - val_loss: 0.7388 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90842\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1317 - acc: 0.9946 - val_loss: 0.7253 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90842\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1365 - acc: 0.9904 - val_loss: 0.7393 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90842\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1342 - acc: 0.9910 - val_loss: 0.6864 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90842\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1449 - acc: 0.9886 - val_loss: 0.8174 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90842\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1436 - acc: 0.9886 - val_loss: 0.8042 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90842\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1369 - acc: 0.9910 - val_loss: 0.7098 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90842\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1549 - acc: 0.9832 - val_loss: 0.8305 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90842\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1366 - acc: 0.9910 - val_loss: 0.7542 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90842\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1301 - acc: 0.9928 - val_loss: 0.8391 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90842\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1311 - acc: 0.9910 - val_loss: 0.8202 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90842\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1218 - acc: 0.9970 - val_loss: 0.8609 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90842\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1164 - acc: 0.9982 - val_loss: 0.7510 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90842\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1179 - acc: 0.9952 - val_loss: 0.7298 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90842\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1277 - acc: 0.9916 - val_loss: 0.7638 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90842\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1394 - acc: 0.9844 - val_loss: 0.7697 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.90842\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1225 - acc: 0.9958 - val_loss: 0.7966 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.90842\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1159 - acc: 0.9964 - val_loss: 0.8070 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.90842\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1139 - acc: 0.9988 - val_loss: 0.9256 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.90842\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1113 - acc: 0.9976 - val_loss: 0.8548 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.90842\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1130 - acc: 0.9970 - val_loss: 0.8059 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.90842\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1116 - acc: 0.9982 - val_loss: 0.8581 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.90842\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1138 - acc: 0.9964 - val_loss: 0.8226 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.90842\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1362 - acc: 0.9874 - val_loss: 1.0920 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.90842\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1608 - acc: 0.9807 - val_loss: 0.8510 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.90842\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2010 - acc: 0.9585 - val_loss: 0.8246 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.90842\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2046 - acc: 0.9642 - val_loss: 0.6121 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.90842\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1332 - acc: 0.9892 - val_loss: 0.6970 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.90842\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1276 - acc: 0.9928 - val_loss: 0.8462 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.90842\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1279 - acc: 0.9910 - val_loss: 0.7844 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.90842\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1149 - acc: 0.9958 - val_loss: 0.8045 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.90842\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1125 - acc: 0.9976 - val_loss: 0.9008 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90842\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1138 - acc: 0.9970 - val_loss: 0.9080 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90842\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1169 - acc: 0.9946 - val_loss: 0.8036 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90842\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1230 - acc: 0.9928 - val_loss: 0.7898 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90842\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1198 - acc: 0.9934 - val_loss: 0.8013 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90842\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1189 - acc: 0.9946 - val_loss: 0.7599 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90842\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1092 - acc: 0.9988 - val_loss: 0.8205 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.90842\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1126 - acc: 0.9976 - val_loss: 0.8348 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90842\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1088 - acc: 0.9976 - val_loss: 0.8459 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90842\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1083 - acc: 0.9982 - val_loss: 0.8087 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90842\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1073 - acc: 0.9994 - val_loss: 0.8398 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90842\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1044 - acc: 0.9982 - val_loss: 0.8362 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.90842\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1030 - acc: 0.9988 - val_loss: 1.0117 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90842\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3039 - acc: 0.9450 - val_loss: 0.5291 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90842\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1900 - acc: 0.9657 - val_loss: 0.5748 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90842\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1375 - acc: 0.9862 - val_loss: 0.5612 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90842\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1211 - acc: 0.9931 - val_loss: 0.6636 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90842\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1100 - acc: 0.9976 - val_loss: 0.7142 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90842\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1142 - acc: 0.9931 - val_loss: 0.6989 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90842\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1278 - acc: 0.9904 - val_loss: 0.6208 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90842\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1157 - acc: 0.9970 - val_loss: 0.7081 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90842\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1105 - acc: 0.9958 - val_loss: 0.6968 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90842\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1154 - acc: 0.9952 - val_loss: 0.7623 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90842\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1116 - acc: 0.9964 - val_loss: 0.8689 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90842\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1227 - acc: 0.9904 - val_loss: 0.8035 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90842\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1092 - acc: 0.9976 - val_loss: 0.7897 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90842\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1045 - acc: 0.9994 - val_loss: 0.8432 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90842\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1022 - acc: 1.0000 - val_loss: 0.7877 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90842\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1013 - acc: 0.9994 - val_loss: 0.8324 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90842\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1001 - acc: 1.0000 - val_loss: 0.7933 - val_acc: 0.8936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90842\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1002 - acc: 1.0000 - val_loss: 0.8300 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90842\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0990 - acc: 1.0000 - val_loss: 0.8422 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90842\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0985 - acc: 1.0000 - val_loss: 0.8346 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90842\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0984 - acc: 1.0000 - val_loss: 0.8304 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00150: val_acc improved from 0.90842 to 0.90842, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0005_second.h5\n",
      "experiment: dropout=0.1 regularization0.0001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.7317 - acc: 0.7625 - val_loss: 0.6095 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6381 - acc: 0.7540 - val_loss: 0.6111 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5885 - acc: 0.7619 - val_loss: 0.6300 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5781 - acc: 0.7580 - val_loss: 0.5474 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5214 - acc: 0.7805 - val_loss: 0.5127 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.76238 to 0.79703, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5136 - acc: 0.7941 - val_loss: 0.5123 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79703\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4840 - acc: 0.8069 - val_loss: 0.4735 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79703\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4665 - acc: 0.8109 - val_loss: 0.4987 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.79703 to 0.81188, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4594 - acc: 0.8175 - val_loss: 0.4637 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.81188 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4471 - acc: 0.8255 - val_loss: 0.4642 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81683\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4271 - acc: 0.8319 - val_loss: 0.4509 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81683 to 0.82673, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4076 - acc: 0.8487 - val_loss: 0.4925 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82673\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4032 - acc: 0.8532 - val_loss: 0.4915 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82673\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3977 - acc: 0.8539 - val_loss: 0.4485 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82673\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3855 - acc: 0.8506 - val_loss: 0.4812 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.82673\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3938 - acc: 0.8512 - val_loss: 0.4635 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.82673 to 0.83416, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3689 - acc: 0.8671 - val_loss: 0.4430 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83416\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3487 - acc: 0.8749 - val_loss: 0.4570 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83416\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3258 - acc: 0.8932 - val_loss: 0.4680 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83416\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3160 - acc: 0.8890 - val_loss: 0.4536 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.83416 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3024 - acc: 0.8942 - val_loss: 0.4579 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.84901\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2926 - acc: 0.9014 - val_loss: 0.4103 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.84901 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2928 - acc: 0.9074 - val_loss: 0.4001 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.85891\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2792 - acc: 0.9059 - val_loss: 0.4320 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.85891 to 0.86386, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2570 - acc: 0.9122 - val_loss: 0.4784 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86386\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2748 - acc: 0.9062 - val_loss: 0.5338 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86386\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2527 - acc: 0.9158 - val_loss: 0.4293 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.86386 to 0.87376, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2248 - acc: 0.9206 - val_loss: 0.5577 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87376\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2399 - acc: 0.9206 - val_loss: 0.4398 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87376\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2112 - acc: 0.9366 - val_loss: 0.4725 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87376\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1968 - acc: 0.9386 - val_loss: 0.4735 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87376\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1793 - acc: 0.9444 - val_loss: 0.4965 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.87376 to 0.87376, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1698 - acc: 0.9465 - val_loss: 0.6166 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87376\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1839 - acc: 0.9477 - val_loss: 0.5027 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87376\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1470 - acc: 0.9606 - val_loss: 0.5212 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87376\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1609 - acc: 0.9543 - val_loss: 0.5090 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87376\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1405 - acc: 0.9651 - val_loss: 0.5388 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.87376 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1456 - acc: 0.9597 - val_loss: 0.5543 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87624\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1225 - acc: 0.9675 - val_loss: 0.6214 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87624\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1167 - acc: 0.9702 - val_loss: 0.5726 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87624\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1334 - acc: 0.9657 - val_loss: 0.6214 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87624\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1494 - acc: 0.9558 - val_loss: 0.5554 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.87624\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1088 - acc: 0.9771 - val_loss: 0.5839 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.87624 to 0.87871, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1135 - acc: 0.9753 - val_loss: 0.6521 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.87871\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1241 - acc: 0.9681 - val_loss: 0.5967 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.87871\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1018 - acc: 0.9759 - val_loss: 0.7799 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.87871\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0866 - acc: 0.9807 - val_loss: 0.7100 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.87871\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0894 - acc: 0.9832 - val_loss: 0.6732 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.87871 to 0.88366, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0828 - acc: 0.9838 - val_loss: 0.8099 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88366\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0910 - acc: 0.9844 - val_loss: 0.7064 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88366\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0781 - acc: 0.9868 - val_loss: 0.7938 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.88366\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0688 - acc: 0.9898 - val_loss: 0.7386 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.88366\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0916 - acc: 0.9795 - val_loss: 0.7644 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88366\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0813 - acc: 0.9844 - val_loss: 0.7440 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.88366 to 0.88366, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0921 - acc: 0.9775 - val_loss: 0.6992 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00055: val_acc improved from 0.88366 to 0.88614, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1043 - acc: 0.9787 - val_loss: 0.9735 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.88614\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1359 - acc: 0.9606 - val_loss: 0.8427 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.88614\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0878 - acc: 0.9856 - val_loss: 0.6414 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.88614\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0665 - acc: 0.9916 - val_loss: 0.6848 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.88614\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0659 - acc: 0.9922 - val_loss: 0.7752 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.88614\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0576 - acc: 0.9940 - val_loss: 0.7989 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.88614\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0633 - acc: 0.9946 - val_loss: 0.8203 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.88614\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0529 - acc: 0.9976 - val_loss: 0.8522 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.88614\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0561 - acc: 0.9940 - val_loss: 0.9300 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.88614\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0542 - acc: 0.9940 - val_loss: 0.9217 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.88614\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0521 - acc: 0.9970 - val_loss: 1.0466 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.88614\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0772 - acc: 0.9898 - val_loss: 0.8266 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.88614\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0734 - acc: 0.9874 - val_loss: 0.8810 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.88614\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0620 - acc: 0.9904 - val_loss: 0.8577 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.88614\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0571 - acc: 0.9958 - val_loss: 0.7826 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.88614\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0617 - acc: 0.9928 - val_loss: 0.8354 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00071: val_acc improved from 0.88614 to 0.89604, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.1_0.0001_second.h5\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0821 - acc: 0.9807 - val_loss: 0.9291 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89604\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0842 - acc: 0.9835 - val_loss: 0.7707 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89604\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0734 - acc: 0.9883 - val_loss: 0.7321 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89604\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0626 - acc: 0.9940 - val_loss: 0.8696 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89604\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0510 - acc: 0.9970 - val_loss: 0.8266 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89604\n",
      "Epoch 77/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.0467 - acc: 0.9994 - val_loss: 0.8456 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89604\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0455 - acc: 0.9982 - val_loss: 1.0723 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89604\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0676 - acc: 0.9868 - val_loss: 0.8848 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89604\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0528 - acc: 0.9946 - val_loss: 0.9340 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89604\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0471 - acc: 0.9964 - val_loss: 0.9778 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89604\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0468 - acc: 0.9970 - val_loss: 0.8810 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89604\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0451 - acc: 0.9976 - val_loss: 0.9020 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89604\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0437 - acc: 0.9976 - val_loss: 0.9085 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89604\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0505 - acc: 0.9943 - val_loss: 0.9054 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89604\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0510 - acc: 0.9946 - val_loss: 0.8683 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89604\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0467 - acc: 0.9970 - val_loss: 0.9094 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89604\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0440 - acc: 0.9982 - val_loss: 1.1507 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89604\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0431 - acc: 0.9988 - val_loss: 0.9952 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.89604\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0554 - acc: 0.9922 - val_loss: 1.1745 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.89604\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0506 - acc: 0.9964 - val_loss: 0.9533 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.89604\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0527 - acc: 0.9928 - val_loss: 0.9096 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.89604\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0564 - acc: 0.9928 - val_loss: 0.9203 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89604\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0692 - acc: 0.9898 - val_loss: 0.7907 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89604\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0504 - acc: 0.9958 - val_loss: 1.0357 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89604\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0492 - acc: 0.9958 - val_loss: 0.7630 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89604\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0472 - acc: 0.9982 - val_loss: 0.7991 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.89604\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0427 - acc: 0.9988 - val_loss: 0.9526 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.89604\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0391 - acc: 1.0000 - val_loss: 0.9757 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.89604\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0387 - acc: 1.0000 - val_loss: 0.9811 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89604\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0386 - acc: 1.0000 - val_loss: 0.9801 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89604\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0387 - acc: 1.0000 - val_loss: 0.9819 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89604\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0385 - acc: 1.0000 - val_loss: 1.0234 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89604\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0374 - acc: 1.0000 - val_loss: 1.0300 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89604\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0373 - acc: 1.0000 - val_loss: 1.0369 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89604\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0371 - acc: 1.0000 - val_loss: 1.0818 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89604\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0367 - acc: 1.0000 - val_loss: 1.0748 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.89604\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0369 - acc: 1.0000 - val_loss: 1.0852 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.89604\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0364 - acc: 1.0000 - val_loss: 1.1148 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.89604\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0363 - acc: 1.0000 - val_loss: 1.1167 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.89604\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0362 - acc: 1.0000 - val_loss: 1.1070 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.89604\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0361 - acc: 1.0000 - val_loss: 1.1097 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.89604\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0359 - acc: 1.0000 - val_loss: 1.1541 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.89604\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0357 - acc: 1.0000 - val_loss: 1.1487 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.89604\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0360 - acc: 1.0000 - val_loss: 1.1545 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.89604\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0444 - acc: 0.9988 - val_loss: 1.0041 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.89604\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0412 - acc: 0.9970 - val_loss: 1.1361 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.89604\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0399 - acc: 0.9988 - val_loss: 1.0741 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.89604\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1985 - acc: 0.9618 - val_loss: 0.6989 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.89604\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1167 - acc: 0.9699 - val_loss: 0.6287 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.89604\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0590 - acc: 0.9916 - val_loss: 0.7381 - val_acc: 0.8861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00121: val_acc did not improve from 0.89604\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0450 - acc: 0.9982 - val_loss: 0.7912 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.89604\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0444 - acc: 0.9988 - val_loss: 0.8072 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.89604\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0414 - acc: 0.9988 - val_loss: 0.8233 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.89604\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0423 - acc: 0.9982 - val_loss: 0.8701 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.89604\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0448 - acc: 0.9976 - val_loss: 0.9180 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.89604\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0440 - acc: 0.9982 - val_loss: 0.8478 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.89604\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0408 - acc: 0.9994 - val_loss: 0.9189 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.89604\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0374 - acc: 1.0000 - val_loss: 0.9282 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.89604\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0364 - acc: 1.0000 - val_loss: 0.9849 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.89604\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0360 - acc: 1.0000 - val_loss: 0.9689 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.89604\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0356 - acc: 1.0000 - val_loss: 0.9747 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.89604\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0356 - acc: 1.0000 - val_loss: 0.9619 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.89604\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0352 - acc: 1.0000 - val_loss: 1.1142 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.89604\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0362 - acc: 0.9994 - val_loss: 1.0123 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.89604\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0389 - acc: 0.9994 - val_loss: 0.9772 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.89604\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0537 - acc: 0.9916 - val_loss: 0.8863 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.89604\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0601 - acc: 0.9934 - val_loss: 0.8134 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.89604\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0405 - acc: 0.9994 - val_loss: 0.8702 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.89604\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0443 - acc: 0.9976 - val_loss: 1.0165 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.89604\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0384 - acc: 0.9994 - val_loss: 0.9345 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.89604\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0358 - acc: 1.0000 - val_loss: 1.0746 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.89604\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0350 - acc: 1.0000 - val_loss: 1.0377 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.89604\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0349 - acc: 1.0000 - val_loss: 1.0694 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.89604\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0347 - acc: 1.0000 - val_loss: 1.1046 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.89604\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0345 - acc: 1.0000 - val_loss: 1.0904 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.89604\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0345 - acc: 1.0000 - val_loss: 1.1648 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.89604\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0344 - acc: 1.0000 - val_loss: 1.1421 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.89604\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0341 - acc: 1.0000 - val_loss: 1.1428 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.89604\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0339 - acc: 1.0000 - val_loss: 1.1605 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.89604\n",
      "experiment: dropout=0.2 regularization0.001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 1.3196 - acc: 0.7375 - val_loss: 1.0387 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.9913 - acc: 0.7586 - val_loss: 0.9034 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76485, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.8817 - acc: 0.7595 - val_loss: 0.8348 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76485 to 0.77723, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.8152 - acc: 0.7696 - val_loss: 0.7913 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.77723 to 0.77723, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7757 - acc: 0.7757 - val_loss: 0.7733 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77723 to 0.77970, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7572 - acc: 0.8027 - val_loss: 0.7261 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.77970 to 0.80446, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7322 - acc: 0.8019 - val_loss: 0.6959 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.80446 to 0.80941, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7028 - acc: 0.8088 - val_loss: 0.6981 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80941\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6746 - acc: 0.8211 - val_loss: 0.6637 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.80941 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6543 - acc: 0.8265 - val_loss: 0.6679 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.81683 to 0.81931, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6536 - acc: 0.8262 - val_loss: 0.6496 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81931 to 0.83416, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6334 - acc: 0.8349 - val_loss: 0.6394 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.83416\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6417 - acc: 0.8143 - val_loss: 0.6504 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.83416 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6153 - acc: 0.8428 - val_loss: 0.6442 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.84158\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5943 - acc: 0.8469 - val_loss: 0.6263 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.84158\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5895 - acc: 0.8535 - val_loss: 0.6247 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.84158 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5723 - acc: 0.8563 - val_loss: 0.6399 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.84406 to 0.84653, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5589 - acc: 0.8611 - val_loss: 0.6260 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.84653 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5401 - acc: 0.8696 - val_loss: 0.6193 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84901\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5308 - acc: 0.8770 - val_loss: 0.6315 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.84901\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5460 - acc: 0.8743 - val_loss: 0.6072 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.84901 to 0.86386, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5419 - acc: 0.8611 - val_loss: 0.5938 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86386\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4985 - acc: 0.8851 - val_loss: 0.6261 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86386\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4948 - acc: 0.8858 - val_loss: 0.7011 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.86386\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4949 - acc: 0.8864 - val_loss: 0.6208 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86386\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4757 - acc: 0.8990 - val_loss: 0.6177 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.86386 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4515 - acc: 0.9011 - val_loss: 0.6112 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.86634 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4380 - acc: 0.9140 - val_loss: 0.5817 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87624\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4874 - acc: 0.8848 - val_loss: 0.5830 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87624\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4452 - acc: 0.9143 - val_loss: 0.6047 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87624\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4416 - acc: 0.9089 - val_loss: 0.5832 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87624\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4057 - acc: 0.9230 - val_loss: 0.6570 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87624\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4095 - acc: 0.9212 - val_loss: 0.6236 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87624\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4137 - acc: 0.9141 - val_loss: 0.6121 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87624\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3853 - acc: 0.9312 - val_loss: 0.6133 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.87624 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3879 - acc: 0.9290 - val_loss: 0.6762 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87624\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3862 - acc: 0.9254 - val_loss: 0.5967 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87624\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3909 - acc: 0.9236 - val_loss: 0.6123 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.87624 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3585 - acc: 0.9410 - val_loss: 0.6928 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.88119 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3362 - acc: 0.9489 - val_loss: 0.7458 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89356\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3382 - acc: 0.9404 - val_loss: 0.6283 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89356\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3252 - acc: 0.9519 - val_loss: 0.6906 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89356\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3120 - acc: 0.9558 - val_loss: 0.7400 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89356\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3618 - acc: 0.9372 - val_loss: 0.6653 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89356\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3045 - acc: 0.9609 - val_loss: 0.6625 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89356\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3197 - acc: 0.9573 - val_loss: 0.6408 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89356\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2994 - acc: 0.9567 - val_loss: 0.6354 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89356\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2935 - acc: 0.9681 - val_loss: 0.6707 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89356\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2781 - acc: 0.9702 - val_loss: 0.6976 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89356\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2631 - acc: 0.9769 - val_loss: 0.6849 - val_acc: 0.8911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89356\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2546 - acc: 0.9783 - val_loss: 0.7230 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89356\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2664 - acc: 0.9721 - val_loss: 0.6953 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89356\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2736 - acc: 0.9717 - val_loss: 0.7255 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89356\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2568 - acc: 0.9765 - val_loss: 0.7440 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89356\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2566 - acc: 0.9757 - val_loss: 0.8455 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89356\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2596 - acc: 0.9735 - val_loss: 0.6503 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.89356 to 0.90099, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.001_second.h5\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2726 - acc: 0.9729 - val_loss: 0.6729 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.90099\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2580 - acc: 0.9783 - val_loss: 0.7178 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.90099\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2804 - acc: 0.9630 - val_loss: 0.7606 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.90099\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3027 - acc: 0.9531 - val_loss: 0.6217 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90099\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2804 - acc: 0.9649 - val_loss: 0.6611 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90099\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2353 - acc: 0.9859 - val_loss: 0.8208 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90099\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2420 - acc: 0.9777 - val_loss: 0.7781 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90099\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2433 - acc: 0.9777 - val_loss: 0.8772 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90099\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2449 - acc: 0.9814 - val_loss: 0.7573 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90099\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2270 - acc: 0.9874 - val_loss: 0.8459 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90099\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2569 - acc: 0.9745 - val_loss: 0.6918 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90099\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2413 - acc: 0.9789 - val_loss: 0.7656 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.90099\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2292 - acc: 0.9850 - val_loss: 0.7667 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90099\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2241 - acc: 0.9874 - val_loss: 0.8180 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90099\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2233 - acc: 0.9877 - val_loss: 0.8899 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.90099\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2168 - acc: 0.9898 - val_loss: 0.8098 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.90099\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2115 - acc: 0.9904 - val_loss: 0.8507 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90099\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2071 - acc: 0.9940 - val_loss: 0.8676 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.90099\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2214 - acc: 0.9892 - val_loss: 0.9574 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.90099\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2254 - acc: 0.9838 - val_loss: 0.9413 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.90099\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2533 - acc: 0.9699 - val_loss: 1.0491 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.90099\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2267 - acc: 0.9850 - val_loss: 0.8101 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.90099\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2365 - acc: 0.9820 - val_loss: 1.0098 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.90099\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2331 - acc: 0.9823 - val_loss: 0.9333 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90099\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2235 - acc: 0.9829 - val_loss: 0.8758 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90099\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2359 - acc: 0.9820 - val_loss: 0.7673 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90099\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2043 - acc: 0.9940 - val_loss: 0.8777 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90099\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2079 - acc: 0.9916 - val_loss: 0.8467 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90099\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2039 - acc: 0.9958 - val_loss: 0.9438 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90099\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2013 - acc: 0.9946 - val_loss: 0.8709 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90099\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2207 - acc: 0.9850 - val_loss: 1.0681 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90099\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2345 - acc: 0.9826 - val_loss: 0.8219 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90099\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2045 - acc: 0.9934 - val_loss: 0.9524 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90099\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2436 - acc: 0.9793 - val_loss: 0.9311 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90099\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2141 - acc: 0.9880 - val_loss: 0.8211 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90099\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2039 - acc: 0.9940 - val_loss: 0.8438 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90099\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1956 - acc: 0.9946 - val_loss: 0.8426 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90099\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1957 - acc: 0.9940 - val_loss: 0.8367 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90099\n",
      "Epoch 95/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.1973 - acc: 0.9940 - val_loss: 0.8850 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90099\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2009 - acc: 0.9916 - val_loss: 1.0736 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90099\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2260 - acc: 0.9814 - val_loss: 0.9854 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90099\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2474 - acc: 0.9729 - val_loss: 0.8116 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90099\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2158 - acc: 0.9892 - val_loss: 0.8673 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90099\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1927 - acc: 0.9964 - val_loss: 0.8857 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90099\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1878 - acc: 0.9970 - val_loss: 0.8973 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.90099\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1857 - acc: 0.9976 - val_loss: 0.9360 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.90099\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1906 - acc: 0.9958 - val_loss: 0.9421 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.90099\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1932 - acc: 0.9964 - val_loss: 0.9804 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.90099\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1866 - acc: 0.9976 - val_loss: 1.0113 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.90099\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1869 - acc: 0.9976 - val_loss: 0.9230 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.90099\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1827 - acc: 0.9994 - val_loss: 0.9120 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.90099\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1890 - acc: 0.9919 - val_loss: 0.8983 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.90099\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1897 - acc: 0.9934 - val_loss: 0.9133 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.90099\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2094 - acc: 0.9856 - val_loss: 1.1250 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.90099\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2082 - acc: 0.9868 - val_loss: 0.7961 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.90099\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2147 - acc: 0.9862 - val_loss: 0.8080 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.90099\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2303 - acc: 0.9753 - val_loss: 0.7444 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.90099\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1981 - acc: 0.9922 - val_loss: 0.8257 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.90099\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1912 - acc: 0.9940 - val_loss: 0.8595 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.90099\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1945 - acc: 0.9922 - val_loss: 0.8251 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.90099\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1853 - acc: 0.9952 - val_loss: 0.8991 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90099\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1764 - acc: 1.0000 - val_loss: 0.9057 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90099\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1747 - acc: 0.9988 - val_loss: 0.8978 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90099\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1755 - acc: 1.0000 - val_loss: 0.9078 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90099\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1791 - acc: 0.9970 - val_loss: 0.8808 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90099\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2142 - acc: 0.9838 - val_loss: 0.8270 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90099\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2011 - acc: 0.9880 - val_loss: 0.8108 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.90099\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1788 - acc: 0.9982 - val_loss: 0.8216 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90099\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1798 - acc: 0.9952 - val_loss: 0.8171 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90099\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1886 - acc: 0.9958 - val_loss: 0.8904 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90099\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1844 - acc: 0.9964 - val_loss: 0.8557 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90099\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1751 - acc: 0.9976 - val_loss: 0.8588 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.90099\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1814 - acc: 0.9958 - val_loss: 1.0541 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90099\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1751 - acc: 0.9976 - val_loss: 0.8045 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90099\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1724 - acc: 0.9994 - val_loss: 0.8694 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90099\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1725 - acc: 0.9982 - val_loss: 0.8659 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90099\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1777 - acc: 0.9946 - val_loss: 0.8997 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90099\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1882 - acc: 0.9928 - val_loss: 0.8626 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90099\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1773 - acc: 0.9952 - val_loss: 0.7855 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90099\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1747 - acc: 0.9988 - val_loss: 0.8843 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90099\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1808 - acc: 0.9958 - val_loss: 0.9032 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90099\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1782 - acc: 0.9952 - val_loss: 0.8805 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90099\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1730 - acc: 0.9982 - val_loss: 0.8943 - val_acc: 0.8861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90099\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1764 - acc: 0.9952 - val_loss: 0.9143 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90099\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1729 - acc: 0.9976 - val_loss: 0.9235 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90099\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1763 - acc: 0.9970 - val_loss: 0.9153 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90099\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1882 - acc: 0.9883 - val_loss: 1.0195 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90099\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2054 - acc: 0.9850 - val_loss: 0.8355 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90099\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1881 - acc: 0.9934 - val_loss: 0.8119 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90099\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1732 - acc: 0.9970 - val_loss: 0.8323 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90099\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1682 - acc: 0.9988 - val_loss: 0.8758 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90099\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1672 - acc: 0.9994 - val_loss: 1.0318 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90099\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1705 - acc: 0.9970 - val_loss: 1.0314 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90099\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1786 - acc: 0.9964 - val_loss: 0.9549 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90099\n",
      "experiment: dropout=0.2 regularization0.0005\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.0907 - acc: 0.7297 - val_loss: 1.0241 - val_acc: 0.5050\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50495, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.8755 - acc: 0.7374 - val_loss: 0.8152 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.50495 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7913 - acc: 0.7601 - val_loss: 0.7475 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7421 - acc: 0.7660 - val_loss: 0.7038 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76238 to 0.76733, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6946 - acc: 0.7655 - val_loss: 0.6816 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.76733 to 0.77475, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6846 - acc: 0.7910 - val_loss: 0.6869 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.77475 to 0.77970, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6385 - acc: 0.8027 - val_loss: 0.6194 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.77970 to 0.80446, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6381 - acc: 0.7986 - val_loss: 0.6155 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80446\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6201 - acc: 0.7934 - val_loss: 0.6359 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.80446 to 0.80693, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6065 - acc: 0.8031 - val_loss: 0.5945 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.80693 to 0.82673, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5960 - acc: 0.8139 - val_loss: 0.6459 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.82673\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5724 - acc: 0.8317 - val_loss: 0.5705 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82673\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5593 - acc: 0.8256 - val_loss: 0.5844 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82673\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5394 - acc: 0.8353 - val_loss: 0.5651 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82673\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5496 - acc: 0.8404 - val_loss: 0.5801 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.82673\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5422 - acc: 0.8334 - val_loss: 0.6064 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.82673\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5062 - acc: 0.8523 - val_loss: 0.5423 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.82673 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 29s 1s/step - loss: 0.4955 - acc: 0.8539 - val_loss: 0.6159 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83663\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5083 - acc: 0.8461 - val_loss: 0.5366 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.83663 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4640 - acc: 0.8686 - val_loss: 0.6111 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85644\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5046 - acc: 0.8485 - val_loss: 0.5635 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85644\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4406 - acc: 0.8755 - val_loss: 0.5365 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.85644 to 0.86139, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4199 - acc: 0.8939 - val_loss: 0.5346 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.86139 to 0.88366, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4485 - acc: 0.8768 - val_loss: 0.5473 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88366\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4095 - acc: 0.8944 - val_loss: 0.5658 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.88366\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4067 - acc: 0.8923 - val_loss: 0.5337 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.88366\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3897 - acc: 0.9056 - val_loss: 0.5514 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.88366\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3547 - acc: 0.9218 - val_loss: 0.5534 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.88366\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3749 - acc: 0.9155 - val_loss: 0.5471 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88366\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3737 - acc: 0.9013 - val_loss: 0.5616 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88366\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3900 - acc: 0.9035 - val_loss: 0.6301 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88366\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3230 - acc: 0.9308 - val_loss: 0.6291 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88366\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3076 - acc: 0.9384 - val_loss: 0.6639 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88366\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2924 - acc: 0.9429 - val_loss: 0.6924 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88366\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3127 - acc: 0.9338 - val_loss: 0.5143 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88366\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2843 - acc: 0.9459 - val_loss: 0.5454 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88366\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2699 - acc: 0.9480 - val_loss: 0.7613 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88366\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2697 - acc: 0.9519 - val_loss: 0.6143 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88366\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2677 - acc: 0.9597 - val_loss: 0.6087 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88366\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2596 - acc: 0.9549 - val_loss: 0.7083 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88366\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2576 - acc: 0.9570 - val_loss: 0.6135 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.88366 to 0.89109, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2329 - acc: 0.9663 - val_loss: 0.7039 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89109\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2835 - acc: 0.9411 - val_loss: 0.5710 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89109\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2444 - acc: 0.9600 - val_loss: 0.6122 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89109\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2191 - acc: 0.9699 - val_loss: 0.8334 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89109\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2373 - acc: 0.9634 - val_loss: 0.5501 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89109\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2092 - acc: 0.9777 - val_loss: 0.5897 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00047: val_acc improved from 0.89109 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2115 - acc: 0.9705 - val_loss: 0.5730 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89356\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2180 - acc: 0.9690 - val_loss: 0.6968 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89356\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2173 - acc: 0.9736 - val_loss: 0.6054 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89356\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1870 - acc: 0.9862 - val_loss: 0.6884 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89356\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1865 - acc: 0.9850 - val_loss: 0.7591 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89356\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1853 - acc: 0.9832 - val_loss: 0.6120 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89356\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1883 - acc: 0.9823 - val_loss: 0.5993 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.89356 to 0.90099, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1787 - acc: 0.9811 - val_loss: 0.7278 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.90099\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1716 - acc: 0.9865 - val_loss: 0.7131 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.90099\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1646 - acc: 0.9922 - val_loss: 0.7035 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.90099 to 0.90099, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1843 - acc: 0.9826 - val_loss: 0.6481 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.90099\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1850 - acc: 0.9838 - val_loss: 0.6891 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.90099\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1611 - acc: 0.9922 - val_loss: 0.6565 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90099\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1742 - acc: 0.9826 - val_loss: 0.7921 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90099\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1558 - acc: 0.9910 - val_loss: 0.8131 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90099\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1578 - acc: 0.9892 - val_loss: 0.7954 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90099\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1509 - acc: 0.9934 - val_loss: 0.6637 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90099\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1696 - acc: 0.9868 - val_loss: 0.7516 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90099\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1854 - acc: 0.9783 - val_loss: 0.7214 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90099\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1741 - acc: 0.9795 - val_loss: 0.6739 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90099\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1491 - acc: 0.9952 - val_loss: 0.8617 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.90099\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1582 - acc: 0.9898 - val_loss: 0.6835 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90099\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1628 - acc: 0.9874 - val_loss: 0.7880 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90099\n",
      "Epoch 71/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 31s 1s/step - loss: 0.1446 - acc: 0.9952 - val_loss: 0.8446 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.90099\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1441 - acc: 0.9934 - val_loss: 0.7421 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00072: val_acc improved from 0.90099 to 0.90594, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1499 - acc: 0.9922 - val_loss: 0.7206 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90594\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1595 - acc: 0.9883 - val_loss: 0.6783 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.90594\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1645 - acc: 0.9874 - val_loss: 0.6525 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.90594\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1523 - acc: 0.9898 - val_loss: 0.7512 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.90594\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1455 - acc: 0.9910 - val_loss: 0.7565 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.90594\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1364 - acc: 0.9976 - val_loss: 0.7467 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.90594\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1382 - acc: 0.9958 - val_loss: 0.7845 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.90594\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1526 - acc: 0.9928 - val_loss: 0.9666 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90594\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1686 - acc: 0.9844 - val_loss: 0.7942 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90594\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1826 - acc: 0.9781 - val_loss: 0.7646 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90594\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1432 - acc: 0.9964 - val_loss: 0.6790 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90594\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1352 - acc: 0.9994 - val_loss: 0.8630 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90594\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1301 - acc: 0.9982 - val_loss: 0.7889 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90594\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1503 - acc: 0.9898 - val_loss: 0.8178 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90594\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1417 - acc: 0.9928 - val_loss: 0.7921 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90594\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1343 - acc: 0.9964 - val_loss: 0.8872 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90594\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1416 - acc: 0.9934 - val_loss: 0.8270 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90594\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1362 - acc: 0.9934 - val_loss: 0.7413 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90594\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1290 - acc: 0.9964 - val_loss: 0.7528 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00091: val_acc improved from 0.90594 to 0.90594, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0005_second.h5\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1289 - acc: 0.9976 - val_loss: 0.8052 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90594\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1250 - acc: 0.9994 - val_loss: 0.7911 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90594\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1225 - acc: 1.0000 - val_loss: 0.7452 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90594\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1224 - acc: 1.0000 - val_loss: 0.8293 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90594\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1210 - acc: 1.0000 - val_loss: 0.7445 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90594\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1213 - acc: 1.0000 - val_loss: 0.7360 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90594\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1200 - acc: 0.9988 - val_loss: 0.7575 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90594\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1187 - acc: 1.0000 - val_loss: 0.9271 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90594\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1184 - acc: 0.9994 - val_loss: 0.8422 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90594\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1177 - acc: 1.0000 - val_loss: 0.7940 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.90594\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1168 - acc: 1.0000 - val_loss: 0.8433 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.90594\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1166 - acc: 1.0000 - val_loss: 0.8498 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.90594\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1203 - acc: 0.9982 - val_loss: 0.9415 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.90594\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1208 - acc: 0.9994 - val_loss: 0.8404 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.90594\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1239 - acc: 0.9976 - val_loss: 0.7611 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.90594\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1173 - acc: 0.9994 - val_loss: 0.8117 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.90594\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1160 - acc: 0.9994 - val_loss: 0.8559 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.90594\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1160 - acc: 0.9988 - val_loss: 0.8765 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.90594\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1144 - acc: 1.0000 - val_loss: 0.8337 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.90594\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1143 - acc: 1.0000 - val_loss: 0.9714 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.90594\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1121 - acc: 1.0000 - val_loss: 0.9284 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.90594\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1117 - acc: 1.0000 - val_loss: 0.8974 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.90594\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1111 - acc: 1.0000 - val_loss: 0.8943 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.90594\n",
      "Epoch 115/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.1105 - acc: 1.0000 - val_loss: 0.8894 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.90594\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1100 - acc: 1.0000 - val_loss: 0.9054 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.90594\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1095 - acc: 1.0000 - val_loss: 0.8938 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90594\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1090 - acc: 1.0000 - val_loss: 0.9295 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90594\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1085 - acc: 1.0000 - val_loss: 0.9374 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90594\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1079 - acc: 1.0000 - val_loss: 0.9031 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90594\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1074 - acc: 1.0000 - val_loss: 0.9216 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90594\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1069 - acc: 1.0000 - val_loss: 0.9117 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90594\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1065 - acc: 1.0000 - val_loss: 0.9506 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.90594\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1060 - acc: 1.0000 - val_loss: 0.9624 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90594\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1057 - acc: 1.0000 - val_loss: 0.9293 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90594\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1073 - acc: 0.9994 - val_loss: 0.8201 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90594\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1252 - acc: 0.9952 - val_loss: 1.0395 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90594\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2359 - acc: 0.9567 - val_loss: 0.8298 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.90594\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1493 - acc: 0.9874 - val_loss: 0.6772 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90594\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1212 - acc: 0.9964 - val_loss: 0.7342 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90594\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1244 - acc: 0.9916 - val_loss: 0.6464 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90594\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1224 - acc: 0.9946 - val_loss: 0.6944 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90594\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1285 - acc: 0.9928 - val_loss: 0.6454 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90594\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1304 - acc: 0.9922 - val_loss: 0.7878 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90594\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1207 - acc: 0.9952 - val_loss: 0.6973 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90594\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1570 - acc: 0.9832 - val_loss: 0.8588 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90594\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1358 - acc: 0.9886 - val_loss: 0.6672 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90594\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1126 - acc: 0.9988 - val_loss: 0.6416 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90594\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1238 - acc: 0.9928 - val_loss: 0.6574 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90594\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1252 - acc: 0.9952 - val_loss: 0.6842 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90594\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1199 - acc: 0.9958 - val_loss: 0.8567 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90594\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1098 - acc: 0.9994 - val_loss: 0.7200 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90594\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1100 - acc: 0.9982 - val_loss: 0.7016 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90594\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1175 - acc: 0.9946 - val_loss: 0.7324 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90594\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1087 - acc: 0.9982 - val_loss: 0.6927 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90594\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1059 - acc: 0.9994 - val_loss: 0.6962 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90594\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1079 - acc: 0.9988 - val_loss: 0.8037 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90594\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1057 - acc: 0.9994 - val_loss: 0.8511 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90594\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1199 - acc: 0.9934 - val_loss: 0.7077 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90594\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1550 - acc: 0.9801 - val_loss: 0.7974 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90594\n",
      "experiment: dropout=0.2 regularization0.0001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7567 - acc: 0.7228 - val_loss: 0.6362 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6101 - acc: 0.7445 - val_loss: 0.5746 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5693 - acc: 0.7654 - val_loss: 0.5473 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5385 - acc: 0.7637 - val_loss: 0.5362 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5239 - acc: 0.7610 - val_loss: 0.5065 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76238\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4987 - acc: 0.7696 - val_loss: 0.4994 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76238 to 0.77475, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4773 - acc: 0.7868 - val_loss: 0.4877 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.77475 to 0.80198, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5055 - acc: 0.7851 - val_loss: 0.4820 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.80198 to 0.81436, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4754 - acc: 0.7959 - val_loss: 0.4714 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.81436 to 0.81931, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4530 - acc: 0.8160 - val_loss: 0.4540 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81931\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.4367 - acc: 0.8322 - val_loss: 0.4506 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81931 to 0.82673, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4242 - acc: 0.8388 - val_loss: 0.4480 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.82673 to 0.83168, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4223 - acc: 0.8458 - val_loss: 0.4365 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.83168 to 0.84653, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4610 - acc: 0.8154 - val_loss: 0.4479 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.84653\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4008 - acc: 0.8493 - val_loss: 0.4694 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.84653\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3927 - acc: 0.8583 - val_loss: 0.4340 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.84653\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3992 - acc: 0.8517 - val_loss: 0.4347 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84653\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3753 - acc: 0.8665 - val_loss: 0.4310 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.84653 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3765 - acc: 0.8526 - val_loss: 0.4085 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.85149 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3602 - acc: 0.8836 - val_loss: 0.4166 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85396\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3499 - acc: 0.8777 - val_loss: 0.4387 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85396\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2998 - acc: 0.9059 - val_loss: 0.5167 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.85396\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3053 - acc: 0.8977 - val_loss: 0.3919 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.85396 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2870 - acc: 0.9077 - val_loss: 0.4768 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87129\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2862 - acc: 0.9068 - val_loss: 0.3947 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87129\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2725 - acc: 0.9149 - val_loss: 0.4326 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87129\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2488 - acc: 0.9251 - val_loss: 0.5093 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87129\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2521 - acc: 0.9174 - val_loss: 0.4541 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87129\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2617 - acc: 0.9234 - val_loss: 0.5761 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87129\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2359 - acc: 0.9368 - val_loss: 0.4794 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87129\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1999 - acc: 0.9396 - val_loss: 0.4979 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.87129 to 0.87871, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2100 - acc: 0.9426 - val_loss: 0.4161 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.87871 to 0.88614, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.2013 - acc: 0.9432 - val_loss: 0.4745 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88614\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1976 - acc: 0.9447 - val_loss: 0.4323 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88614\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1702 - acc: 0.9507 - val_loss: 0.4374 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.88614 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1868 - acc: 0.9468 - val_loss: 0.5186 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.89356\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1959 - acc: 0.9408 - val_loss: 0.4139 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.89356\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1668 - acc: 0.9537 - val_loss: 0.5103 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.89356\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1531 - acc: 0.9570 - val_loss: 0.4507 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.89356 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1378 - acc: 0.9666 - val_loss: 0.5919 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89356\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1276 - acc: 0.9699 - val_loss: 0.6002 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89356\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2023 - acc: 0.9360 - val_loss: 0.6225 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89356\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1517 - acc: 0.9600 - val_loss: 0.5811 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89356\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1488 - acc: 0.9594 - val_loss: 0.4471 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.89356 to 0.90842, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.2_0.0001_second.h5\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1207 - acc: 0.9727 - val_loss: 0.4501 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.90842\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1215 - acc: 0.9747 - val_loss: 0.4853 - val_acc: 0.8911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00046: val_acc did not improve from 0.90842\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1077 - acc: 0.9769 - val_loss: 0.5962 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.90842\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1030 - acc: 0.9801 - val_loss: 0.6690 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.90842\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.1318 - acc: 0.9711 - val_loss: 0.4647 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.90842\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1048 - acc: 0.9811 - val_loss: 0.5644 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.90842\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0923 - acc: 0.9850 - val_loss: 0.6699 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.90842\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1093 - acc: 0.9777 - val_loss: 0.5739 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.90842\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1086 - acc: 0.9765 - val_loss: 0.5688 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.90842\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1068 - acc: 0.9777 - val_loss: 0.5753 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.90842\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1222 - acc: 0.9696 - val_loss: 0.7832 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.90842\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1190 - acc: 0.9717 - val_loss: 0.6092 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.90842\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0935 - acc: 0.9850 - val_loss: 0.5796 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.90842\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1153 - acc: 0.9760 - val_loss: 0.6646 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.90842\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1836 - acc: 0.9432 - val_loss: 0.9993 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.90842\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1701 - acc: 0.9441 - val_loss: 0.4680 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90842\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0939 - acc: 0.9844 - val_loss: 0.7324 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90842\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0783 - acc: 0.9880 - val_loss: 0.6307 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90842\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0844 - acc: 0.9886 - val_loss: 0.8572 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90842\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0862 - acc: 0.9838 - val_loss: 0.6935 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90842\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0697 - acc: 0.9916 - val_loss: 0.6247 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90842\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0724 - acc: 0.9898 - val_loss: 0.6349 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90842\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0995 - acc: 0.9820 - val_loss: 0.6446 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90842\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0812 - acc: 0.9856 - val_loss: 0.5654 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.90842\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0741 - acc: 0.9886 - val_loss: 0.6017 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90842\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0734 - acc: 0.9910 - val_loss: 0.7312 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90842\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0663 - acc: 0.9916 - val_loss: 0.6426 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.90842\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0755 - acc: 0.9850 - val_loss: 0.6493 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.90842\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0638 - acc: 0.9946 - val_loss: 0.7021 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90842\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0612 - acc: 0.9952 - val_loss: 0.7180 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.90842\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0597 - acc: 0.9940 - val_loss: 0.7682 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.90842\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0555 - acc: 0.9958 - val_loss: 0.7690 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.90842\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0602 - acc: 0.9934 - val_loss: 0.7784 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.90842\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0604 - acc: 0.9925 - val_loss: 0.8959 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.90842\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0936 - acc: 0.9811 - val_loss: 0.6023 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.90842\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0740 - acc: 0.9892 - val_loss: 0.7579 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90842\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0738 - acc: 0.9913 - val_loss: 0.5944 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90842\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0786 - acc: 0.9880 - val_loss: 0.5659 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90842\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0660 - acc: 0.9928 - val_loss: 0.6569 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90842\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0704 - acc: 0.9904 - val_loss: 0.6539 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90842\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0587 - acc: 0.9946 - val_loss: 0.6059 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90842\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0588 - acc: 0.9928 - val_loss: 0.5972 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90842\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0578 - acc: 0.9928 - val_loss: 0.8045 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90842\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0589 - acc: 0.9922 - val_loss: 0.8049 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90842\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1109 - acc: 0.9757 - val_loss: 0.5945 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90842\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0917 - acc: 0.9841 - val_loss: 0.6505 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90842\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0629 - acc: 0.9904 - val_loss: 0.6788 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90842\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0639 - acc: 0.9928 - val_loss: 0.6604 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90842\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0591 - acc: 0.9916 - val_loss: 0.7704 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90842\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0588 - acc: 0.9928 - val_loss: 0.8072 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90842\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0560 - acc: 0.9934 - val_loss: 0.7863 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90842\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0529 - acc: 0.9940 - val_loss: 0.7671 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90842\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0517 - acc: 0.9952 - val_loss: 0.9674 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90842\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0480 - acc: 0.9982 - val_loss: 0.8664 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90842\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0735 - acc: 0.9874 - val_loss: 0.7976 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90842\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0950 - acc: 0.9807 - val_loss: 0.7795 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90842\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0689 - acc: 0.9910 - val_loss: 0.6800 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.90842\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0609 - acc: 0.9904 - val_loss: 0.7154 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.90842\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0497 - acc: 0.9958 - val_loss: 0.8704 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.90842\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0598 - acc: 0.9916 - val_loss: 0.6714 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.90842\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0529 - acc: 0.9934 - val_loss: 0.7155 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.90842\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0476 - acc: 0.9982 - val_loss: 0.7018 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.90842\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0462 - acc: 0.9970 - val_loss: 0.7207 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.90842\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0435 - acc: 0.9988 - val_loss: 0.7718 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.90842\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0464 - acc: 0.9952 - val_loss: 0.8704 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.90842\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0427 - acc: 0.9988 - val_loss: 0.8449 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.90842\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0437 - acc: 0.9988 - val_loss: 0.7750 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.90842\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0397 - acc: 1.0000 - val_loss: 0.8549 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.90842\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0453 - acc: 0.9976 - val_loss: 0.8453 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.90842\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0406 - acc: 1.0000 - val_loss: 0.8274 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.90842\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0417 - acc: 0.9994 - val_loss: 0.9928 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.90842\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0676 - acc: 0.9934 - val_loss: 1.1556 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.90842\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1487 - acc: 0.9621 - val_loss: 0.6089 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90842\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0693 - acc: 0.9874 - val_loss: 0.6718 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90842\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0506 - acc: 0.9958 - val_loss: 0.8903 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90842\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0527 - acc: 0.9952 - val_loss: 0.7019 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90842\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0457 - acc: 0.9982 - val_loss: 0.7774 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90842\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0450 - acc: 0.9976 - val_loss: 0.8493 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90842\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0413 - acc: 0.9988 - val_loss: 0.8185 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.90842\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0396 - acc: 0.9994 - val_loss: 0.8366 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90842\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0390 - acc: 0.9994 - val_loss: 0.9577 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90842\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0407 - acc: 0.9982 - val_loss: 0.8623 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90842\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0711 - acc: 0.9922 - val_loss: 0.7423 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90842\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0431 - acc: 0.9982 - val_loss: 0.7864 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.90842\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0458 - acc: 0.9976 - val_loss: 0.8524 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90842\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0405 - acc: 0.9994 - val_loss: 0.9741 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90842\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0471 - acc: 0.9970 - val_loss: 0.9167 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90842\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0555 - acc: 0.9946 - val_loss: 0.9723 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90842\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0567 - acc: 0.9940 - val_loss: 0.6447 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90842\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0509 - acc: 0.9934 - val_loss: 0.7160 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90842\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0481 - acc: 0.9964 - val_loss: 0.7856 - val_acc: 0.8936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90842\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0450 - acc: 0.9982 - val_loss: 0.7800 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90842\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.8610 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90842\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0381 - acc: 1.0000 - val_loss: 0.8609 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90842\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0502 - acc: 0.9940 - val_loss: 0.9081 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90842\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.0415 - acc: 0.9982 - val_loss: 0.8255 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90842\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0381 - acc: 1.0000 - val_loss: 0.8664 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90842\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0431 - acc: 0.9970 - val_loss: 0.7889 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90842\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0405 - acc: 0.9988 - val_loss: 0.8245 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90842\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0388 - acc: 0.9988 - val_loss: 0.8500 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90842\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0460 - acc: 0.9976 - val_loss: 0.8828 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90842\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0591 - acc: 0.9922 - val_loss: 0.7265 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90842\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0408 - acc: 0.9976 - val_loss: 0.7722 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90842\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0377 - acc: 1.0000 - val_loss: 0.7729 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90842\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0366 - acc: 0.9994 - val_loss: 0.8082 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90842\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0363 - acc: 1.0000 - val_loss: 0.8293 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90842\n",
      "experiment: dropout=0.3 regularization0.001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.3810 - acc: 0.7427 - val_loss: 1.1104 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 1.0842 - acc: 0.7595 - val_loss: 0.9936 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.9575 - acc: 0.7633 - val_loss: 0.9256 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.8897 - acc: 0.7633 - val_loss: 0.8677 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76238\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.8557 - acc: 0.7639 - val_loss: 0.8213 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.76238 to 0.76485, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.8065 - acc: 0.7730 - val_loss: 0.7792 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76485 to 0.78713, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7739 - acc: 0.7883 - val_loss: 0.7492 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.78713 to 0.80941, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7335 - acc: 0.8034 - val_loss: 0.7260 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80941\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7082 - acc: 0.8133 - val_loss: 0.7290 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.80941 to 0.82426, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6957 - acc: 0.8202 - val_loss: 0.6866 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82426\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6717 - acc: 0.8329 - val_loss: 0.6648 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.82426 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6474 - acc: 0.8421 - val_loss: 0.6665 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.83663\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6592 - acc: 0.8379 - val_loss: 0.6479 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.83663 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6500 - acc: 0.8391 - val_loss: 0.6467 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.84158 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6167 - acc: 0.8452 - val_loss: 0.6179 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.84158\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5994 - acc: 0.8592 - val_loss: 0.6218 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.84158\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5936 - acc: 0.8575 - val_loss: 0.6885 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84158\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5868 - acc: 0.8650 - val_loss: 0.6081 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.84158 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5715 - acc: 0.8698 - val_loss: 0.6444 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84406\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5495 - acc: 0.8746 - val_loss: 0.6191 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.84406 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5394 - acc: 0.8746 - val_loss: 0.5882 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.85644 to 0.87376, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5343 - acc: 0.8903 - val_loss: 0.5860 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87376\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5044 - acc: 0.8957 - val_loss: 0.5960 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87376\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4824 - acc: 0.9086 - val_loss: 0.6044 - val_acc: 0.8465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87376\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4750 - acc: 0.9095 - val_loss: 0.6080 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87376\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4606 - acc: 0.9110 - val_loss: 0.5852 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.87376 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4887 - acc: 0.8951 - val_loss: 0.5912 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87624\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4555 - acc: 0.9137 - val_loss: 0.5972 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87624\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4329 - acc: 0.9291 - val_loss: 0.6016 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87624\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4102 - acc: 0.9332 - val_loss: 0.6157 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87624\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4066 - acc: 0.9368 - val_loss: 0.5992 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87624\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3718 - acc: 0.9428 - val_loss: 0.6252 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87624\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3987 - acc: 0.9356 - val_loss: 0.6237 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.87624 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3632 - acc: 0.9486 - val_loss: 0.7290 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88119\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3660 - acc: 0.9462 - val_loss: 0.6471 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88119\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3738 - acc: 0.9442 - val_loss: 0.6606 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88119\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3749 - acc: 0.9459 - val_loss: 0.6126 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88119\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3483 - acc: 0.9529 - val_loss: 0.7867 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88119\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4025 - acc: 0.9236 - val_loss: 0.5649 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88119\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3484 - acc: 0.9537 - val_loss: 0.6804 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88119\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3265 - acc: 0.9607 - val_loss: 0.6644 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88119\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3148 - acc: 0.9618 - val_loss: 0.7449 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88119\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2821 - acc: 0.9789 - val_loss: 0.7862 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88119\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2929 - acc: 0.9727 - val_loss: 0.7072 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.88119 to 0.88614, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2766 - acc: 0.9747 - val_loss: 0.9190 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88614\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2941 - acc: 0.9711 - val_loss: 0.6983 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.88614\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2857 - acc: 0.9741 - val_loss: 0.7190 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88614\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2970 - acc: 0.9735 - val_loss: 0.6966 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88614\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2841 - acc: 0.9757 - val_loss: 0.7373 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88614\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2964 - acc: 0.9664 - val_loss: 1.1376 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88614\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3462 - acc: 0.9459 - val_loss: 0.7387 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.88614\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2804 - acc: 0.9723 - val_loss: 0.7041 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00052: val_acc improved from 0.88614 to 0.88861, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2920 - acc: 0.9690 - val_loss: 0.6493 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88861\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2872 - acc: 0.9759 - val_loss: 0.7145 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.88861\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2727 - acc: 0.9724 - val_loss: 0.7573 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.88861\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2567 - acc: 0.9832 - val_loss: 0.8392 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.88861\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2966 - acc: 0.9658 - val_loss: 0.7772 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.88861\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2545 - acc: 0.9781 - val_loss: 0.8765 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.88861\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2399 - acc: 0.9880 - val_loss: 0.8348 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.88861\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2984 - acc: 0.9778 - val_loss: 1.0152 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.88861\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2922 - acc: 0.9666 - val_loss: 0.6795 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.88861\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2524 - acc: 0.9850 - val_loss: 0.8786 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.88861\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2320 - acc: 0.9934 - val_loss: 0.8264 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.88861\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2354 - acc: 0.9904 - val_loss: 0.8758 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.88861\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2697 - acc: 0.9795 - val_loss: 0.7389 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.88861\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2295 - acc: 0.9940 - val_loss: 0.8594 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.88861\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2281 - acc: 0.9910 - val_loss: 0.8529 - val_acc: 0.8837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00067: val_acc did not improve from 0.88861\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2213 - acc: 0.9940 - val_loss: 0.9368 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.88861\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2148 - acc: 0.9958 - val_loss: 0.9417 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.88861\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2168 - acc: 0.9958 - val_loss: 0.8974 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.88861\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2294 - acc: 0.9910 - val_loss: 1.0297 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.88861\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3423 - acc: 0.9561 - val_loss: 0.6510 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.88861\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2492 - acc: 0.9856 - val_loss: 0.7239 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.88861\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2325 - acc: 0.9904 - val_loss: 0.7473 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.88861\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2212 - acc: 0.9946 - val_loss: 0.7959 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.88861\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2281 - acc: 0.9907 - val_loss: 0.7811 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.88861\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2194 - acc: 0.9934 - val_loss: 0.8276 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.88861\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2219 - acc: 0.9910 - val_loss: 0.7862 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.88861\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2147 - acc: 0.9922 - val_loss: 0.8216 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.88861\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2184 - acc: 0.9922 - val_loss: 0.7607 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.88861\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2294 - acc: 0.9865 - val_loss: 0.7444 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.88861\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2288 - acc: 0.9856 - val_loss: 0.8580 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.88861\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2228 - acc: 0.9886 - val_loss: 0.7596 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.88861\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2102 - acc: 0.9952 - val_loss: 0.8663 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.88861\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2044 - acc: 0.9964 - val_loss: 0.8328 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.88861\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2159 - acc: 0.9898 - val_loss: 1.1735 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.88861\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2138 - acc: 0.9928 - val_loss: 0.9771 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.88861\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2148 - acc: 0.9892 - val_loss: 0.8628 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.88861\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2167 - acc: 0.9892 - val_loss: 1.0259 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.88861\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2056 - acc: 0.9970 - val_loss: 0.9513 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.88861\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2319 - acc: 0.9871 - val_loss: 1.0108 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.88861\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2704 - acc: 0.9777 - val_loss: 0.6496 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.88861\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2085 - acc: 0.9946 - val_loss: 0.7923 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.88861\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2024 - acc: 0.9958 - val_loss: 0.8260 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.88861\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2132 - acc: 0.9886 - val_loss: 0.8939 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.88861\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2058 - acc: 0.9940 - val_loss: 0.8560 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.88861\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1990 - acc: 0.9958 - val_loss: 0.9584 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.88861\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1993 - acc: 0.9961 - val_loss: 0.9041 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.88861\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2002 - acc: 0.9958 - val_loss: 0.9634 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.88861\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1948 - acc: 0.9976 - val_loss: 0.9350 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.88861\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2037 - acc: 0.9922 - val_loss: 0.9459 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.88861\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1920 - acc: 0.9988 - val_loss: 0.9736 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.88861\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2009 - acc: 0.9952 - val_loss: 0.8084 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.88861\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1969 - acc: 0.9964 - val_loss: 0.9129 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.88861\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1920 - acc: 0.9976 - val_loss: 1.0092 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.88861\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1962 - acc: 0.9952 - val_loss: 0.9268 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.88861\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1974 - acc: 0.9964 - val_loss: 0.9079 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.88861\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2067 - acc: 0.9928 - val_loss: 0.9100 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.88861\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2972 - acc: 0.9594 - val_loss: 0.6778 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.88861\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2223 - acc: 0.9892 - val_loss: 0.7953 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.88861\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1978 - acc: 0.9976 - val_loss: 0.8511 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.88861\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1983 - acc: 0.9934 - val_loss: 0.8001 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.88861\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1875 - acc: 0.9982 - val_loss: 0.9340 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.88861\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1848 - acc: 0.9994 - val_loss: 1.0533 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.88861\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2081 - acc: 0.9904 - val_loss: 0.8133 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.88861\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1984 - acc: 0.9937 - val_loss: 0.8274 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00116: val_acc improved from 0.88861 to 0.89109, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.001_second.h5\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1904 - acc: 0.9964 - val_loss: 0.8816 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.89109\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1826 - acc: 1.0000 - val_loss: 0.8935 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.89109\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1833 - acc: 0.9994 - val_loss: 1.0396 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.89109\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2003 - acc: 0.9922 - val_loss: 0.9533 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.89109\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2127 - acc: 0.9874 - val_loss: 0.8915 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.89109\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1968 - acc: 0.9934 - val_loss: 0.8313 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.89109\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1984 - acc: 0.9946 - val_loss: 0.9281 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.89109\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2213 - acc: 0.9835 - val_loss: 0.9099 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.89109\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2564 - acc: 0.9705 - val_loss: 0.6560 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.89109\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1910 - acc: 0.9970 - val_loss: 0.7869 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.89109\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1845 - acc: 0.9982 - val_loss: 0.8233 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.89109\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1822 - acc: 0.9994 - val_loss: 0.8931 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.89109\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1790 - acc: 1.0000 - val_loss: 0.9600 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.89109\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1816 - acc: 0.9976 - val_loss: 0.9869 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.89109\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1835 - acc: 0.9970 - val_loss: 0.8780 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.89109\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1790 - acc: 0.9994 - val_loss: 0.9586 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.89109\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1866 - acc: 0.9952 - val_loss: 0.9262 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.89109\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1938 - acc: 0.9934 - val_loss: 0.9723 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.89109\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1931 - acc: 0.9928 - val_loss: 0.8548 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.89109\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1896 - acc: 0.9952 - val_loss: 0.7464 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.89109\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1779 - acc: 0.9994 - val_loss: 0.8478 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.89109\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1771 - acc: 0.9988 - val_loss: 0.8970 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.89109\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1872 - acc: 0.9970 - val_loss: 0.8844 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.89109\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1943 - acc: 0.9916 - val_loss: 0.7287 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.89109\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1960 - acc: 0.9907 - val_loss: 1.0921 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.89109\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2264 - acc: 0.9801 - val_loss: 0.7276 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.89109\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1993 - acc: 0.9928 - val_loss: 0.6899 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.89109\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1852 - acc: 0.9964 - val_loss: 0.8020 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.89109\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1837 - acc: 0.9970 - val_loss: 0.8419 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.89109\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1769 - acc: 0.9994 - val_loss: 0.7593 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.89109\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1789 - acc: 0.9988 - val_loss: 0.9263 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.89109\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1874 - acc: 0.9940 - val_loss: 0.8640 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.89109\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2003 - acc: 0.9904 - val_loss: 0.8946 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.89109\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1834 - acc: 0.9964 - val_loss: 0.9052 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.89109\n",
      "experiment: dropout=0.3 regularization0.0005\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.9782 - acc: 0.7504 - val_loss: 0.8463 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.8210 - acc: 0.7505 - val_loss: 0.7735 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7423 - acc: 0.7640 - val_loss: 0.7076 - val_acc: 0.7896\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.78960, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 4/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.6935 - acc: 0.7721 - val_loss: 0.6604 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.78960\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6563 - acc: 0.7881 - val_loss: 0.6344 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78960 to 0.79455, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6387 - acc: 0.7869 - val_loss: 0.6255 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.79455 to 0.80693, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6133 - acc: 0.8088 - val_loss: 0.5979 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.80693 to 0.81436, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5909 - acc: 0.8175 - val_loss: 0.5910 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.81436\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5804 - acc: 0.8261 - val_loss: 0.5910 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.81436 to 0.82426, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5624 - acc: 0.8286 - val_loss: 0.5721 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82426\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5641 - acc: 0.8334 - val_loss: 0.5668 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.82426 to 0.82673, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5370 - acc: 0.8385 - val_loss: 0.6321 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82673\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5396 - acc: 0.8323 - val_loss: 0.6064 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82673\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5358 - acc: 0.8340 - val_loss: 0.5691 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82673\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5099 - acc: 0.8517 - val_loss: 0.5405 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.82673 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4910 - acc: 0.8710 - val_loss: 0.5426 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.83663 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5101 - acc: 0.8436 - val_loss: 0.5627 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83911\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4983 - acc: 0.8656 - val_loss: 0.5360 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.83911 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4499 - acc: 0.8803 - val_loss: 0.5429 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84406\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4521 - acc: 0.8801 - val_loss: 0.5322 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.84406 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4357 - acc: 0.8845 - val_loss: 0.5106 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85644\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4281 - acc: 0.8852 - val_loss: 0.5156 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.85644\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4132 - acc: 0.8930 - val_loss: 0.5077 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.85644\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3798 - acc: 0.9068 - val_loss: 0.5964 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.85644 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3781 - acc: 0.9101 - val_loss: 0.6440 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86634\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4330 - acc: 0.8704 - val_loss: 0.5336 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86634\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3809 - acc: 0.9081 - val_loss: 0.5328 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.86634\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3927 - acc: 0.8956 - val_loss: 0.5462 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.86634 to 0.86881, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3302 - acc: 0.9254 - val_loss: 0.6106 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.86881\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3508 - acc: 0.9215 - val_loss: 0.5729 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.86881\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3324 - acc: 0.9224 - val_loss: 0.6606 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.86881\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3035 - acc: 0.9287 - val_loss: 0.5241 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.86881 to 0.87376, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2995 - acc: 0.9372 - val_loss: 0.5682 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87376\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2909 - acc: 0.9402 - val_loss: 0.6481 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.87376 to 0.87871, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2778 - acc: 0.9459 - val_loss: 0.6366 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87871\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2749 - acc: 0.9422 - val_loss: 0.6300 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87871\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2959 - acc: 0.9387 - val_loss: 0.5945 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87871\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2562 - acc: 0.9522 - val_loss: 0.6393 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87871\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2463 - acc: 0.9522 - val_loss: 0.6173 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87871\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2689 - acc: 0.9480 - val_loss: 0.5356 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87871\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2377 - acc: 0.9564 - val_loss: 0.6353 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.87871\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2586 - acc: 0.9498 - val_loss: 0.6585 - val_acc: 0.8515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_acc did not improve from 0.87871\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2526 - acc: 0.9546 - val_loss: 0.5858 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.87871\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2284 - acc: 0.9621 - val_loss: 0.6226 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.87871 to 0.88614, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2152 - acc: 0.9747 - val_loss: 0.6036 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.88614 to 0.90347, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0005_second.h5\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2194 - acc: 0.9714 - val_loss: 0.6261 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.90347\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2299 - acc: 0.9664 - val_loss: 0.6528 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.90347\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2400 - acc: 0.9549 - val_loss: 0.6441 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.90347\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1971 - acc: 0.9801 - val_loss: 0.6300 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.90347\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1788 - acc: 0.9838 - val_loss: 0.6762 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.90347\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1982 - acc: 0.9777 - val_loss: 0.6406 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.90347\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1797 - acc: 0.9835 - val_loss: 0.6339 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.90347\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1677 - acc: 0.9850 - val_loss: 0.6985 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.90347\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1674 - acc: 0.9904 - val_loss: 0.8872 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.90347\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1838 - acc: 0.9771 - val_loss: 0.8066 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.90347\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1834 - acc: 0.9777 - val_loss: 0.7361 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.90347\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1827 - acc: 0.9814 - val_loss: 0.7382 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.90347\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1894 - acc: 0.9763 - val_loss: 0.6910 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.90347\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1617 - acc: 0.9886 - val_loss: 0.7412 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.90347\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1627 - acc: 0.9850 - val_loss: 0.7333 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90347\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1517 - acc: 0.9934 - val_loss: 0.9041 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90347\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1701 - acc: 0.9820 - val_loss: 0.8117 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90347\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1802 - acc: 0.9811 - val_loss: 0.8545 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90347\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1962 - acc: 0.9751 - val_loss: 0.7864 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90347\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1865 - acc: 0.9747 - val_loss: 0.7196 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90347\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1543 - acc: 0.9910 - val_loss: 0.7288 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90347\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1530 - acc: 0.9886 - val_loss: 0.6937 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90347\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1715 - acc: 0.9838 - val_loss: 0.7121 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.90347\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1560 - acc: 0.9892 - val_loss: 1.0585 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90347\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1572 - acc: 0.9874 - val_loss: 0.8034 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90347\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1514 - acc: 0.9883 - val_loss: 0.8460 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.90347\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1387 - acc: 0.9958 - val_loss: 0.7987 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.90347\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1428 - acc: 0.9940 - val_loss: 0.9892 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90347\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1541 - acc: 0.9904 - val_loss: 0.9115 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.90347\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1448 - acc: 0.9928 - val_loss: 0.8312 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.90347\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1360 - acc: 0.9940 - val_loss: 0.8506 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.90347\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1341 - acc: 0.9958 - val_loss: 0.8389 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.90347\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1408 - acc: 0.9916 - val_loss: 0.8911 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.90347\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1511 - acc: 0.9907 - val_loss: 0.8279 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.90347\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1346 - acc: 0.9946 - val_loss: 0.7874 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90347\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1288 - acc: 0.9964 - val_loss: 0.7587 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90347\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1359 - acc: 0.9940 - val_loss: 0.8905 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90347\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1297 - acc: 0.9976 - val_loss: 0.8682 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90347\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1366 - acc: 0.9934 - val_loss: 0.8962 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90347\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1386 - acc: 0.9934 - val_loss: 0.7742 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90347\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1366 - acc: 0.9934 - val_loss: 0.8880 - val_acc: 0.8663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90347\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2126 - acc: 0.9663 - val_loss: 0.6451 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90347\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1529 - acc: 0.9904 - val_loss: 0.6630 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90347\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1308 - acc: 0.9970 - val_loss: 0.7589 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90347\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1383 - acc: 0.9922 - val_loss: 0.6873 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90347\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1327 - acc: 0.9952 - val_loss: 0.8698 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90347\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1366 - acc: 0.9928 - val_loss: 0.8698 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90347\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1341 - acc: 0.9958 - val_loss: 0.7686 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90347\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1562 - acc: 0.9868 - val_loss: 0.9366 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90347\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1614 - acc: 0.9807 - val_loss: 0.7544 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90347\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1367 - acc: 0.9952 - val_loss: 0.7297 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90347\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1266 - acc: 0.9970 - val_loss: 0.7732 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90347\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1409 - acc: 0.9883 - val_loss: 0.8321 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90347\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1472 - acc: 0.9874 - val_loss: 0.7277 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90347\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1450 - acc: 0.9910 - val_loss: 0.7776 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90347\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1297 - acc: 0.9964 - val_loss: 0.8165 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.90347\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1808 - acc: 0.9793 - val_loss: 0.8202 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.90347\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1551 - acc: 0.9844 - val_loss: 0.7806 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.90347\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1327 - acc: 0.9970 - val_loss: 0.7436 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.90347\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1222 - acc: 0.9988 - val_loss: 0.7795 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.90347\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1240 - acc: 0.9982 - val_loss: 0.7506 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.90347\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1231 - acc: 0.9967 - val_loss: 0.8128 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.90347\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1207 - acc: 0.9988 - val_loss: 0.7910 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.90347\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1176 - acc: 0.9994 - val_loss: 0.8084 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.90347\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1159 - acc: 1.0000 - val_loss: 0.8676 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.90347\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1201 - acc: 0.9982 - val_loss: 0.8600 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.90347\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1171 - acc: 0.9994 - val_loss: 0.8347 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.90347\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1160 - acc: 0.9994 - val_loss: 0.8293 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.90347\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1152 - acc: 1.0000 - val_loss: 0.8497 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.90347\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1171 - acc: 0.9988 - val_loss: 0.9501 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.90347\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1189 - acc: 0.9988 - val_loss: 0.9314 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.90347\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1377 - acc: 0.9904 - val_loss: 0.9165 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90347\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1265 - acc: 0.9958 - val_loss: 1.0112 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90347\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1203 - acc: 0.9964 - val_loss: 0.8153 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90347\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1149 - acc: 1.0000 - val_loss: 0.8561 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90347\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1457 - acc: 0.9910 - val_loss: 0.9032 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90347\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1385 - acc: 0.9892 - val_loss: 0.7592 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90347\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1196 - acc: 0.9982 - val_loss: 0.7685 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.90347\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1185 - acc: 0.9976 - val_loss: 0.8393 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90347\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1242 - acc: 0.9925 - val_loss: 0.8547 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90347\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1282 - acc: 0.9916 - val_loss: 0.8804 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90347\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1180 - acc: 0.9976 - val_loss: 0.9335 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90347\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1311 - acc: 0.9928 - val_loss: 0.8208 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.90347\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1394 - acc: 0.9898 - val_loss: 0.8745 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90347\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1192 - acc: 0.9970 - val_loss: 0.7933 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90347\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1162 - acc: 0.9976 - val_loss: 0.8633 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90347\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1205 - acc: 0.9964 - val_loss: 0.8612 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90347\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1163 - acc: 0.9988 - val_loss: 0.9066 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90347\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1138 - acc: 0.9982 - val_loss: 0.8982 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90347\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1109 - acc: 0.9994 - val_loss: 0.8680 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90347\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1098 - acc: 1.0000 - val_loss: 0.8760 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90347\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1087 - acc: 1.0000 - val_loss: 0.8847 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90347\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1087 - acc: 1.0000 - val_loss: 0.9115 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90347\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1113 - acc: 0.9988 - val_loss: 0.9121 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90347\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1127 - acc: 0.9982 - val_loss: 0.9636 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90347\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1106 - acc: 0.9994 - val_loss: 0.9644 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90347\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1083 - acc: 1.0000 - val_loss: 0.8853 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90347\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1083 - acc: 0.9994 - val_loss: 0.9051 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90347\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1069 - acc: 1.0000 - val_loss: 0.8828 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90347\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1068 - acc: 1.0000 - val_loss: 0.8827 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90347\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1059 - acc: 1.0000 - val_loss: 0.8987 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90347\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1056 - acc: 1.0000 - val_loss: 0.9027 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90347\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1053 - acc: 1.0000 - val_loss: 0.8990 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90347\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1061 - acc: 0.9994 - val_loss: 0.9064 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90347\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1046 - acc: 1.0000 - val_loss: 0.9373 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90347\n",
      "experiment: dropout=0.3 regularization0.0001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7475 - acc: 0.7429 - val_loss: 0.6491 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6215 - acc: 0.7625 - val_loss: 0.5781 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5743 - acc: 0.7607 - val_loss: 0.5764 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5604 - acc: 0.7648 - val_loss: 0.5328 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76238 to 0.78218, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.5343 - acc: 0.7767 - val_loss: 0.5219 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.78218\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5089 - acc: 0.7883 - val_loss: 0.4984 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.78218 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5038 - acc: 0.7961 - val_loss: 0.4817 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.79950 to 0.80693, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4724 - acc: 0.8172 - val_loss: 0.4641 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.80693 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4519 - acc: 0.8190 - val_loss: 0.4575 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.81683 to 0.82178, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4606 - acc: 0.8217 - val_loss: 0.4504 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.82178 to 0.82426, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4305 - acc: 0.8364 - val_loss: 0.4978 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.82426\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4282 - acc: 0.8347 - val_loss: 0.5086 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82426\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4330 - acc: 0.8373 - val_loss: 0.4668 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82426\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3848 - acc: 0.8629 - val_loss: 0.4483 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.82426 to 0.83663, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3843 - acc: 0.8623 - val_loss: 0.4414 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.83663 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3721 - acc: 0.8686 - val_loss: 0.4290 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.83911\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3425 - acc: 0.8777 - val_loss: 0.4227 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.83911 to 0.84901, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3353 - acc: 0.8834 - val_loss: 0.5057 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.84901\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3217 - acc: 0.8914 - val_loss: 0.4669 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84901\n",
      "Epoch 20/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 31s 1s/step - loss: 0.3102 - acc: 0.8894 - val_loss: 0.4992 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.84901\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3045 - acc: 0.8981 - val_loss: 0.4307 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.84901\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2868 - acc: 0.9053 - val_loss: 0.4782 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.84901 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2696 - acc: 0.9083 - val_loss: 0.4516 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86634\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2793 - acc: 0.9020 - val_loss: 0.4548 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.86634\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2612 - acc: 0.9149 - val_loss: 0.4103 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86634\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2309 - acc: 0.9269 - val_loss: 0.4565 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86634\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.3205 - acc: 0.8903 - val_loss: 0.4056 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.86634\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2428 - acc: 0.9293 - val_loss: 0.4526 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.86634 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2081 - acc: 0.9366 - val_loss: 0.4732 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87624\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1945 - acc: 0.9414 - val_loss: 0.5318 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87624\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1781 - acc: 0.9510 - val_loss: 0.4603 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.87624 to 0.88614, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1778 - acc: 0.9450 - val_loss: 0.5068 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88614\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1754 - acc: 0.9537 - val_loss: 0.5490 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88614\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1565 - acc: 0.9564 - val_loss: 0.4898 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88614\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1497 - acc: 0.9555 - val_loss: 0.5001 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88614\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1343 - acc: 0.9669 - val_loss: 0.5222 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.88614 to 0.89604, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1724 - acc: 0.9549 - val_loss: 0.5626 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.89604\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1305 - acc: 0.9652 - val_loss: 0.7107 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.89604\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1301 - acc: 0.9666 - val_loss: 0.5794 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.89604\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1120 - acc: 0.9763 - val_loss: 0.6310 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89604\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1510 - acc: 0.9652 - val_loss: 0.6606 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89604\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1363 - acc: 0.9669 - val_loss: 0.5865 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89604\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1277 - acc: 0.9739 - val_loss: 0.5167 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89604\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1155 - acc: 0.9759 - val_loss: 0.5916 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89604\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0908 - acc: 0.9801 - val_loss: 0.5818 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89604\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0865 - acc: 0.9850 - val_loss: 0.5788 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89604\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0849 - acc: 0.9868 - val_loss: 0.5957 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89604\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0801 - acc: 0.9874 - val_loss: 0.6165 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89604\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0937 - acc: 0.9795 - val_loss: 0.6007 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.89604 to 0.90594, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0781 - acc: 0.9898 - val_loss: 0.6688 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00050: val_acc improved from 0.90594 to 0.90842, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0942 - acc: 0.9820 - val_loss: 0.5991 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.90842\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0853 - acc: 0.9847 - val_loss: 0.6585 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.90842\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0733 - acc: 0.9898 - val_loss: 0.6544 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.90842\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0709 - acc: 0.9922 - val_loss: 0.6657 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.90842\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0639 - acc: 0.9940 - val_loss: 0.6256 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.90842\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0613 - acc: 0.9952 - val_loss: 0.6591 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.90842\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0770 - acc: 0.9874 - val_loss: 0.6695 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.90842\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0782 - acc: 0.9862 - val_loss: 0.6904 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.90842\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0967 - acc: 0.9795 - val_loss: 0.6208 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.90842\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0744 - acc: 0.9904 - val_loss: 0.6861 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90842\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0750 - acc: 0.9886 - val_loss: 0.5994 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90842\n",
      "Epoch 62/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.0595 - acc: 0.9970 - val_loss: 0.6281 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90842\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0570 - acc: 0.9970 - val_loss: 0.6385 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.90842 to 0.91089, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0560 - acc: 0.9970 - val_loss: 0.6577 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.91089\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0597 - acc: 0.9952 - val_loss: 0.6835 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.91089\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0744 - acc: 0.9868 - val_loss: 0.7428 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.91089\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0992 - acc: 0.9823 - val_loss: 0.6423 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.91089\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0615 - acc: 0.9946 - val_loss: 0.7437 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.91089\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0611 - acc: 0.9958 - val_loss: 0.7263 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.91089\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0590 - acc: 0.9958 - val_loss: 0.7055 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.91089\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0599 - acc: 0.9958 - val_loss: 0.7367 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.91089\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0531 - acc: 0.9976 - val_loss: 0.7598 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.91089\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0545 - acc: 0.9982 - val_loss: 0.8942 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.91089\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0514 - acc: 0.9982 - val_loss: 0.7810 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.91089\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0502 - acc: 0.9982 - val_loss: 0.7887 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.91089\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0461 - acc: 1.0000 - val_loss: 0.7923 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.91089\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0449 - acc: 1.0000 - val_loss: 0.8126 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.91089\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0457 - acc: 1.0000 - val_loss: 0.8419 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.91089\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0489 - acc: 0.9970 - val_loss: 0.8542 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.91089\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0465 - acc: 0.9994 - val_loss: 0.7886 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.91089\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0496 - acc: 0.9964 - val_loss: 0.8792 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.91089\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0562 - acc: 0.9946 - val_loss: 0.7836 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.91089\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0502 - acc: 0.9970 - val_loss: 0.9510 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.91089\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0725 - acc: 0.9922 - val_loss: 0.7610 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.91089\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0559 - acc: 0.9952 - val_loss: 0.8388 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.91089\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0489 - acc: 0.9982 - val_loss: 0.8817 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.91089\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0486 - acc: 0.9982 - val_loss: 0.7674 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.91089\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0936 - acc: 0.9880 - val_loss: 0.6610 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.91089\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0907 - acc: 0.9835 - val_loss: 0.6392 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.91089\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0568 - acc: 0.9964 - val_loss: 0.7361 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.91089\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0499 - acc: 0.9970 - val_loss: 0.7850 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91089\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0459 - acc: 0.9994 - val_loss: 0.7298 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91089\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0455 - acc: 0.9994 - val_loss: 0.7380 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.91089\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0437 - acc: 1.0000 - val_loss: 0.7808 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.91089\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0431 - acc: 1.0000 - val_loss: 0.7280 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.91089\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0422 - acc: 1.0000 - val_loss: 0.7649 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91089\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0421 - acc: 1.0000 - val_loss: 0.7670 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.91089\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0427 - acc: 1.0000 - val_loss: 0.7564 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.91089\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0422 - acc: 1.0000 - val_loss: 0.7465 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91089\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0419 - acc: 1.0000 - val_loss: 0.7676 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.91089\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0420 - acc: 1.0000 - val_loss: 0.7493 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.91089\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0419 - acc: 1.0000 - val_loss: 0.7895 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.91089\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0412 - acc: 1.0000 - val_loss: 0.7770 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00103: val_acc improved from 0.91089 to 0.91337, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0410 - acc: 1.0000 - val_loss: 0.7707 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.91337\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0408 - acc: 1.0000 - val_loss: 0.7760 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.91337\n",
      "Epoch 106/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.0406 - acc: 1.0000 - val_loss: 0.7980 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.91337\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0404 - acc: 1.0000 - val_loss: 0.7883 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00107: val_acc improved from 0.91337 to 0.91584, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.3_0.0001_second.h5\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0403 - acc: 1.0000 - val_loss: 0.8085 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.91584\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0404 - acc: 1.0000 - val_loss: 0.7939 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.91584\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0400 - acc: 1.0000 - val_loss: 0.8064 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.91584\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0406 - acc: 1.0000 - val_loss: 0.8232 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.91584\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0398 - acc: 1.0000 - val_loss: 0.7938 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.91584\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0397 - acc: 1.0000 - val_loss: 0.7934 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.91584\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0395 - acc: 1.0000 - val_loss: 0.8334 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.91584\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.8186 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.91584\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0391 - acc: 1.0000 - val_loss: 0.8174 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.91584\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0390 - acc: 1.0000 - val_loss: 0.8191 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.91584\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0388 - acc: 1.0000 - val_loss: 0.8171 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.91584\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0387 - acc: 1.0000 - val_loss: 0.8121 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.91584\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0386 - acc: 1.0000 - val_loss: 0.8101 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.91584\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0385 - acc: 1.0000 - val_loss: 0.8476 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.91584\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0384 - acc: 1.0000 - val_loss: 0.8257 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.91584\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0382 - acc: 1.0000 - val_loss: 0.8240 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.91584\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0381 - acc: 1.0000 - val_loss: 0.8376 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.91584\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0380 - acc: 1.0000 - val_loss: 0.8359 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.91584\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0378 - acc: 1.0000 - val_loss: 0.8319 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.91584\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0377 - acc: 1.0000 - val_loss: 0.8239 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.91584\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0376 - acc: 1.0000 - val_loss: 0.8385 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.91584\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0374 - acc: 1.0000 - val_loss: 0.8308 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.91584\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0373 - acc: 1.0000 - val_loss: 0.8465 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.91584\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0372 - acc: 1.0000 - val_loss: 0.8451 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.91584\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0370 - acc: 1.0000 - val_loss: 0.8428 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.91584\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0369 - acc: 1.0000 - val_loss: 0.8417 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.91584\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0368 - acc: 1.0000 - val_loss: 0.8474 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.91584\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0366 - acc: 1.0000 - val_loss: 0.8479 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.91584\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0365 - acc: 1.0000 - val_loss: 0.8496 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.91584\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0364 - acc: 1.0000 - val_loss: 0.8451 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.91584\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0363 - acc: 1.0000 - val_loss: 0.8443 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.91584\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0361 - acc: 1.0000 - val_loss: 0.8482 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.91584\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0360 - acc: 1.0000 - val_loss: 0.8500 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.91584\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0359 - acc: 1.0000 - val_loss: 0.8503 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.91584\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0357 - acc: 1.0000 - val_loss: 0.8517 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.91584\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0356 - acc: 1.0000 - val_loss: 0.8459 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.91584\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0355 - acc: 1.0000 - val_loss: 0.8465 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.91584\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0354 - acc: 1.0000 - val_loss: 0.8656 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.91584\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0352 - acc: 1.0000 - val_loss: 0.8785 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.91584\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0351 - acc: 1.0000 - val_loss: 0.8528 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.91584\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0350 - acc: 1.0000 - val_loss: 0.8628 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.91584\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0349 - acc: 1.0000 - val_loss: 0.8597 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.91584\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0347 - acc: 1.0000 - val_loss: 0.8596 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.91584\n",
      "experiment: dropout=0.4 regularization0.001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.3263 - acc: 0.7522 - val_loss: 1.0864 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.0466 - acc: 0.7492 - val_loss: 1.0358 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.9284 - acc: 0.7615 - val_loss: 0.9363 - val_acc: 0.7327\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.8628 - acc: 0.7676 - val_loss: 0.8161 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76238 to 0.78465, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.8232 - acc: 0.7817 - val_loss: 0.7866 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.78465 to 0.78713, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7696 - acc: 0.8009 - val_loss: 0.7613 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.78713 to 0.80198, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7551 - acc: 0.7934 - val_loss: 0.7509 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.80198 to 0.80446, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7463 - acc: 0.7908 - val_loss: 0.7238 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.80446 to 0.80941, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6990 - acc: 0.8172 - val_loss: 0.7139 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80941\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7033 - acc: 0.8031 - val_loss: 0.6946 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.80941 to 0.81436, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6790 - acc: 0.8268 - val_loss: 0.6660 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81436 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6767 - acc: 0.8239 - val_loss: 0.6637 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.83911\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6675 - acc: 0.8347 - val_loss: 0.6590 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.83911\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6582 - acc: 0.8215 - val_loss: 0.6394 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.83911 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6504 - acc: 0.8268 - val_loss: 0.6366 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85149\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6238 - acc: 0.8452 - val_loss: 0.6386 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.85149\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5931 - acc: 0.8547 - val_loss: 0.6373 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85149\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5946 - acc: 0.8575 - val_loss: 0.6202 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85149\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5930 - acc: 0.8424 - val_loss: 0.6283 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85149\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5879 - acc: 0.8581 - val_loss: 0.6523 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85149\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5897 - acc: 0.8535 - val_loss: 0.6382 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85149\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5397 - acc: 0.8818 - val_loss: 0.6285 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.85149 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5386 - acc: 0.8761 - val_loss: 0.5947 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.85396 to 0.86139, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5142 - acc: 0.8908 - val_loss: 0.5923 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.86139 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5349 - acc: 0.8861 - val_loss: 0.6275 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.86634\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5246 - acc: 0.8888 - val_loss: 0.5936 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86634\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4816 - acc: 0.9017 - val_loss: 0.6308 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.86634\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5156 - acc: 0.8867 - val_loss: 0.5809 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.86634 to 0.86881, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4651 - acc: 0.9104 - val_loss: 0.6170 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.86881\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4308 - acc: 0.9197 - val_loss: 0.6127 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.86881\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4185 - acc: 0.9230 - val_loss: 0.6176 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.86881\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4225 - acc: 0.9218 - val_loss: 0.6830 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.86881\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4637 - acc: 0.9128 - val_loss: 0.6616 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.86881\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4258 - acc: 0.9131 - val_loss: 0.5827 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.86881 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3915 - acc: 0.9435 - val_loss: 0.5692 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.89356\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3788 - acc: 0.9393 - val_loss: 0.5898 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.89356\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3653 - acc: 0.9489 - val_loss: 0.5903 - val_acc: 0.8762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: val_acc did not improve from 0.89356\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3890 - acc: 0.9366 - val_loss: 0.8209 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.89356\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4885 - acc: 0.8927 - val_loss: 0.6009 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.89356\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3702 - acc: 0.9444 - val_loss: 0.6140 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89356\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3605 - acc: 0.9456 - val_loss: 0.6411 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89356\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3613 - acc: 0.9402 - val_loss: 0.6509 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89356\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3433 - acc: 0.9597 - val_loss: 0.6125 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89356\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3532 - acc: 0.9471 - val_loss: 0.6283 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89356\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3347 - acc: 0.9531 - val_loss: 0.7224 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89356\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3139 - acc: 0.9681 - val_loss: 0.6574 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89356\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3053 - acc: 0.9663 - val_loss: 0.6901 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89356\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3148 - acc: 0.9609 - val_loss: 0.5910 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.89356 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2856 - acc: 0.9747 - val_loss: 0.7456 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89851\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2922 - acc: 0.9699 - val_loss: 0.6889 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89851\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2959 - acc: 0.9741 - val_loss: 0.6360 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89851\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2771 - acc: 0.9765 - val_loss: 0.6962 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89851\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2791 - acc: 0.9777 - val_loss: 0.8091 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89851\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2761 - acc: 0.9735 - val_loss: 0.7051 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89851\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3026 - acc: 0.9687 - val_loss: 0.7227 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89851\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2861 - acc: 0.9666 - val_loss: 0.6553 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89851\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2717 - acc: 0.9699 - val_loss: 0.7046 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89851\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3001 - acc: 0.9675 - val_loss: 0.6477 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.89851\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2727 - acc: 0.9763 - val_loss: 0.6944 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.89851 to 0.90347, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2864 - acc: 0.9687 - val_loss: 0.7061 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90347\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2579 - acc: 0.9820 - val_loss: 0.7046 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90347\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2495 - acc: 0.9820 - val_loss: 0.7816 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90347\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2728 - acc: 0.9714 - val_loss: 0.8156 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90347\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2649 - acc: 0.9771 - val_loss: 0.7430 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90347\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2466 - acc: 0.9835 - val_loss: 0.7069 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90347\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2541 - acc: 0.9820 - val_loss: 0.7383 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90347\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2551 - acc: 0.9753 - val_loss: 0.6425 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90347\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2395 - acc: 0.9859 - val_loss: 0.6418 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.90347 to 0.90594, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2449 - acc: 0.9850 - val_loss: 0.6501 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00069: val_acc improved from 0.90594 to 0.91584, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.001_second.h5\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2392 - acc: 0.9874 - val_loss: 0.8405 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.91584\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2308 - acc: 0.9892 - val_loss: 0.6980 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.91584\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2279 - acc: 0.9886 - val_loss: 0.7281 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.91584\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2366 - acc: 0.9850 - val_loss: 0.8443 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.91584\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2406 - acc: 0.9844 - val_loss: 0.8348 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.91584\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2225 - acc: 0.9910 - val_loss: 0.8010 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.91584\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2159 - acc: 0.9946 - val_loss: 0.8755 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.91584\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2280 - acc: 0.9910 - val_loss: 0.7904 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.91584\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2620 - acc: 0.9759 - val_loss: 0.6894 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.91584\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2391 - acc: 0.9838 - val_loss: 0.7219 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.91584\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2340 - acc: 0.9862 - val_loss: 0.6986 - val_acc: 0.8936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00080: val_acc did not improve from 0.91584\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2189 - acc: 0.9922 - val_loss: 0.7503 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.91584\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2170 - acc: 0.9934 - val_loss: 0.7581 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.91584\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2158 - acc: 0.9916 - val_loss: 0.7415 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.91584\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2158 - acc: 0.9910 - val_loss: 0.7809 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.91584\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2417 - acc: 0.9820 - val_loss: 0.8276 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.91584\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2275 - acc: 0.9880 - val_loss: 0.7887 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.91584\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2298 - acc: 0.9847 - val_loss: 0.7025 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.91584\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2142 - acc: 0.9928 - val_loss: 0.7623 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.91584\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2075 - acc: 0.9946 - val_loss: 0.8185 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.91584\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2379 - acc: 0.9826 - val_loss: 0.7379 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.91584\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2420 - acc: 0.9844 - val_loss: 0.7848 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91584\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2275 - acc: 0.9850 - val_loss: 0.7524 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91584\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2213 - acc: 0.9910 - val_loss: 0.8984 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.91584\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2183 - acc: 0.9910 - val_loss: 0.7180 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.91584\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2117 - acc: 0.9922 - val_loss: 0.7509 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.91584\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2106 - acc: 0.9946 - val_loss: 0.7275 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91584\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2015 - acc: 0.9964 - val_loss: 0.7602 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.91584\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1991 - acc: 0.9964 - val_loss: 0.7872 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.91584\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1948 - acc: 0.9976 - val_loss: 0.8260 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91584\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1987 - acc: 0.9958 - val_loss: 0.7983 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.91584\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2042 - acc: 0.9922 - val_loss: 0.8780 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.91584\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2041 - acc: 0.9940 - val_loss: 0.7955 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.91584\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2266 - acc: 0.9874 - val_loss: 0.7341 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.91584\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2124 - acc: 0.9928 - val_loss: 0.8035 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.91584\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2014 - acc: 0.9940 - val_loss: 0.8497 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.91584\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1948 - acc: 0.9958 - val_loss: 0.7999 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.91584\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1886 - acc: 0.9994 - val_loss: 0.8298 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.91584\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1904 - acc: 0.9970 - val_loss: 0.8410 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.91584\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2048 - acc: 0.9916 - val_loss: 0.8219 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.91584\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1980 - acc: 0.9922 - val_loss: 0.9393 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.91584\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2045 - acc: 0.9916 - val_loss: 0.8002 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.91584\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2027 - acc: 0.9916 - val_loss: 0.7409 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.91584\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2008 - acc: 0.9940 - val_loss: 0.8269 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.91584\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1978 - acc: 0.9904 - val_loss: 0.7977 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.91584\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1972 - acc: 0.9934 - val_loss: 0.7776 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.91584\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2183 - acc: 0.9898 - val_loss: 1.0080 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.91584\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2295 - acc: 0.9807 - val_loss: 0.7023 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.91584\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2013 - acc: 0.9934 - val_loss: 0.7511 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.91584\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1952 - acc: 0.9934 - val_loss: 0.8685 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.91584\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1946 - acc: 0.9937 - val_loss: 0.7762 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.91584\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1937 - acc: 0.9955 - val_loss: 0.7703 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.91584\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1856 - acc: 0.9976 - val_loss: 0.8068 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.91584\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2214 - acc: 0.9859 - val_loss: 0.8084 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.91584\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1998 - acc: 0.9904 - val_loss: 0.8228 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.91584\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1972 - acc: 0.9958 - val_loss: 0.8422 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.91584\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1871 - acc: 0.9964 - val_loss: 0.8070 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.91584\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1944 - acc: 0.9922 - val_loss: 0.8671 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.91584\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1930 - acc: 0.9964 - val_loss: 0.8220 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.91584\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2027 - acc: 0.9922 - val_loss: 0.9547 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.91584\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1934 - acc: 0.9958 - val_loss: 0.8515 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.91584\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1852 - acc: 0.9964 - val_loss: 0.8032 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.91584\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1818 - acc: 0.9976 - val_loss: 0.8044 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.91584\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1937 - acc: 0.9928 - val_loss: 0.9208 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.91584\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2269 - acc: 0.9820 - val_loss: 0.9526 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.91584\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2039 - acc: 0.9892 - val_loss: 0.8192 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.91584\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2005 - acc: 0.9910 - val_loss: 0.8171 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.91584\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2073 - acc: 0.9868 - val_loss: 0.7384 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.91584\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1868 - acc: 0.9946 - val_loss: 0.7381 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.91584\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1907 - acc: 0.9949 - val_loss: 0.7035 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.91584\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1884 - acc: 0.9964 - val_loss: 0.7319 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.91584\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1918 - acc: 0.9961 - val_loss: 0.7710 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.91584\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1921 - acc: 0.9940 - val_loss: 0.7048 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.91584\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1821 - acc: 0.9970 - val_loss: 0.8150 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.91584\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1781 - acc: 0.9982 - val_loss: 0.8155 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.91584\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1762 - acc: 0.9988 - val_loss: 0.8299 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.91584\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1761 - acc: 0.9982 - val_loss: 0.7949 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.91584\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1753 - acc: 0.9994 - val_loss: 0.8340 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.91584\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1748 - acc: 0.9982 - val_loss: 0.9089 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.91584\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1755 - acc: 0.9988 - val_loss: 0.8033 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.91584\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1752 - acc: 0.9982 - val_loss: 0.8396 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.91584\n",
      "experiment: dropout=0.4 regularization0.0005\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 1.0175 - acc: 0.7534 - val_loss: 0.8686 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.8607 - acc: 0.7505 - val_loss: 0.7908 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.7651 - acc: 0.7639 - val_loss: 0.7828 - val_acc: 0.6980\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7311 - acc: 0.7515 - val_loss: 0.7223 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76238 to 0.76980, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6894 - acc: 0.7619 - val_loss: 0.6855 - val_acc: 0.7673\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76980\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6569 - acc: 0.7826 - val_loss: 0.6306 - val_acc: 0.7946\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76980 to 0.79455, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6233 - acc: 0.7961 - val_loss: 0.6075 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.79455 to 0.80198, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6118 - acc: 0.7953 - val_loss: 0.6053 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.80198 to 0.80446, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6050 - acc: 0.8006 - val_loss: 0.5704 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.80446 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5838 - acc: 0.8069 - val_loss: 0.5994 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81683\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5682 - acc: 0.8208 - val_loss: 0.5774 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81683 to 0.83168, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5426 - acc: 0.8379 - val_loss: 0.5426 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.83168 to 0.83416, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5273 - acc: 0.8412 - val_loss: 0.5617 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.83416\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5080 - acc: 0.8581 - val_loss: 0.5365 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.83416 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5141 - acc: 0.8458 - val_loss: 0.5502 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.84158\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4802 - acc: 0.8674 - val_loss: 0.5146 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.84158 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4688 - acc: 0.8698 - val_loss: 0.5158 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.84406 to 0.85644, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4822 - acc: 0.8605 - val_loss: 0.5850 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85644\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4516 - acc: 0.8765 - val_loss: 0.5122 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85644\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4268 - acc: 0.8909 - val_loss: 0.5816 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.85644 to 0.86139, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3993 - acc: 0.9013 - val_loss: 0.4865 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.86139 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3950 - acc: 0.9029 - val_loss: 0.5693 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87624\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3865 - acc: 0.9005 - val_loss: 0.4818 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.87624 to 0.87871, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3678 - acc: 0.9104 - val_loss: 0.5614 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87871\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3507 - acc: 0.9131 - val_loss: 0.5007 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.87871 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3701 - acc: 0.9101 - val_loss: 0.5507 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.88119\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3158 - acc: 0.9323 - val_loss: 0.5188 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.88119\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3034 - acc: 0.9323 - val_loss: 0.5342 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.88119\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3142 - acc: 0.9303 - val_loss: 0.6949 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88119\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2968 - acc: 0.9378 - val_loss: 0.6180 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88119\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2864 - acc: 0.9344 - val_loss: 0.6038 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.88119 to 0.88861, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2873 - acc: 0.9417 - val_loss: 0.5957 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88861\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2781 - acc: 0.9441 - val_loss: 0.5350 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88861\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2583 - acc: 0.9492 - val_loss: 0.5922 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88861\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3142 - acc: 0.9252 - val_loss: 0.6389 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88861\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2769 - acc: 0.9429 - val_loss: 0.5276 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.88861 to 0.89109, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2469 - acc: 0.9531 - val_loss: 0.5683 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.89109 to 0.89604, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2185 - acc: 0.9717 - val_loss: 0.5766 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.89604\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2234 - acc: 0.9666 - val_loss: 0.6652 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.89604\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2145 - acc: 0.9666 - val_loss: 0.6563 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89604\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2113 - acc: 0.9693 - val_loss: 0.6075 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89604\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2260 - acc: 0.9612 - val_loss: 0.6324 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89604\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2257 - acc: 0.9624 - val_loss: 0.6204 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89604\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2312 - acc: 0.9546 - val_loss: 0.5952 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89604\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2033 - acc: 0.9735 - val_loss: 0.7300 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89604\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1949 - acc: 0.9757 - val_loss: 0.6509 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.89604 to 0.91584, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1996 - acc: 0.9771 - val_loss: 0.9646 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.91584\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2362 - acc: 0.9651 - val_loss: 0.6001 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.91584\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2057 - acc: 0.9702 - val_loss: 0.5767 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.91584\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1901 - acc: 0.9777 - val_loss: 0.6425 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.91584\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1734 - acc: 0.9844 - val_loss: 0.6405 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.91584\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1718 - acc: 0.9862 - val_loss: 0.6448 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.91584\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1661 - acc: 0.9880 - val_loss: 0.7491 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.91584\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1750 - acc: 0.9820 - val_loss: 1.4275 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.91584\n",
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.3218 - acc: 0.9381 - val_loss: 0.5169 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.91584\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1936 - acc: 0.9807 - val_loss: 0.5714 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.91584\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1720 - acc: 0.9868 - val_loss: 0.6762 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.91584\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1668 - acc: 0.9862 - val_loss: 0.6239 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.91584\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1731 - acc: 0.9856 - val_loss: 0.6623 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.91584\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1602 - acc: 0.9898 - val_loss: 0.6796 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.91584\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1564 - acc: 0.9898 - val_loss: 0.7204 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.91584\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1633 - acc: 0.9862 - val_loss: 0.8436 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.91584\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1496 - acc: 0.9928 - val_loss: 0.7306 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.91584\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1455 - acc: 0.9940 - val_loss: 0.7011 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.91584\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1553 - acc: 0.9886 - val_loss: 0.8710 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.91584\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1593 - acc: 0.9874 - val_loss: 0.6975 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.91584\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1522 - acc: 0.9904 - val_loss: 0.6788 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.91584\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1437 - acc: 0.9940 - val_loss: 0.7346 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.91584\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1362 - acc: 0.9958 - val_loss: 0.7172 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.91584\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1363 - acc: 0.9952 - val_loss: 0.7302 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.91584\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1387 - acc: 0.9958 - val_loss: 0.7773 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.91584\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1455 - acc: 0.9928 - val_loss: 0.8108 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.91584\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1419 - acc: 0.9940 - val_loss: 0.7089 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.91584\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1334 - acc: 0.9958 - val_loss: 0.7379 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.91584\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1476 - acc: 0.9910 - val_loss: 0.7376 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.91584\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1448 - acc: 0.9886 - val_loss: 0.7497 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.91584\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1322 - acc: 0.9958 - val_loss: 0.7270 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.91584\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1280 - acc: 0.9982 - val_loss: 0.8400 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.91584\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1351 - acc: 0.9952 - val_loss: 0.7878 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.91584\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1589 - acc: 0.9850 - val_loss: 0.6483 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.91584\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1490 - acc: 0.9880 - val_loss: 0.8071 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.91584\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1377 - acc: 0.9940 - val_loss: 0.8135 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.91584\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1324 - acc: 0.9958 - val_loss: 0.7755 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.91584\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1364 - acc: 0.9940 - val_loss: 0.8010 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.91584\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1592 - acc: 0.9847 - val_loss: 0.7564 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.91584\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1536 - acc: 0.9892 - val_loss: 0.7803 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.91584\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1387 - acc: 0.9946 - val_loss: 0.7028 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.91584\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1472 - acc: 0.9904 - val_loss: 0.7074 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.91584\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1482 - acc: 0.9898 - val_loss: 0.6502 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.91584\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1378 - acc: 0.9928 - val_loss: 0.6530 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.91584\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1300 - acc: 0.9958 - val_loss: 0.7059 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91584\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1235 - acc: 0.9982 - val_loss: 0.7768 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91584\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1272 - acc: 0.9976 - val_loss: 0.7159 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00093: val_acc improved from 0.91584 to 0.91832, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1241 - acc: 0.9982 - val_loss: 0.7339 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.91832\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1272 - acc: 0.9958 - val_loss: 0.7084 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.91832\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1256 - acc: 0.9970 - val_loss: 0.7483 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91832\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1242 - acc: 0.9970 - val_loss: 0.8164 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.91832\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1225 - acc: 0.9976 - val_loss: 0.8241 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.91832\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1262 - acc: 0.9964 - val_loss: 0.7885 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.91832\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1204 - acc: 0.9964 - val_loss: 0.8367 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.91832\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1161 - acc: 0.9994 - val_loss: 0.8666 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.91832\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1181 - acc: 0.9970 - val_loss: 0.8879 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.91832\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1203 - acc: 0.9970 - val_loss: 0.8238 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.91832\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1170 - acc: 0.9988 - val_loss: 0.7549 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.91832\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1183 - acc: 0.9976 - val_loss: 0.8017 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.91832\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1169 - acc: 0.9988 - val_loss: 0.9475 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.91832\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1323 - acc: 0.9952 - val_loss: 0.7842 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.91832\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1438 - acc: 0.9922 - val_loss: 0.7680 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.91832\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1424 - acc: 0.9916 - val_loss: 0.6738 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.91832\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1478 - acc: 0.9892 - val_loss: 0.9936 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.91832\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1402 - acc: 0.9898 - val_loss: 0.6923 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.91832\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1382 - acc: 0.9916 - val_loss: 0.7016 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.91832\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1295 - acc: 0.9934 - val_loss: 0.8079 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.91832\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1246 - acc: 0.9949 - val_loss: 0.8050 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.91832\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1539 - acc: 0.9832 - val_loss: 0.6816 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.91832\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1353 - acc: 0.9922 - val_loss: 0.7234 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.91832\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1220 - acc: 0.9970 - val_loss: 0.8098 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.91832\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1263 - acc: 0.9964 - val_loss: 0.7676 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.91832\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1206 - acc: 0.9964 - val_loss: 0.8409 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.91832\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1245 - acc: 0.9934 - val_loss: 0.7550 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.91832\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1220 - acc: 0.9952 - val_loss: 0.7239 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.91832\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1173 - acc: 0.9988 - val_loss: 0.8569 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.91832\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1318 - acc: 0.9952 - val_loss: 0.7637 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.91832\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1183 - acc: 0.9970 - val_loss: 0.8244 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.91832\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1365 - acc: 0.9880 - val_loss: 0.7585 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.91832\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1256 - acc: 0.9928 - val_loss: 0.7475 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.91832\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1159 - acc: 0.9982 - val_loss: 0.7979 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.91832\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1235 - acc: 0.9952 - val_loss: 0.7851 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.91832\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1232 - acc: 0.9964 - val_loss: 0.8271 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.91832\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1116 - acc: 1.0000 - val_loss: 0.8046 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.91832\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1126 - acc: 0.9988 - val_loss: 0.8402 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.91832\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1175 - acc: 0.9958 - val_loss: 0.8075 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.91832\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1211 - acc: 0.9958 - val_loss: 0.7951 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.91832\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1290 - acc: 0.9952 - val_loss: 0.9732 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.91832\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1365 - acc: 0.9916 - val_loss: 0.6914 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.91832\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1189 - acc: 0.9952 - val_loss: 0.6728 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.91832\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1150 - acc: 0.9982 - val_loss: 0.6799 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.91832\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1093 - acc: 0.9994 - val_loss: 0.7440 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.91832\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1118 - acc: 0.9982 - val_loss: 0.7721 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.91832\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1101 - acc: 0.9988 - val_loss: 0.7658 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.91832\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1068 - acc: 1.0000 - val_loss: 0.7417 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00141: val_acc improved from 0.91832 to 0.92327, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1060 - acc: 1.0000 - val_loss: 0.7397 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.92327\n",
      "Epoch 143/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 0.1062 - acc: 1.0000 - val_loss: 0.7556 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.92327\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1055 - acc: 1.0000 - val_loss: 0.7477 - val_acc: 0.9257\n",
      "\n",
      "Epoch 00144: val_acc improved from 0.92327 to 0.92574, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1050 - acc: 1.0000 - val_loss: 0.7549 - val_acc: 0.9257\n",
      "\n",
      "Epoch 00145: val_acc improved from 0.92574 to 0.92574, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0005_second.h5\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1047 - acc: 1.0000 - val_loss: 0.7589 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.92574\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1044 - acc: 1.0000 - val_loss: 0.7606 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.92574\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1039 - acc: 1.0000 - val_loss: 0.7578 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.92574\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1035 - acc: 1.0000 - val_loss: 0.7811 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.92574\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1039 - acc: 1.0000 - val_loss: 0.7808 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.92574\n",
      "experiment: dropout=0.4 regularization0.0001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7582 - acc: 0.7462 - val_loss: 0.6154 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 30s 1s/step - loss: 0.6032 - acc: 0.7654 - val_loss: 0.5769 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5746 - acc: 0.7625 - val_loss: 0.5567 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5449 - acc: 0.7751 - val_loss: 0.5310 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76238 to 0.77228, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5355 - acc: 0.7751 - val_loss: 0.5206 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77228 to 0.77475, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5139 - acc: 0.7829 - val_loss: 0.4960 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.77475 to 0.79950, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4959 - acc: 0.7992 - val_loss: 0.5038 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.79950 to 0.80941, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4712 - acc: 0.8114 - val_loss: 0.4736 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80941\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4650 - acc: 0.8177 - val_loss: 0.4703 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.80941 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4695 - acc: 0.8214 - val_loss: 0.4633 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81683\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4553 - acc: 0.8211 - val_loss: 0.4822 - val_acc: 0.8045\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.81683\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4381 - acc: 0.8268 - val_loss: 0.4429 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.81683 to 0.82673, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4283 - acc: 0.8388 - val_loss: 0.4822 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82673\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4330 - acc: 0.8325 - val_loss: 0.4449 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.82673 to 0.83911, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4085 - acc: 0.8506 - val_loss: 0.4465 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.83911 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.4238 - acc: 0.8394 - val_loss: 0.4209 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.84406\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3897 - acc: 0.8554 - val_loss: 0.4104 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.84406 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3726 - acc: 0.8740 - val_loss: 0.4306 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85149\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3905 - acc: 0.8530 - val_loss: 0.4087 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85149\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3572 - acc: 0.8695 - val_loss: 0.4091 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85149\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3336 - acc: 0.8863 - val_loss: 0.5898 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85149\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3386 - acc: 0.8813 - val_loss: 0.4341 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.85149 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3525 - acc: 0.8702 - val_loss: 0.4537 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.85396\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3517 - acc: 0.8716 - val_loss: 0.4355 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.85396\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2995 - acc: 0.8903 - val_loss: 0.4490 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.85396 to 0.86139, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2812 - acc: 0.8999 - val_loss: 0.4295 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86139\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2836 - acc: 0.9032 - val_loss: 0.6426 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.86139\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.3329 - acc: 0.8849 - val_loss: 0.4203 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.86139\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2706 - acc: 0.9254 - val_loss: 0.4328 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.86139 to 0.86386, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 30/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 31s 1s/step - loss: 0.2398 - acc: 0.9255 - val_loss: 0.4868 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.86386 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2429 - acc: 0.9252 - val_loss: 0.4520 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88119\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2361 - acc: 0.9278 - val_loss: 0.4550 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88119\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2028 - acc: 0.9453 - val_loss: 0.5085 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88119\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1859 - acc: 0.9465 - val_loss: 0.5359 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88119\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1837 - acc: 0.9423 - val_loss: 0.7511 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88119\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.2044 - acc: 0.9344 - val_loss: 0.4995 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88119\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1587 - acc: 0.9615 - val_loss: 0.4989 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.88119 to 0.88861, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1758 - acc: 0.9546 - val_loss: 0.5003 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88861\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1652 - acc: 0.9513 - val_loss: 0.5705 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88861\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1390 - acc: 0.9666 - val_loss: 0.6204 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88861\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1464 - acc: 0.9657 - val_loss: 0.4865 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88861\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1473 - acc: 0.9588 - val_loss: 0.4534 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88861\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1679 - acc: 0.9549 - val_loss: 0.4738 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88861\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1416 - acc: 0.9657 - val_loss: 0.6864 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88861\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1337 - acc: 0.9675 - val_loss: 0.4817 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.88861 to 0.90594, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1204 - acc: 0.9765 - val_loss: 0.5067 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.90594\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1221 - acc: 0.9705 - val_loss: 0.6526 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.90594\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.1085 - acc: 0.9783 - val_loss: 0.5262 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.90594\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1045 - acc: 0.9765 - val_loss: 0.6451 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.90594\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1006 - acc: 0.9795 - val_loss: 0.5371 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.90594\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0919 - acc: 0.9807 - val_loss: 0.5215 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.90594\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0996 - acc: 0.9795 - val_loss: 0.5981 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.90594\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0895 - acc: 0.9856 - val_loss: 0.5941 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.90594\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0839 - acc: 0.9850 - val_loss: 0.6237 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.90594\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0768 - acc: 0.9892 - val_loss: 0.6303 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.90594\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0805 - acc: 0.9805 - val_loss: 0.5840 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.90594\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0864 - acc: 0.9820 - val_loss: 0.6376 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.90594\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0980 - acc: 0.9805 - val_loss: 0.6445 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.90594\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.1025 - acc: 0.9781 - val_loss: 0.5182 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.90594\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0867 - acc: 0.9838 - val_loss: 0.5571 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90594\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0757 - acc: 0.9898 - val_loss: 0.5331 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90594\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0697 - acc: 0.9904 - val_loss: 0.5847 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00062: val_acc improved from 0.90594 to 0.90594, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0728 - acc: 0.9898 - val_loss: 0.5839 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90594\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0828 - acc: 0.9853 - val_loss: 0.5685 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90594\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0744 - acc: 0.9886 - val_loss: 0.5974 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00065: val_acc improved from 0.90594 to 0.90842, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0725 - acc: 0.9904 - val_loss: 0.6042 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.90842 to 0.91089, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0604 - acc: 0.9946 - val_loss: 0.6582 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.91089\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0843 - acc: 0.9823 - val_loss: 0.7528 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.91089\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0999 - acc: 0.9838 - val_loss: 0.6098 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.91089\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0780 - acc: 0.9868 - val_loss: 0.6207 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.91089\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0664 - acc: 0.9892 - val_loss: 0.6523 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.91089\n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 0.0613 - acc: 0.9946 - val_loss: 0.6326 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.91089\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0669 - acc: 0.9928 - val_loss: 0.7342 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.91089\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0765 - acc: 0.9859 - val_loss: 0.6133 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.91089\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0925 - acc: 0.9862 - val_loss: 0.5378 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.91089\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0866 - acc: 0.9793 - val_loss: 0.6678 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.91089\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0742 - acc: 0.9904 - val_loss: 0.5165 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.91089\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0691 - acc: 0.9898 - val_loss: 0.5151 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.91089\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0580 - acc: 0.9946 - val_loss: 0.5588 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.91089\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0539 - acc: 0.9964 - val_loss: 0.5982 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.91089 to 0.91089, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0550 - acc: 0.9958 - val_loss: 0.6816 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.91089\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0613 - acc: 0.9922 - val_loss: 0.5870 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00082: val_acc improved from 0.91089 to 0.91832, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0541 - acc: 0.9976 - val_loss: 0.5625 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.91832\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0483 - acc: 0.9994 - val_loss: 0.6052 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.91832\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0485 - acc: 0.9976 - val_loss: 0.6123 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.91832\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0503 - acc: 0.9964 - val_loss: 0.6315 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.91832 to 0.92079, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0477 - acc: 0.9982 - val_loss: 0.6288 - val_acc: 0.9257\n",
      "\n",
      "Epoch 00087: val_acc improved from 0.92079 to 0.92574, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.0001_second.h5\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0476 - acc: 0.9982 - val_loss: 0.6483 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.92574\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0509 - acc: 0.9952 - val_loss: 0.7023 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.92574\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0552 - acc: 0.9940 - val_loss: 0.7384 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.92574\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0507 - acc: 0.9976 - val_loss: 0.7182 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.92574\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0672 - acc: 0.9919 - val_loss: 0.8163 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.92574\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0745 - acc: 0.9874 - val_loss: 0.6201 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.92574\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0543 - acc: 0.9976 - val_loss: 0.6586 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.92574\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0581 - acc: 0.9925 - val_loss: 1.2300 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.92574\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0634 - acc: 0.9940 - val_loss: 0.8840 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.92574\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0613 - acc: 0.9934 - val_loss: 0.7261 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.92574\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0543 - acc: 0.9976 - val_loss: 0.7115 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.92574\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0505 - acc: 0.9976 - val_loss: 0.7714 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.92574\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0594 - acc: 0.9931 - val_loss: 0.8210 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.92574\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0767 - acc: 0.9904 - val_loss: 0.6873 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.92574\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0616 - acc: 0.9934 - val_loss: 0.7235 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.92574\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0574 - acc: 0.9940 - val_loss: 0.7579 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.92574\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0539 - acc: 0.9958 - val_loss: 0.6637 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.92574\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0454 - acc: 0.9976 - val_loss: 0.7293 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.92574\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0588 - acc: 0.9946 - val_loss: 0.7514 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.92574\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0471 - acc: 0.9982 - val_loss: 0.6614 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.92574\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0545 - acc: 0.9949 - val_loss: 0.6649 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.92574\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0484 - acc: 0.9970 - val_loss: 0.6609 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.92574\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0476 - acc: 0.9982 - val_loss: 0.6723 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.92574\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0432 - acc: 0.9994 - val_loss: 0.7246 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.92574\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0457 - acc: 0.9970 - val_loss: 0.9742 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.92574\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0712 - acc: 0.9904 - val_loss: 0.6714 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.92574\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0531 - acc: 0.9958 - val_loss: 0.6508 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.92574\n",
      "Epoch 115/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 32s 1s/step - loss: 0.0593 - acc: 0.9946 - val_loss: 0.7139 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.92574\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0513 - acc: 0.9952 - val_loss: 0.6874 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.92574\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0492 - acc: 0.9970 - val_loss: 0.6899 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.92574\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0461 - acc: 0.9970 - val_loss: 0.7203 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.92574\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0474 - acc: 0.9976 - val_loss: 0.6640 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.92574\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0441 - acc: 0.9994 - val_loss: 0.7805 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.92574\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0418 - acc: 0.9988 - val_loss: 0.6925 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.92574\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0413 - acc: 0.9994 - val_loss: 0.6925 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.92574\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0427 - acc: 0.9988 - val_loss: 0.7120 - val_acc: 0.9208\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.92574\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0419 - acc: 0.9982 - val_loss: 0.7097 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.92574\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0449 - acc: 0.9976 - val_loss: 0.7580 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.92574\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0450 - acc: 0.9976 - val_loss: 0.7836 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.92574\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0557 - acc: 0.9943 - val_loss: 0.6880 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.92574\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0557 - acc: 0.9952 - val_loss: 0.7543 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.92574\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0491 - acc: 0.9970 - val_loss: 0.6661 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.92574\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0422 - acc: 0.9994 - val_loss: 0.8938 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.92574\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0504 - acc: 0.9970 - val_loss: 0.7903 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.92574\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0643 - acc: 0.9883 - val_loss: 0.8576 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.92574\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0984 - acc: 0.9817 - val_loss: 0.5327 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.92574\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0575 - acc: 0.9940 - val_loss: 0.5370 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.92574\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0588 - acc: 0.9964 - val_loss: 0.7350 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.92574\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0592 - acc: 0.9970 - val_loss: 0.7846 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.92574\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0624 - acc: 0.9892 - val_loss: 0.6146 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.92574\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0526 - acc: 0.9946 - val_loss: 0.6566 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.92574\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0420 - acc: 0.9994 - val_loss: 0.6754 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.92574\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0399 - acc: 1.0000 - val_loss: 0.6878 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.92574\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0395 - acc: 1.0000 - val_loss: 0.8051 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.92574\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.7195 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.92574\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0389 - acc: 1.0000 - val_loss: 0.7093 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.92574\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0385 - acc: 1.0000 - val_loss: 0.7114 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.92574\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0389 - acc: 0.9994 - val_loss: 0.7374 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.92574\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.0403 - acc: 0.9988 - val_loss: 0.8412 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.92574\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0395 - acc: 1.0000 - val_loss: 0.7483 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.92574\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0382 - acc: 1.0000 - val_loss: 0.7669 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.92574\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.0381 - acc: 1.0000 - val_loss: 0.7706 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.92574\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0379 - acc: 1.0000 - val_loss: 0.8150 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.92574\n",
      "experiment: dropout=0.5 regularization0.001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 1.3686 - acc: 0.7420 - val_loss: 1.1115 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 1.0681 - acc: 0.7535 - val_loss: 0.9814 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.9491 - acc: 0.7627 - val_loss: 0.9143 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.77970, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.9136 - acc: 0.7622 - val_loss: 0.8554 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.77970\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.8313 - acc: 0.7871 - val_loss: 0.7988 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77970 to 0.79703, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.8048 - acc: 0.7898 - val_loss: 0.8024 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79703\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7822 - acc: 0.7929 - val_loss: 0.7526 - val_acc: 0.8144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_acc improved from 0.79703 to 0.81436, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7436 - acc: 0.8087 - val_loss: 0.7437 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.81436 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.7291 - acc: 0.8136 - val_loss: 0.7299 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81683\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7039 - acc: 0.8190 - val_loss: 0.7082 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.81683 to 0.81683, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.6852 - acc: 0.8331 - val_loss: 0.6931 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81683 to 0.82921, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6679 - acc: 0.8329 - val_loss: 0.6768 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82921\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6820 - acc: 0.8292 - val_loss: 0.6971 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.82921 to 0.83416, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.6547 - acc: 0.8428 - val_loss: 0.6580 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.83416 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6222 - acc: 0.8542 - val_loss: 0.6818 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.84158 to 0.84406, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6331 - acc: 0.8539 - val_loss: 0.6576 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.84406 to 0.85149, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.6124 - acc: 0.8605 - val_loss: 0.6868 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85149\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5796 - acc: 0.8728 - val_loss: 0.6596 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85149\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 31s 1s/step - loss: 0.5884 - acc: 0.8548 - val_loss: 0.6470 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85149\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5637 - acc: 0.8902 - val_loss: 0.6160 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.85149 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5528 - acc: 0.8803 - val_loss: 0.6907 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85891\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5530 - acc: 0.8704 - val_loss: 0.6021 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.85891\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5202 - acc: 0.8902 - val_loss: 0.5864 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.85891\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.5061 - acc: 0.9023 - val_loss: 0.6445 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.85891\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4882 - acc: 0.9110 - val_loss: 0.6085 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.85891\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4893 - acc: 0.8977 - val_loss: 0.6260 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.85891 to 0.86386, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4753 - acc: 0.9081 - val_loss: 0.6324 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.86386\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4504 - acc: 0.9218 - val_loss: 0.6122 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.86386 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4387 - acc: 0.9239 - val_loss: 0.5990 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.88119\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4168 - acc: 0.9323 - val_loss: 0.5616 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88119\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4164 - acc: 0.9324 - val_loss: 0.7571 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88119\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.4245 - acc: 0.9257 - val_loss: 0.5907 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88119\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3987 - acc: 0.9323 - val_loss: 0.6405 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88119\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3830 - acc: 0.9423 - val_loss: 0.6676 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88119\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3724 - acc: 0.9504 - val_loss: 0.6146 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88119\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.3887 - acc: 0.9432 - val_loss: 0.6287 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88119\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3865 - acc: 0.9404 - val_loss: 0.6492 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88119\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3587 - acc: 0.9444 - val_loss: 0.6342 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88119\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3530 - acc: 0.9510 - val_loss: 0.6630 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88119\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3290 - acc: 0.9627 - val_loss: 0.6812 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88119\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3225 - acc: 0.9621 - val_loss: 0.7398 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88119\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3296 - acc: 0.9609 - val_loss: 0.7158 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88119\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3604 - acc: 0.9507 - val_loss: 0.6673 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88119\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3928 - acc: 0.9428 - val_loss: 0.6765 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88119\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3282 - acc: 0.9585 - val_loss: 0.6864 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88119\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3030 - acc: 0.9708 - val_loss: 0.7373 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.88119 to 0.88119, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2859 - acc: 0.9759 - val_loss: 0.7040 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88119\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3342 - acc: 0.9538 - val_loss: 0.7052 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88119\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2866 - acc: 0.9793 - val_loss: 0.7167 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88119\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2819 - acc: 0.9777 - val_loss: 0.9838 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88119\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2847 - acc: 0.9771 - val_loss: 0.8158 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.88119\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2837 - acc: 0.9759 - val_loss: 0.7307 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.88119\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2841 - acc: 0.9777 - val_loss: 0.7518 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88119\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2866 - acc: 0.9745 - val_loss: 0.8324 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.88119\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2970 - acc: 0.9723 - val_loss: 0.7294 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.88119\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2873 - acc: 0.9763 - val_loss: 0.7212 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.88119 to 0.89109, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2986 - acc: 0.9708 - val_loss: 0.7465 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89109\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2829 - acc: 0.9789 - val_loss: 0.8901 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00058: val_acc improved from 0.89109 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2657 - acc: 0.9817 - val_loss: 0.9243 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89356\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2663 - acc: 0.9769 - val_loss: 0.8072 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.89356\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3083 - acc: 0.9615 - val_loss: 0.8131 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89356\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3065 - acc: 0.9663 - val_loss: 0.8529 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89356\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2747 - acc: 0.9771 - val_loss: 0.8096 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89356\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2702 - acc: 0.9814 - val_loss: 0.7205 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89356\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2559 - acc: 0.9862 - val_loss: 0.7439 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89356\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2654 - acc: 0.9814 - val_loss: 0.7565 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89356\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2556 - acc: 0.9826 - val_loss: 0.7423 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89356\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2498 - acc: 0.9844 - val_loss: 0.7382 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89356\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2379 - acc: 0.9910 - val_loss: 0.7857 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89356\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2443 - acc: 0.9859 - val_loss: 0.8322 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89356\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2370 - acc: 0.9910 - val_loss: 0.8288 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89356\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2344 - acc: 0.9904 - val_loss: 0.8308 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89356\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2337 - acc: 0.9898 - val_loss: 0.9057 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89356\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2596 - acc: 0.9832 - val_loss: 0.9151 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89356\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2763 - acc: 0.9693 - val_loss: 0.7265 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89356\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2453 - acc: 0.9874 - val_loss: 0.8154 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89356\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2345 - acc: 0.9910 - val_loss: 0.8482 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89356\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2292 - acc: 0.9898 - val_loss: 0.8438 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89356\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2446 - acc: 0.9850 - val_loss: 0.7685 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89356\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2399 - acc: 0.9916 - val_loss: 0.8320 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89356\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2395 - acc: 0.9874 - val_loss: 0.8103 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89356\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2932 - acc: 0.9721 - val_loss: 1.0675 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89356\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2605 - acc: 0.9783 - val_loss: 0.8191 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89356\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2356 - acc: 0.9886 - val_loss: 0.8101 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89356\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2248 - acc: 0.9928 - val_loss: 0.8359 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89356\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2247 - acc: 0.9928 - val_loss: 0.8670 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89356\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2295 - acc: 0.9910 - val_loss: 0.8364 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89356\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2261 - acc: 0.9892 - val_loss: 0.8571 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89356\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2311 - acc: 0.9874 - val_loss: 0.8258 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.89356\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2207 - acc: 0.9940 - val_loss: 0.8350 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.89356\n",
      "Epoch 91/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 33s 1s/step - loss: 0.2188 - acc: 0.9928 - val_loss: 0.8752 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.89356\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2244 - acc: 0.9898 - val_loss: 0.8038 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00092: val_acc improved from 0.89356 to 0.89604, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2090 - acc: 0.9976 - val_loss: 0.8386 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00093: val_acc improved from 0.89604 to 0.89604, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2165 - acc: 0.9919 - val_loss: 0.8445 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89604\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2106 - acc: 0.9946 - val_loss: 0.9681 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89604\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2097 - acc: 0.9952 - val_loss: 0.8747 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89604\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2196 - acc: 0.9922 - val_loss: 0.9619 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.89604\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2191 - acc: 0.9898 - val_loss: 0.9892 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.89604\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2132 - acc: 0.9931 - val_loss: 0.9124 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.89604\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2097 - acc: 0.9958 - val_loss: 0.9694 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89604\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2078 - acc: 0.9940 - val_loss: 0.9600 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.89604\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2054 - acc: 0.9958 - val_loss: 0.9690 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.89604\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2046 - acc: 0.9964 - val_loss: 0.9374 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.89604\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2123 - acc: 0.9907 - val_loss: 0.9624 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.89604\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2463 - acc: 0.9838 - val_loss: 0.7985 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.89604\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2217 - acc: 0.9892 - val_loss: 0.9228 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.89604\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2185 - acc: 0.9898 - val_loss: 0.9488 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.89604\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2346 - acc: 0.9868 - val_loss: 0.8017 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.89604\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2418 - acc: 0.9820 - val_loss: 0.9459 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.89604\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2301 - acc: 0.9859 - val_loss: 0.8605 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.89604\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2146 - acc: 0.9922 - val_loss: 0.8479 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.89604\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2090 - acc: 0.9940 - val_loss: 0.8860 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.89604\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2005 - acc: 0.9964 - val_loss: 0.8672 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.89604\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1988 - acc: 0.9982 - val_loss: 0.8972 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00114: val_acc improved from 0.89604 to 0.90347, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.001_second.h5\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2011 - acc: 0.9961 - val_loss: 0.8930 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.90347\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2012 - acc: 0.9946 - val_loss: 0.8974 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.90347\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1993 - acc: 0.9958 - val_loss: 0.9276 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90347\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2248 - acc: 0.9874 - val_loss: 0.9633 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90347\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2241 - acc: 0.9877 - val_loss: 0.8729 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90347\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2936 - acc: 0.9654 - val_loss: 0.7738 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90347\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2217 - acc: 0.9883 - val_loss: 0.8024 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90347\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2030 - acc: 0.9940 - val_loss: 0.8068 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90347\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1968 - acc: 0.9964 - val_loss: 0.8748 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.90347\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1999 - acc: 0.9952 - val_loss: 0.8345 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90347\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1947 - acc: 0.9982 - val_loss: 0.9954 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90347\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1947 - acc: 0.9970 - val_loss: 0.9536 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90347\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2100 - acc: 0.9922 - val_loss: 0.8829 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90347\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2097 - acc: 0.9910 - val_loss: 0.8834 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.90347\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2023 - acc: 0.9925 - val_loss: 0.8039 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90347\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1965 - acc: 0.9964 - val_loss: 0.8908 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90347\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1967 - acc: 0.9952 - val_loss: 0.9825 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90347\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2061 - acc: 0.9934 - val_loss: 0.9150 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90347\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2026 - acc: 0.9940 - val_loss: 0.8498 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90347\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2134 - acc: 0.9898 - val_loss: 0.9713 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90347\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1981 - acc: 0.9952 - val_loss: 0.8589 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90347\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2010 - acc: 0.9940 - val_loss: 0.9517 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90347\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1975 - acc: 0.9946 - val_loss: 0.9351 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90347\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1900 - acc: 0.9982 - val_loss: 0.9485 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90347\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1873 - acc: 0.9982 - val_loss: 1.0287 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90347\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1866 - acc: 0.9988 - val_loss: 0.9959 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90347\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1960 - acc: 0.9964 - val_loss: 0.9505 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90347\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1867 - acc: 0.9982 - val_loss: 0.9433 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90347\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1854 - acc: 0.9994 - val_loss: 0.9810 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90347\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1863 - acc: 0.9982 - val_loss: 0.9999 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90347\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1879 - acc: 0.9964 - val_loss: 0.9780 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90347\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1913 - acc: 0.9970 - val_loss: 0.8984 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90347\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2008 - acc: 0.9922 - val_loss: 0.9656 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90347\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2258 - acc: 0.9868 - val_loss: 1.2177 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90347\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2247 - acc: 0.9814 - val_loss: 1.0051 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90347\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2383 - acc: 0.9789 - val_loss: 0.8298 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90347\n",
      "experiment: dropout=0.5 regularization0.0005\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 1.0081 - acc: 0.7390 - val_loss: 0.8484 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.8331 - acc: 0.7636 - val_loss: 0.7791 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7689 - acc: 0.7566 - val_loss: 0.7341 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.76238 to 0.76485, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.7414 - acc: 0.7639 - val_loss: 0.7051 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76485\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.7105 - acc: 0.7610 - val_loss: 0.7493 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76485\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.6891 - acc: 0.7721 - val_loss: 0.6622 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76485 to 0.77970, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.6567 - acc: 0.7833 - val_loss: 0.6242 - val_acc: 0.7822\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.77970 to 0.78218, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6259 - acc: 0.7926 - val_loss: 0.6075 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.78218 to 0.81188, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6187 - acc: 0.7986 - val_loss: 0.5968 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.81188\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5889 - acc: 0.8111 - val_loss: 0.5856 - val_acc: 0.8119\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.81188\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5798 - acc: 0.8160 - val_loss: 0.5687 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81188 to 0.82673, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.5749 - acc: 0.8132 - val_loss: 0.6036 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.82673\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.5601 - acc: 0.8298 - val_loss: 0.5765 - val_acc: 0.8465\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.82673 to 0.84653, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5437 - acc: 0.8262 - val_loss: 0.5820 - val_acc: 0.8218\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.84653\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5333 - acc: 0.8301 - val_loss: 0.5305 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.84653 to 0.85396, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5104 - acc: 0.8446 - val_loss: 0.5209 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.85396 to 0.86634, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5086 - acc: 0.8484 - val_loss: 0.5417 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86634\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4981 - acc: 0.8503 - val_loss: 0.5652 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86634\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4795 - acc: 0.8566 - val_loss: 0.5673 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.86634\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4674 - acc: 0.8728 - val_loss: 0.5069 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.86634\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4593 - acc: 0.8680 - val_loss: 0.5039 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.86634\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4408 - acc: 0.8716 - val_loss: 0.5038 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86634\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4724 - acc: 0.8795 - val_loss: 0.5044 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.86634 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4304 - acc: 0.8791 - val_loss: 0.5095 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87624\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4153 - acc: 0.8936 - val_loss: 0.5340 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87624\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4068 - acc: 0.9005 - val_loss: 0.5480 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87624\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3882 - acc: 0.8966 - val_loss: 0.5311 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87624\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3651 - acc: 0.9065 - val_loss: 0.5180 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87624\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4317 - acc: 0.8811 - val_loss: 0.5545 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87624\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3857 - acc: 0.8999 - val_loss: 0.5009 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87624\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3346 - acc: 0.9293 - val_loss: 0.5521 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87624\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3427 - acc: 0.9194 - val_loss: 0.5492 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87624\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.3959 - acc: 0.8897 - val_loss: 0.5391 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87624\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3557 - acc: 0.9086 - val_loss: 0.5279 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.87624\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3320 - acc: 0.9170 - val_loss: 0.5821 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87624\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3052 - acc: 0.9342 - val_loss: 0.5291 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.87624\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2889 - acc: 0.9396 - val_loss: 0.5293 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87624\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2920 - acc: 0.9405 - val_loss: 0.5601 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87624\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2657 - acc: 0.9459 - val_loss: 0.6071 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.87624 to 0.88366, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2759 - acc: 0.9417 - val_loss: 0.6322 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88366\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2698 - acc: 0.9468 - val_loss: 0.5788 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88366\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2582 - acc: 0.9537 - val_loss: 0.7367 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88366\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2765 - acc: 0.9468 - val_loss: 0.5987 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88366\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2448 - acc: 0.9576 - val_loss: 0.6454 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.88366 to 0.88614, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2182 - acc: 0.9663 - val_loss: 0.6704 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88614\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2301 - acc: 0.9651 - val_loss: 0.8826 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.88614\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2751 - acc: 0.9453 - val_loss: 0.6305 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88614\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2277 - acc: 0.9651 - val_loss: 0.6582 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88614\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2012 - acc: 0.9741 - val_loss: 0.6266 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.88614 to 0.89356, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1992 - acc: 0.9759 - val_loss: 0.6517 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89356\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1984 - acc: 0.9765 - val_loss: 0.7022 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89356\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2103 - acc: 0.9693 - val_loss: 0.6223 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89356\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1952 - acc: 0.9745 - val_loss: 0.6994 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89356\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1881 - acc: 0.9826 - val_loss: 0.9284 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89356\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2326 - acc: 0.9660 - val_loss: 0.5856 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89356\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1863 - acc: 0.9765 - val_loss: 0.6194 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.89356 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2384 - acc: 0.9642 - val_loss: 0.8431 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89851\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2586 - acc: 0.9495 - val_loss: 0.6248 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.89851\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1944 - acc: 0.9723 - val_loss: 0.6715 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89851\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1837 - acc: 0.9801 - val_loss: 0.6389 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.89851 to 0.90099, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1884 - acc: 0.9783 - val_loss: 0.6745 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90099\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1700 - acc: 0.9862 - val_loss: 0.6886 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90099\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1687 - acc: 0.9847 - val_loss: 0.8914 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90099\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1767 - acc: 0.9832 - val_loss: 0.7588 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90099\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1738 - acc: 0.9814 - val_loss: 0.7275 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90099\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1686 - acc: 0.9814 - val_loss: 0.8255 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90099\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1706 - acc: 0.9820 - val_loss: 0.7814 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90099\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1612 - acc: 0.9850 - val_loss: 0.7896 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.90099\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1816 - acc: 0.9789 - val_loss: 0.9997 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90099\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1540 - acc: 0.9886 - val_loss: 0.8106 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90099\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1811 - acc: 0.9790 - val_loss: 1.0045 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.90099\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1745 - acc: 0.9805 - val_loss: 0.9256 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.90099\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1589 - acc: 0.9856 - val_loss: 0.8225 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90099\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1692 - acc: 0.9795 - val_loss: 0.8725 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.90099\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1626 - acc: 0.9856 - val_loss: 0.7396 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.90099\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1482 - acc: 0.9910 - val_loss: 0.8575 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.90099\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1504 - acc: 0.9892 - val_loss: 0.9641 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.90099\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1489 - acc: 0.9886 - val_loss: 0.8189 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.90099\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1408 - acc: 0.9928 - val_loss: 0.8692 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.90099\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1360 - acc: 0.9952 - val_loss: 0.8947 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90099\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1312 - acc: 0.9988 - val_loss: 0.9500 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90099\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1346 - acc: 0.9952 - val_loss: 0.9843 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90099\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1410 - acc: 0.9928 - val_loss: 0.9505 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90099\n",
      "Epoch 84/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2240 - acc: 0.9702 - val_loss: 0.6757 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90099\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1725 - acc: 0.9787 - val_loss: 0.6112 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90099\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1505 - acc: 0.9889 - val_loss: 0.7284 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90099\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1346 - acc: 0.9940 - val_loss: 0.7659 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90099\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1323 - acc: 0.9958 - val_loss: 0.8244 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90099\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1400 - acc: 0.9916 - val_loss: 0.7950 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90099\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1360 - acc: 0.9934 - val_loss: 0.9201 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90099\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1422 - acc: 0.9904 - val_loss: 0.8098 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90099\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1317 - acc: 0.9970 - val_loss: 0.9582 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90099\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1313 - acc: 0.9958 - val_loss: 0.9959 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90099\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1889 - acc: 0.9763 - val_loss: 0.8525 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90099\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1691 - acc: 0.9801 - val_loss: 0.7743 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90099\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1433 - acc: 0.9901 - val_loss: 0.8909 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90099\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1424 - acc: 0.9916 - val_loss: 0.7725 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90099\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1279 - acc: 0.9976 - val_loss: 0.8295 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90099\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1326 - acc: 0.9952 - val_loss: 0.9030 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00099: val_acc improved from 0.90099 to 0.90099, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1423 - acc: 0.9904 - val_loss: 0.7855 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90099\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1367 - acc: 0.9940 - val_loss: 0.8429 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.90099\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1285 - acc: 0.9964 - val_loss: 0.8279 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00102: val_acc improved from 0.90099 to 0.90347, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1222 - acc: 0.9982 - val_loss: 0.8482 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.90347\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1215 - acc: 0.9976 - val_loss: 0.8921 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.90347\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1211 - acc: 0.9994 - val_loss: 0.9450 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.90347\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1231 - acc: 0.9970 - val_loss: 0.8963 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00106: val_acc improved from 0.90347 to 0.90347, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1200 - acc: 0.9988 - val_loss: 0.9287 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.90347\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1182 - acc: 0.9994 - val_loss: 1.0149 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.90347\n",
      "Epoch 109/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 35s 1s/step - loss: 0.1269 - acc: 0.9946 - val_loss: 0.9602 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.90347\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1250 - acc: 0.9964 - val_loss: 0.9436 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.90347\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1333 - acc: 0.9940 - val_loss: 1.0488 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.90347\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1356 - acc: 0.9934 - val_loss: 0.9371 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.90347\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1370 - acc: 0.9916 - val_loss: 1.0553 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.90347\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1265 - acc: 0.9946 - val_loss: 0.8590 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.90347\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1203 - acc: 0.9988 - val_loss: 0.8205 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00115: val_acc improved from 0.90347 to 0.90842, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1151 - acc: 1.0000 - val_loss: 0.8690 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.90842\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1171 - acc: 0.9976 - val_loss: 0.8489 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.90842\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1155 - acc: 0.9994 - val_loss: 0.9402 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.90842\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1145 - acc: 0.9994 - val_loss: 0.8849 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.90842\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1147 - acc: 0.9988 - val_loss: 0.8718 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.90842\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1170 - acc: 0.9976 - val_loss: 0.9275 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.90842\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1176 - acc: 0.9976 - val_loss: 0.9327 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.90842\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1142 - acc: 1.0000 - val_loss: 0.9029 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00123: val_acc improved from 0.90842 to 0.90842, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0005_second.h5\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1122 - acc: 1.0000 - val_loss: 0.9520 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.90842\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1119 - acc: 0.9994 - val_loss: 1.0105 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.90842\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.2015 - acc: 0.9760 - val_loss: 1.7632 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.90842\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3442 - acc: 0.9203 - val_loss: 0.6807 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.90842\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1478 - acc: 0.9868 - val_loss: 0.7795 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.90842\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1292 - acc: 0.9946 - val_loss: 0.7907 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.90842\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1244 - acc: 0.9958 - val_loss: 0.7768 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.90842\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1241 - acc: 0.9940 - val_loss: 1.0098 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.90842\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1404 - acc: 0.9874 - val_loss: 0.7683 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.90842\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1232 - acc: 0.9970 - val_loss: 0.7703 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.90842\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1178 - acc: 0.9976 - val_loss: 0.9256 - val_acc: 0.8738\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.90842\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1155 - acc: 0.9988 - val_loss: 0.8236 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.90842\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1137 - acc: 0.9994 - val_loss: 0.8637 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.90842\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1117 - acc: 1.0000 - val_loss: 0.8961 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.90842\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1110 - acc: 1.0000 - val_loss: 0.8948 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.90842\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1118 - acc: 0.9982 - val_loss: 0.8966 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.90842\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1116 - acc: 0.9994 - val_loss: 0.9205 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.90842\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1131 - acc: 0.9982 - val_loss: 0.8900 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.90842\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1121 - acc: 0.9982 - val_loss: 0.9729 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.90842\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1129 - acc: 0.9982 - val_loss: 0.9421 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.90842\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1101 - acc: 0.9988 - val_loss: 0.9706 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.90842\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1145 - acc: 0.9976 - val_loss: 0.8723 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.90842\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1105 - acc: 0.9994 - val_loss: 0.9745 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.90842\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1120 - acc: 0.9982 - val_loss: 0.9154 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.90842\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1128 - acc: 0.9976 - val_loss: 0.9711 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.90842\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1072 - acc: 1.0000 - val_loss: 1.0001 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.90842\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1066 - acc: 1.0000 - val_loss: 1.0027 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.90842\n",
      "experiment: dropout=0.5 regularization0.0001\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.7203 - acc: 0.7447 - val_loss: 0.5853 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.6023 - acc: 0.7526 - val_loss: 0.5715 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.76238\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5760 - acc: 0.7619 - val_loss: 0.5628 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76238\n",
      "Epoch 4/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.5429 - acc: 0.7630 - val_loss: 0.5128 - val_acc: 0.7797\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76238 to 0.77970, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 5/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.5419 - acc: 0.7637 - val_loss: 0.5100 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.77970 to 0.78713, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 6/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.5117 - acc: 0.7746 - val_loss: 0.4967 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.78713\n",
      "Epoch 7/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.4958 - acc: 0.7754 - val_loss: 0.4863 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.78713 to 0.79703, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 8/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4827 - acc: 0.7983 - val_loss: 0.4835 - val_acc: 0.8094\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.79703 to 0.80941, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 9/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4651 - acc: 0.8163 - val_loss: 0.4557 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.80941 to 0.81436, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 10/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4574 - acc: 0.8220 - val_loss: 0.4482 - val_acc: 0.8317\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.81436 to 0.83168, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 11/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.4409 - acc: 0.8245 - val_loss: 0.4475 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.83168 to 0.83416, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 12/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4527 - acc: 0.8172 - val_loss: 0.4393 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.83416 to 0.84158, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 13/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4399 - acc: 0.8277 - val_loss: 0.4724 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.84158\n",
      "Epoch 14/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.4266 - acc: 0.8470 - val_loss: 0.4406 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.84158\n",
      "Epoch 15/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.3954 - acc: 0.8470 - val_loss: 0.5098 - val_acc: 0.7921\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.84158\n",
      "Epoch 16/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3900 - acc: 0.8596 - val_loss: 0.4353 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.84158 to 0.85891, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 17/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3765 - acc: 0.8638 - val_loss: 0.4560 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85891\n",
      "Epoch 18/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3513 - acc: 0.8774 - val_loss: 0.5203 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85891\n",
      "Epoch 19/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3543 - acc: 0.8726 - val_loss: 0.4638 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85891\n",
      "Epoch 20/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3500 - acc: 0.8744 - val_loss: 0.4358 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85891\n",
      "Epoch 21/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3514 - acc: 0.8824 - val_loss: 0.5036 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85891\n",
      "Epoch 22/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.3105 - acc: 0.8953 - val_loss: 0.5236 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.85891\n",
      "Epoch 23/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2919 - acc: 0.9113 - val_loss: 0.4468 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.85891 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 24/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.2818 - acc: 0.9055 - val_loss: 0.5645 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87129\n",
      "Epoch 25/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2777 - acc: 0.9113 - val_loss: 0.4618 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87129\n",
      "Epoch 26/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2497 - acc: 0.9200 - val_loss: 0.4513 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.87129 to 0.87129, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 27/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2689 - acc: 0.9053 - val_loss: 0.5272 - val_acc: 0.8391\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87129\n",
      "Epoch 28/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2391 - acc: 0.9242 - val_loss: 0.4261 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.87129 to 0.87624, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 29/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2284 - acc: 0.9281 - val_loss: 0.4387 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.87624 to 0.89109, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 30/150\n",
      "26/26 [==============================] - 32s 1s/step - loss: 0.2067 - acc: 0.9441 - val_loss: 0.4829 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.89109\n",
      "Epoch 31/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1855 - acc: 0.9486 - val_loss: 0.5953 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.89109\n",
      "Epoch 32/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1658 - acc: 0.9597 - val_loss: 0.6438 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.89109\n",
      "Epoch 33/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.2495 - acc: 0.9189 - val_loss: 0.6749 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.89109\n",
      "Epoch 34/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2156 - acc: 0.9321 - val_loss: 0.5001 - val_acc: 0.8342\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.89109\n",
      "Epoch 35/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1782 - acc: 0.9459 - val_loss: 0.4627 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.89109\n",
      "Epoch 36/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1620 - acc: 0.9588 - val_loss: 0.5084 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.89109\n",
      "Epoch 37/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1604 - acc: 0.9621 - val_loss: 0.5602 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.89109\n",
      "Epoch 38/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1544 - acc: 0.9630 - val_loss: 0.4895 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.89109\n",
      "Epoch 39/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1467 - acc: 0.9627 - val_loss: 0.4826 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.89109 to 0.89851, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 40/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1348 - acc: 0.9699 - val_loss: 0.5188 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89851\n",
      "Epoch 41/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1308 - acc: 0.9718 - val_loss: 0.5663 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89851\n",
      "Epoch 42/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1146 - acc: 0.9807 - val_loss: 0.6073 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89851\n",
      "Epoch 43/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.1339 - acc: 0.9654 - val_loss: 0.5549 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89851\n",
      "Epoch 44/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1147 - acc: 0.9741 - val_loss: 0.6354 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89851\n",
      "Epoch 45/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1202 - acc: 0.9759 - val_loss: 0.4961 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89851\n",
      "Epoch 46/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1075 - acc: 0.9799 - val_loss: 0.7142 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89851\n",
      "Epoch 47/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1144 - acc: 0.9777 - val_loss: 0.6601 - val_acc: 0.8292\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89851\n",
      "Epoch 48/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1350 - acc: 0.9660 - val_loss: 0.4911 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89851\n",
      "Epoch 49/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1152 - acc: 0.9741 - val_loss: 0.6240 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89851\n",
      "Epoch 50/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1139 - acc: 0.9714 - val_loss: 0.5976 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89851\n",
      "Epoch 51/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.1036 - acc: 0.9835 - val_loss: 0.5983 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00051: val_acc improved from 0.89851 to 0.90842, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 52/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.1290 - acc: 0.9717 - val_loss: 0.5747 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.90842\n",
      "Epoch 53/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.1253 - acc: 0.9706 - val_loss: 0.4273 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.90842\n",
      "Epoch 54/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0893 - acc: 0.9856 - val_loss: 0.6305 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.90842\n",
      "Epoch 55/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0764 - acc: 0.9874 - val_loss: 0.4942 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.90842\n",
      "Epoch 56/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0757 - acc: 0.9898 - val_loss: 0.5141 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.90842 to 0.91832, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 57/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0757 - acc: 0.9898 - val_loss: 0.5791 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.91832\n",
      "Epoch 58/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0867 - acc: 0.9868 - val_loss: 0.6372 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.91832\n",
      "Epoch 59/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0839 - acc: 0.9874 - val_loss: 0.6408 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.91832\n",
      "Epoch 60/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0817 - acc: 0.9862 - val_loss: 0.5457 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.91832\n",
      "Epoch 61/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0881 - acc: 0.9838 - val_loss: 0.5673 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.91832\n",
      "Epoch 62/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0771 - acc: 0.9886 - val_loss: 0.7113 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.91832\n",
      "Epoch 63/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0640 - acc: 0.9928 - val_loss: 0.5766 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.91832\n",
      "Epoch 64/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0851 - acc: 0.9874 - val_loss: 0.6312 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.91832\n",
      "Epoch 65/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0721 - acc: 0.9898 - val_loss: 0.6997 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.91832\n",
      "Epoch 66/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0762 - acc: 0.9880 - val_loss: 0.5738 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.91832\n",
      "Epoch 67/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0799 - acc: 0.9880 - val_loss: 0.6971 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.91832\n",
      "Epoch 68/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0879 - acc: 0.9880 - val_loss: 0.6488 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.91832\n",
      "Epoch 69/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0784 - acc: 0.9862 - val_loss: 0.5624 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.91832\n",
      "Epoch 70/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0686 - acc: 0.9916 - val_loss: 0.6263 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.91832\n",
      "Epoch 71/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0539 - acc: 0.9976 - val_loss: 0.5596 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.91832\n",
      "Epoch 72/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0624 - acc: 0.9907 - val_loss: 0.6107 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.91832\n",
      "Epoch 73/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0580 - acc: 0.9946 - val_loss: 0.6248 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.91832\n",
      "Epoch 74/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0752 - acc: 0.9889 - val_loss: 0.6872 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.91832\n",
      "Epoch 75/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0701 - acc: 0.9910 - val_loss: 0.6520 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.91832\n",
      "Epoch 76/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0651 - acc: 0.9934 - val_loss: 0.6585 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.91832\n",
      "Epoch 77/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0601 - acc: 0.9964 - val_loss: 0.5827 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.91832\n",
      "Epoch 78/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0548 - acc: 0.9958 - val_loss: 0.5664 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.91832\n",
      "Epoch 79/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0631 - acc: 0.9928 - val_loss: 0.6559 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.91832\n",
      "Epoch 80/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0521 - acc: 0.9964 - val_loss: 0.6139 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.91832\n",
      "Epoch 81/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0542 - acc: 0.9964 - val_loss: 0.5925 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.91832\n",
      "Epoch 82/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.0548 - acc: 0.9958 - val_loss: 0.9960 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.91832\n",
      "Epoch 83/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0601 - acc: 0.9934 - val_loss: 0.6422 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.91832\n",
      "Epoch 84/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 35s 1s/step - loss: 0.0471 - acc: 0.9988 - val_loss: 0.6354 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.91832\n",
      "Epoch 85/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0566 - acc: 0.9952 - val_loss: 0.6381 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.91832\n",
      "Epoch 86/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0481 - acc: 0.9976 - val_loss: 0.6920 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.91832\n",
      "Epoch 87/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0527 - acc: 0.9961 - val_loss: 0.6310 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.91832\n",
      "Epoch 88/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0539 - acc: 0.9946 - val_loss: 0.6620 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.91832\n",
      "Epoch 89/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0513 - acc: 0.9970 - val_loss: 0.7569 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.91832\n",
      "Epoch 90/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0532 - acc: 0.9958 - val_loss: 0.6908 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.91832\n",
      "Epoch 91/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0465 - acc: 0.9976 - val_loss: 0.6945 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.91832\n",
      "Epoch 92/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0443 - acc: 1.0000 - val_loss: 0.7909 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.91832\n",
      "Epoch 93/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0427 - acc: 1.0000 - val_loss: 0.7579 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.91832\n",
      "Epoch 94/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0427 - acc: 1.0000 - val_loss: 0.7835 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.91832\n",
      "Epoch 95/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0440 - acc: 0.9994 - val_loss: 0.8104 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.91832\n",
      "Epoch 96/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0485 - acc: 0.9976 - val_loss: 1.0023 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.91832\n",
      "Epoch 97/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0499 - acc: 0.9970 - val_loss: 0.6760 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.91832\n",
      "Epoch 98/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0554 - acc: 0.9940 - val_loss: 0.6571 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00098: val_acc improved from 0.91832 to 0.92327, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.5_0.0001_second.h5\n",
      "Epoch 99/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.0511 - acc: 0.9982 - val_loss: 0.7862 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.92327\n",
      "Epoch 100/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0476 - acc: 0.9976 - val_loss: 0.7325 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.92327\n",
      "Epoch 101/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0483 - acc: 0.9982 - val_loss: 0.7888 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.92327\n",
      "Epoch 102/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0445 - acc: 0.9988 - val_loss: 0.8326 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.92327\n",
      "Epoch 103/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0484 - acc: 0.9976 - val_loss: 0.9207 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.92327\n",
      "Epoch 104/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0591 - acc: 0.9940 - val_loss: 0.7878 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.92327\n",
      "Epoch 105/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0658 - acc: 0.9952 - val_loss: 0.7796 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.92327\n",
      "Epoch 106/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0516 - acc: 0.9976 - val_loss: 0.7899 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.92327\n",
      "Epoch 107/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0435 - acc: 0.9988 - val_loss: 0.8287 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.92327\n",
      "Epoch 108/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0431 - acc: 0.9994 - val_loss: 0.8720 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.92327\n",
      "Epoch 109/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0561 - acc: 0.9925 - val_loss: 1.1542 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.92327\n",
      "Epoch 110/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0719 - acc: 0.9892 - val_loss: 0.7282 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.92327\n",
      "Epoch 111/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0586 - acc: 0.9952 - val_loss: 0.8977 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.92327\n",
      "Epoch 112/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0859 - acc: 0.9907 - val_loss: 0.8971 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.92327\n",
      "Epoch 113/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.2211 - acc: 0.9438 - val_loss: 0.5253 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.92327\n",
      "Epoch 114/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0833 - acc: 0.9880 - val_loss: 0.6534 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.92327\n",
      "Epoch 115/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.0559 - acc: 0.9946 - val_loss: 0.5996 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.92327\n",
      "Epoch 116/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0476 - acc: 0.9994 - val_loss: 0.6736 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.92327\n",
      "Epoch 117/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0423 - acc: 1.0000 - val_loss: 0.6535 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.92327\n",
      "Epoch 118/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0484 - acc: 0.9970 - val_loss: 0.6914 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.92327\n",
      "Epoch 119/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0483 - acc: 0.9970 - val_loss: 0.6088 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.92327\n",
      "Epoch 120/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0424 - acc: 0.9994 - val_loss: 0.6597 - val_acc: 0.8911\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.92327\n",
      "Epoch 121/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0417 - acc: 1.0000 - val_loss: 0.6838 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.92327\n",
      "Epoch 122/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0477 - acc: 0.9976 - val_loss: 0.6732 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.92327\n",
      "Epoch 123/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0426 - acc: 0.9994 - val_loss: 0.6688 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.92327\n",
      "Epoch 124/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0407 - acc: 1.0000 - val_loss: 0.7614 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.92327\n",
      "Epoch 125/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0398 - acc: 1.0000 - val_loss: 0.7476 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.92327\n",
      "Epoch 126/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0401 - acc: 1.0000 - val_loss: 0.7615 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.92327\n",
      "Epoch 127/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0396 - acc: 1.0000 - val_loss: 0.7698 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.92327\n",
      "Epoch 128/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0394 - acc: 1.0000 - val_loss: 0.8237 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.92327\n",
      "Epoch 129/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0391 - acc: 1.0000 - val_loss: 0.8394 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.92327\n",
      "Epoch 130/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.7915 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.92327\n",
      "Epoch 131/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0390 - acc: 1.0000 - val_loss: 0.8022 - val_acc: 0.8886\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.92327\n",
      "Epoch 132/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0388 - acc: 1.0000 - val_loss: 0.8089 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.92327\n",
      "Epoch 133/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0386 - acc: 1.0000 - val_loss: 0.8239 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.92327\n",
      "Epoch 134/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0386 - acc: 1.0000 - val_loss: 0.8472 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.92327\n",
      "Epoch 135/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0384 - acc: 1.0000 - val_loss: 0.8478 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.92327\n",
      "Epoch 136/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0383 - acc: 1.0000 - val_loss: 0.8566 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.92327\n",
      "Epoch 137/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0391 - acc: 0.9994 - val_loss: 0.8955 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.92327\n",
      "Epoch 138/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0401 - acc: 0.9994 - val_loss: 0.7994 - val_acc: 0.9010\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.92327\n",
      "Epoch 139/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0402 - acc: 0.9988 - val_loss: 0.7663 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.92327\n",
      "Epoch 140/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0393 - acc: 0.9994 - val_loss: 0.8292 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.92327\n",
      "Epoch 141/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0387 - acc: 1.0000 - val_loss: 0.9189 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.92327\n",
      "Epoch 142/150\n",
      "26/26 [==============================] - 33s 1s/step - loss: 0.0379 - acc: 1.0000 - val_loss: 0.9373 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.92327\n",
      "Epoch 143/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0377 - acc: 1.0000 - val_loss: 0.9668 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.92327\n",
      "Epoch 144/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0416 - acc: 0.9976 - val_loss: 0.8323 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.92327\n",
      "Epoch 145/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0382 - acc: 1.0000 - val_loss: 0.8373 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.92327\n",
      "Epoch 146/150\n",
      "26/26 [==============================] - 36s 1s/step - loss: 0.0385 - acc: 1.0000 - val_loss: 0.7991 - val_acc: 0.9059\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.92327\n",
      "Epoch 147/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0464 - acc: 0.9958 - val_loss: 0.7858 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.92327\n",
      "Epoch 148/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0590 - acc: 0.9928 - val_loss: 0.7109 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.92327\n",
      "Epoch 149/150\n",
      "26/26 [==============================] - 35s 1s/step - loss: 0.0636 - acc: 0.9916 - val_loss: 0.5437 - val_acc: 0.8861\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.92327\n",
      "Epoch 150/150\n",
      "26/26 [==============================] - 34s 1s/step - loss: 0.0485 - acc: 0.9976 - val_loss: 0.6313 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.92327\n"
     ]
    }
   ],
   "source": [
    "# create optimizer to train model\n",
    "adam = optimizers.Adam(lr=0.0001,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.00001,amsgrad=False)\n",
    "# create array to store experiment results\n",
    "data = []\n",
    "data.append(['dropout','regularization','val_acc'])\n",
    "# run experiment\n",
    "for _dropout in dropout:\n",
    "    for _regularization in regularization:\n",
    "        # Print level\n",
    "        print('experiment: dropout='+str(_dropout)+' regularization'+str(_regularization))\n",
    "        # build model\n",
    "        model = build_VGG19_fine_tuning_model(_dropout, _regularization)\n",
    "        # train model\n",
    "        history = train_model(adam,model,'leishmaniasis',150,\n",
    "                              save_as='finetuning_reg_drop_'+str(_dropout)+'_'+str(_regularization)+'_second')\n",
    "        # store result\n",
    "        data.append([_dropout,_regularization,max(history.history['val_acc'])])\n",
    "        # Export history\n",
    "        export_history(history,'finetuning_reg_drop_'+str(_dropout)+'_'+str(_regularization)+'_second',150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, imprimimos los resultados del experimento; para cada combinación de dropout y regularización, cual es el máximo valor de la exactitud de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dropout</td>\n",
       "      <td>regularization</td>\n",
       "      <td>val_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.925743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.908416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.89604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.90099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.905941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.908416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.891089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.903465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.915842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.915842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.925743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.925743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.903465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.908416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.923267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0               1         2\n",
       "0   dropout  regularization   val_acc\n",
       "1       0.1           0.001  0.925743\n",
       "2       0.1          0.0005  0.908416\n",
       "3       0.1          0.0001   0.89604\n",
       "4       0.2           0.001   0.90099\n",
       "5       0.2          0.0005  0.905941\n",
       "6       0.2          0.0001  0.908416\n",
       "7       0.3           0.001  0.891089\n",
       "8       0.3          0.0005  0.903465\n",
       "9       0.3          0.0001  0.915842\n",
       "10      0.4           0.001  0.915842\n",
       "11      0.4          0.0005  0.925743\n",
       "12      0.4          0.0001  0.925743\n",
       "13      0.5           0.001  0.903465\n",
       "14      0.5          0.0005  0.908416\n",
       "15      0.5          0.0001  0.923267"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print data from the experiment\n",
    "result = pd.DataFrame(data)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tercer intento\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds model for transfer learning and fine tunning\n",
    "#  PARAMS\n",
    "#     dropout: dropout value for dropout layers\n",
    "#     regularization: regularization \n",
    "def build_VGG19_fine_tuning_model(dropout, regularization):\n",
    "    model = Sequential()\n",
    "    # block 1\n",
    "    model.add(Conv2D(64, (3, 3),activation='relu',padding='same',name='block1_conv1',input_shape=(224,224,3)))\n",
    "    model.add(Conv2D(64, (3, 3),activation='relu',padding='same',name='block1_conv2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3, 3),activation='relu',padding='same',name='block2_conv1'))\n",
    "    model.add(Conv2D(128, (3, 3),activation='relu',padding='same',name='block2_conv2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv1'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv2'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv3'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same',name='block3_conv4'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv1'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv2'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv3'))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block4_conv4'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n",
    "    # Block 5\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv1',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv2',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv3',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(Conv2D(512, (3, 3),activation='relu',padding='same',name='block5_conv4',\n",
    "                   kernel_regularizer=regularizers.l2(regularization),\n",
    "                   bias_regularizer=regularizers.l2(regularization)))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'))\n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                        cache_subdir='models',\n",
    "                        file_hash='253f8cb515780f3b799900260a226db6')\n",
    "    model.load_weights(weights_path)\n",
    "    # Freeze conv layers that are not going to be trained\n",
    "    for layer in model.layers[:-5]:\n",
    "        layer.trainable = False \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1,activation='sigmoid'))    \n",
    "    # Multigpu model to speed up\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    return model  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo, definimos los diferentes niveles de regularización y dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = [0.4, 0.5]\n",
    "regularization = [0.005,0.01,0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tercero, deifnimos la función de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train model\n",
    "#     PARAMS:\n",
    "#        optimizer: optimizer to train model\n",
    "#        model: model to be trained\n",
    "#        dataset: dataset to train modelo \n",
    "#        epoochs: number of epochs to train model\n",
    "#        save_as: name to save best weigths\n",
    "#     RETURNS\n",
    "#        history: vaules for acc and val_acc for each epoch\n",
    "def train_model(optimizer,model,dataset,epochs,save_as,image_size=224,batch_size=64):\n",
    "    # Data generator to  rescale training images\n",
    "    # Data Augmentation: Horizontal and vertical flips\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255.0,\n",
    "                                      vertical_flip=True,\n",
    "                                      horizontal_flip=True)\n",
    "    # Data generator to rescale test images\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255.0)\n",
    "    # Data flow training images\n",
    "    train_flow = train_datagen.flow_from_directory(\n",
    "        directory='src/'+dataset+'/training',  \n",
    "        target_size=(image_size, image_size),  \n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    # Data flow test images\n",
    "    test_flow = test_datagen.flow_from_directory(\n",
    "        directory='src/'+dataset+'/test',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['acc'])\n",
    "    # Create check point call back to store best validation weigths\n",
    "    bestWeigthsPath='src/trainingWeigths/best_' + save_as+'.h5'\n",
    "    checkpoint = ModelCheckpoint(bestWeigthsPath, monitor='val_acc',save_weights_only=False, verbose=1, save_best_only=True, mode='max')\n",
    "    # Run experiment\n",
    "    history = model.fit_generator(\n",
    "            generator=train_flow,\n",
    "            epochs=epochs,\n",
    "            validation_data=test_flow,\n",
    "            callbacks=[checkpoint],\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuarto, definimos función para exportar historia de cada entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_history(history, save_as,epochs):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    file = open('src/history/history' + save_as+'.txt','w')\n",
    "    file.write('acc,val_acc'+'\\n')\n",
    "    for i in range(epochs):\n",
    "        file.write(str(acc[i])+','+str(val_acc[i])+'\\n')\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quinto, ejecutamos el experimento para cada combinación de dropout y regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment: dropout=0.4 regularization0.005\n",
      "Found 1618 images belonging to 2 classes.\n",
      "Found 404 images belonging to 2 classes.\n",
      "Epoch 1/150\n",
      "26/26 [==============================] - 28s 1s/step - loss: 4.4568 - acc: 0.7495 - val_loss: 3.9674 - val_acc: 0.7624\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76238, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.005.h5\n",
      "Epoch 2/150\n",
      "26/26 [==============================] - 27s 1s/step - loss: 3.6226 - acc: 0.7621 - val_loss: 3.2726 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76238 to 0.77475, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.005.h5\n",
      "Epoch 3/150\n",
      "26/26 [==============================] - 38s 1s/step - loss: 3.0656 - acc: 0.7628 - val_loss: 2.8047 - val_acc: 0.7871\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77475 to 0.78713, saving model to src/trainingWeigths/best_finetuning_reg_drop_0.4_0.005.h5\n",
      "Epoch 4/150\n",
      " 5/26 [====>.........................] - ETA: 46s - loss: 2.7734 - acc: 0.8187"
     ]
    }
   ],
   "source": [
    "# create optimizer to train model\n",
    "adam = optimizers.Adam(lr=0.0001,beta_1=0.9,beta_2=0.999,epsilon=None,decay=0.00001,amsgrad=False)\n",
    "# create array to store experiment results\n",
    "data = []\n",
    "data.append(['dropout','regularization','val_acc'])\n",
    "# run experiment\n",
    "for _dropout in dropout:\n",
    "    for _regularization in regularization:\n",
    "        # Print level\n",
    "        print('experiment: dropout='+str(_dropout)+' regularization'+str(_regularization))\n",
    "        # build model\n",
    "        model = build_VGG19_fine_tuning_model(_dropout, _regularization)\n",
    "        # train model\n",
    "        history = train_model(adam,model,'leishmaniasis',150,\n",
    "                              save_as='finetuning_reg_drop_'+str(_dropout)+'_'+str(_regularization))\n",
    "        # store result\n",
    "        data.append([_dropout,_regularization,max(history.history['val_acc'])])\n",
    "        # Export history\n",
    "        export_history(history,'finetuning_reg_drop_'+str(_dropout)+'_'+str(_regularization),150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, imprimimos los resultados del experimento; para cada combinación de dropout y regularización, cual es el máximo valor de la exactitud de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print data from the experiment\n",
    "result = pd.DataFrame(data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
